{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with CNN/DailyMail Dataset on Microsoft Foundry\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models using **Supervised Fine-Tuning (SFT)** with the CNN/DailyMail News Summarization dataset.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Understand supervised fine-tuning for summarization tasks\n",
    "2. Prepare and format news summarization data\n",
    "3. Upload datasets to Microsoft Foundry\n",
    "4. Create and monitor a supervised fine-tuning job\n",
    "5. Deploy and test your fine-tuned model\n",
    "\n",
    "**Note**: Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install all required packages from requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-projects>=2.0.0b1 (from -r requirements.txt (line 2))\n",
      "  Downloading azure_ai_projects-2.0.0b3-py3-none-any.whl.metadata (68 kB)\n",
      "     ---------------------------------------- 0.0/68.9 kB ? eta -:--:--\n",
      "     ----------------- ---------------------- 30.7/68.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 68.9/68.9 kB 751.8 kB/s eta 0:00:00\n",
      "Collecting openai (from -r requirements.txt (line 5))\n",
      "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting azure-identity (from -r requirements.txt (line 8))\n",
      "  Using cached azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 12))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.35.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_storage_blob-12.27.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai->-r requirements.txt (line 5))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\work\\amlrepos\\fine-tuning\\envsftcnn\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.15.0)\n",
      "Collecting cryptography>=2.5 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting msrest>=0.7.1 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-mgmt-core>=1.6.0 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\envsftcnn\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading azure_ai_projects-2.0.0b3-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 92.2/240.7 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 240.7/240.7 kB 3.0 MB/s eta 0:00:00\n",
      "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
      "Using cached azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
      "Using cached azure_storage_blob-12.27.1-py3-none-any.whl (428 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.9/152.9 kB 8.9 MB/s eta 0:00:00\n",
      "Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: urllib3, typing-inspection, tqdm, sniffio, python-dotenv, PyJWT, pydantic-core, pycparser, oauthlib, jiter, isodate, idna, h11, distro, charset_normalizer, certifi, annotated-types, requests, pydantic, httpcore, cffi, anyio, requests-oauthlib, httpx, cryptography, azure-core, openai, msrest, azure-storage-blob, azure-mgmt-core, msal, azure-mgmt-cognitiveservices, msal-extensions, azure-identity, azure-ai-projects\n",
      "Successfully installed PyJWT-2.10.1 annotated-types-0.7.0 anyio-4.12.0 azure-ai-projects-2.0.0b3 azure-core-1.37.0 azure-identity-1.25.1 azure-mgmt-cognitiveservices-14.1.0 azure-mgmt-core-1.6.0 azure-storage-blob-12.27.1 certifi-2026.1.4 cffi-2.0.0 charset_normalizer-3.4.4 cryptography-46.0.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 isodate-0.7.2 jiter-0.12.0 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 oauthlib-3.3.1 openai-2.14.0 pycparser-2.23 pydantic-2.12.5 pydantic-core-2.41.5 python-dotenv-1.2.1 requests-2.32.5 requests-oauthlib-2.0.0 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2 urllib3-2.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Azure Environment\n",
    "\n",
    "Set your Microsoft Foundry Project endpoint, model name and other environment variables. We're using **gpt-4.1** in this example, but you can use other supported GPT models. Copy the file `.env.template` (located in this folder), and save it as file named `.env`. Enter appropriate values for the environment variables used for the job you want to run.\n",
    "\n",
    "```\n",
    "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint>\n",
    "MODEL_NAME=gpt-4.1\n",
    "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
    "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
    "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_name = os.environ.get(\"MODEL_NAME\")\n",
    "\n",
    "# Define dataset file paths\n",
    "training_file_path = \"training.jsonl\"\n",
    "validation_file_path = \"validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to Microsoft Foundry Project\n",
    "\n",
    "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows.\n",
    "\n",
    "**Important**: Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Microsoft Foundry Project\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "openai_client = project_client.get_openai_client()\n",
    "\n",
    "print(\"Connected to Microsoft Foundry Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Training Files\n",
    "\n",
    "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job.\n",
    "If training or validation files are already uploaded to storage account, then you can import those files directly instead of reuploading these files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Uploading validation file...\n",
      "Training file ID: file-cd8eca04fa5c4d97bdbe9b8f6df3e9a0\n",
      "Validation file ID: file-1de00b2fbb2e416a858d6cf8a93aaa9b\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading training file...\")\n",
    "with open(training_file_path, \"rb\") as f:\n",
    "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(\"Uploading validation file...\")\n",
    "with open(validation_file_path, \"rb\") as f:\n",
    "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "train_file_id = train_file.id\n",
    "val_file_id = validation_file.id\n",
    "\n",
    "print(f\"Training file ID: {train_file_id}\")\n",
    "print(f\"Validation file ID: {val_file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Training Files from Azure Blob Storage (Optional)\n",
    "\n",
    "If your training data is already stored in Azure Blob Storage, you can import it directly without re-uploading.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload `training.jsonl` and `validation.jsonl` to Azure Blob Storage\n",
    "2. Generate SAS tokens with read permissions.\n",
    "3. Set environment variables:\n",
    "   ```bash\n",
    "   TRAINING_FILE_BLOB_URL=https://<storage-account>.blob.core.windows.net/<container>/training.jsonl?<sas-token>\n",
    "   VALIDATION_FILE_BLOB_URL=https://<storage-account>.blob.core.windows.net/<container>/validation.jsonl?<sas-token>\n",
    "   ```\n",
    "\n",
    "**Note:** This uses the Azure-specific `/openai/files/import` endpoint (preview API). Skip this section if you've already uploaded files in Section 5 so either upload files or import files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing files from Azure Blob Storage...\n",
      "Importing training file...\n",
      "Training file imported: file-9e05ab8899db40cfaf06cbbf03d8f74d\n",
      "Importing validation file...\n",
      "Validation file imported: file-120e7bb085c04454a24598380e3a8688\n",
      "Import completed! Files are being processed...\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "training_blob_url = os.environ.get(\"TRAINING_FILE_BLOB_URL\")\n",
    "validation_blob_url = os.environ.get(\"VALIDATION_FILE_BLOB_URL\")\n",
    "\n",
    "if training_blob_url:\n",
    "    print(\"Importing files from Azure Blob Storage...\")\n",
    "    \n",
    "    import_url = f\"{endpoint}/openai/files/import?api-version=2025-11-15-preview\"\n",
    "    \n",
    "    token = credential.get_token(\"https://ai.azure.com/.default\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    print(\"Importing training file...\")\n",
    "    train_import_request = {\n",
    "        \"filename\": \"training.jsonl\",\n",
    "        \"purpose\": \"fine-tune\",\n",
    "        \"content_url\": training_blob_url\n",
    "    }\n",
    "    \n",
    "    with httpx.Client() as client:\n",
    "        response = client.post(import_url, headers=headers, json=train_import_request)\n",
    "        response.raise_for_status()\n",
    "        train_file_data = response.json()\n",
    "        train_file_id = train_file_data[\"id\"]\n",
    "        print(f\"Training file imported: {train_file_id}\")\n",
    "        \n",
    "        if validation_blob_url:\n",
    "            print(\"Importing validation file...\")\n",
    "            val_import_request = {\n",
    "                \"filename\": \"validation.jsonl\",\n",
    "                \"purpose\": \"fine-tune\",\n",
    "                \"content_url\": validation_blob_url\n",
    "            }\n",
    "            \n",
    "            response = client.post(import_url, headers=headers, json=val_import_request)\n",
    "            response.raise_for_status()\n",
    "            val_file_data = response.json()\n",
    "            val_file_id = val_file_data[\"id\"]\n",
    "            print(f\"Validation file imported: {val_file_id}\")\n",
    "    \n",
    "    print(\"Import completed! Files are being processed...\")\n",
    "else:\n",
    "    print(\"Blob URLs not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wait for File Processing\n",
    "\n",
    "Microsoft Foundry needs to process the uploaded files before they can be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for files to be processed...\n",
      "Files ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for files to be processed...\")\n",
    "openai_client.files.wait_for_processing(train_file_id)\n",
    "openai_client.files.wait_for_processing(val_file_id)\n",
    "print(\"Files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Supervised Fine-Tuning Job\n",
    "\n",
    "Create a supervised fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
    "\n",
    "**Hyperparameters:**\n",
    "1. **n_epochs (3)**: Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
    "2. **batch_size (1)**: Number of training examples processed together in each iteration. Smaller batches provide more frequent updates. Typical range: 1-8.\n",
    "3. **learning_rate_multiplier (1.0)**: Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
    "\n",
    "**Note**: Adjust these based on your dataset size and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating supervised fine-tuning job...\n",
      "Fine-tuning job created!\n",
      "Job ID: ftjob-8aad139b19084ebf968c1e58f2dc3326\n",
      "Status: pending\n",
      "Model: gpt-4.1-2025-04-14\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating supervised fine-tuning job...\")\n",
    "\n",
    "fine_tune_job = openai_client.fine_tuning.jobs.create(\n",
    "    model=model_name,\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\"hyperparameters\": {\"n_epochs\": 3, \"batch_size\": 1, \"learning_rate_multiplier\": 1.0}},\n",
    "    },\n",
    "    extra_body={\"trainingType\": \"Standard\"},\n",
    "    suffix=\"cnn-dailymail-summarization\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created!\")\n",
    "print(f\"Job ID: {fine_tune_job.id}\")\n",
    "print(f\"Status: {fine_tune_job.status}\")\n",
    "print(f\"Model: {fine_tune_job.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitor Training Progress\n",
    "\n",
    "Track the status of your fine-tuning job. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
    "print(f\"Status: {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieve Fine-Tuned Model\n",
    "\n",
    "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: gpt-4.1-2025-04-14.ft-8aad139b19084ebf968c1e58f2dc3326-cnn-dailymail-summarization\n"
     ]
    }
   ],
   "source": [
    "completed_job = openai_client.fine_tuning.jobs.retrieve(\"ftjob-8aad139b19084ebf968c1e58f2dc3326\")\n",
    "\n",
    "if completed_job.status == \"succeeded\":\n",
    "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
    "    print(f\"Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
    "else:\n",
    "    print(f\"Status: {completed_job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Deploy the Fine-Tuned Model\n",
    "\n",
    "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying fine-tuned model: gpt-4.1-2025-04-14.ft-8aad139b19084ebf968c1e58f2dc3326-cnn-dailymail-summarization\n",
      "Waiting for deployment to complete...\n",
      "Model deployment completed: gpt-4.1-cnn-dailymail-finetuned\n"
     ]
    }
   ],
   "source": [
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
    "\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
    "\n",
    "deployment_name = \"gpt-4.1-cnn-dailymail-finetuned\"\n",
    "\n",
    "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
    "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
    "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
    "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=50)\n",
    "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
    "    \n",
    "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
    "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "        resource_group_name=resource_group,\n",
    "        account_name=account_name,\n",
    "        deployment_name=deployment_name,\n",
    "        deployment=deployment_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting for deployment to complete...\")\n",
    "    deployment.result()\n",
    "\n",
    "print(f\"Model deployment completed: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Fine-Tuned Model\n",
    "\n",
    "Test your fine-tuned model by generating a summary for a sample news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists at a leading research university have created a more efficient solar panel .\n",
      "The new panels are 40% more efficient than current models .\n",
      "A special coating captures more sunlight and converts it to electricity .\n",
      "The technology could be available for commercial use within five years .\n",
      "Experts say the breakthrough will help reduce the cost of clean energy .\n"
     ]
    }
   ],
   "source": [
    "test_article = \"\"\"Scientists at a leading research university have made a breakthrough in renewable energy technology. The team developed a new type of solar panel that is 40% more efficient than current models. The innovation uses a special coating that captures more sunlight and converts it into electricity more effectively. Researchers say the technology could be available for commercial use within the next five years. The discovery is expected to significantly reduce the cost of solar energy and help accelerate the transition to clean energy sources. Environmental experts have praised the development as a major step forward in the fight against climate change.\"\"\"\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model=deployment_name,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize this article:{test_article}\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Congratulations!\n",
    "\n",
    "You've successfully fine-tuned a model for news summarization using the CNN/DailyMail dataset!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Test with more examples**: Try different news articles to evaluate performance\n",
    "2. **Adjust hyperparameters**: Experiment with different epoch counts, batch sizes, or learning rates\n",
    "3. **Deploy to production**: Integrate your fine-tuned model into applications\n",
    "4. **Fine-tune further**: Use your own domain-specific articles for specialized summarization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envrft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
