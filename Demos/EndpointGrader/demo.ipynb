{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4098466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd5785",
   "metadata": {},
   "source": [
    "# Setup\n",
    "You should have a local `.env` file with at least:\n",
    "* `X_FUNCTIONS_KEY`\n",
    "* `AZURE_FUNCTIONS_ENDPOINT`\n",
    "* `AZURE_OPENAI_API_KEY`\n",
    "* `AZURE_OPENAI_ENDPOINT`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a1c62",
   "metadata": {},
   "source": [
    "For now, we're using an API key. We configure the non-Azure OpenAI client to talk to the OpenAI/v1 API in Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d588d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "print(\"> Created OpenAI client.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6353d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a \"unique enough\" identifier that lets us run this notebook\n",
    "# multiple times and easily keep track of things each run creates.\n",
    "import uuid\n",
    "\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]\n",
    "print(f\"> Using unique enough key: {UNIQUE_ENOUGH_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_PROMPT = \"\"\"\n",
    "You are an expert in arithmetic problem solving. Given a target number and a list of\n",
    "numbers, your task is to combine all of the numbers exactly once using addition (+),\n",
    "subtraction (-), multiplication (x), or division (/) to reach the target.\n",
    "\n",
    "- You must use every number exactly once.\n",
    "- Use parentheses as needed to control the order of operations.\n",
    "- Return only a valid JSON object with the following exact syntax:\n",
    "  {\n",
    "    \"expression\": <the expression>,\n",
    "    \"result\": <the result>\n",
    "  }\n",
    "- The expression should be a string representation of the mathematical expression.\n",
    "- The result should be an integer.\n",
    "- If the exact target is not possible, return the closest valid result using all numbers.\n",
    "\n",
    "# Example 1\n",
    "target: 850\n",
    "numbers: [100, 75, 50, 25, 6, 3]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((100 * 6) + (75 + 25) + (3 * 50))\",\n",
    "  \"result\": 850\n",
    "}\n",
    "\n",
    "# Example 2\n",
    "target: 945\n",
    "numbers: [25, 100, 9, 3, 6, 2]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((100 * 9) + (6 * 3) + (25 + 2))\",\n",
    "  \"result\": 945\n",
    "}\n",
    "\n",
    "# Example 3\n",
    "target: 310\n",
    "numbers: [7, 50, 75, 3, 8, 2]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((75 * 3) + (50 * 2) - (8 + 7))\",\n",
    "  \"result\": 310\n",
    "}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b731b",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "We're first going to assess how some models perform natively at this task given the above prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317250db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our datasets for eval\n",
    "\n",
    "with open(\"./data/countdown_eval_100.jsonl\", mode=\"rb\") as f:\n",
    "    eval_file = client.files.create(file=f, purpose=\"evals\")\n",
    "eval_file = client.files.wait_for_processing(eval_file.id)\n",
    "\n",
    "print(f\"> Uploaded eval file {eval_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to describe the source and schema for our files.\n",
    "EVAL_DATA_SOURCE = {\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"target\": {\"type\": \"integer\"},\n",
    "            \"nums\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n",
    "        },\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "    \"type\": \"custom\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace46dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our expected Response schema from the models. We'll use structured outputs\n",
    "# so the model must output only JSON.\n",
    "RESPONSE_SCHEMA = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_expression\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\"type\": \"string\"},\n",
    "                \"result\": {\"type\": \"integer\"},\n",
    "            },\n",
    "            \"required\": [\"expression\", \"result\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e7cb1",
   "metadata": {},
   "source": [
    "## Endpoint Grader\n",
    "Our endpoint grader points to a remote endpoint that hosts the grader function.\n",
    "\n",
    "It has an API surface that's basically the Python grader, but via POST'ing JSON\n",
    "to an endpoint.\n",
    "\n",
    "We're using an Azure Function with key based auth, so each request will need\n",
    "the `X-Functions-Key` header with our authentication key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll define our Grader.\n",
    "import os\n",
    "\n",
    "URL = os.getenv(\"AZURE_FUNCTIONS_ENDPOINT\")\n",
    "ENDPOINT_GRADER = {\n",
    "    \"type\": \"endpoint\",\n",
    "    \"name\": \"Remote Countdown Grader\",\n",
    "    \"url\": URL,\n",
    "    \"headers\": {\n",
    "        \"X-Functions-Key\": os.getenv(\"X_FUNCTIONS_KEY\"),\n",
    "    },\n",
    "    \"pass_threshold\": 5.0,\n",
    "    # \"rate_limit\": 50,\n",
    "}\n",
    "print(f\"> Defined endpoint grader for {URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f531e2b",
   "metadata": {},
   "source": [
    "For comparison, we'll use the same code but run as a Python grader.\n",
    "\n",
    "In practice, this makes no sense...but it demonstrates how similar the two\n",
    "graders are in shape.\n",
    "\n",
    "The code is in [grader.py](./grader.py), so we can import the module and\n",
    "use Python's native `inspect` module to dump it as source code into a\n",
    "string we pass to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an analagous Python Grader using the same code.\n",
    "import inspect\n",
    "import grader\n",
    "\n",
    "code = inspect.getsource(grader)\n",
    "\n",
    "PYTHON_GRADER = {\n",
    "    \"type\": \"python\",\n",
    "    \"name\": \"Python Countdown Grader\",\n",
    "    \"source\": code,\n",
    "    \"pass_threshold\": 5.0,\n",
    "}\n",
    "print(\"> Defined Python grader:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc68632",
   "metadata": {},
   "source": [
    "Let's now make sure our Endpoint Grader is reachable. If it's not, there's no sense in\n",
    "submitting an Eval or RFT job. We can POST some sample input and check if we get back\n",
    "a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually test the grader.\n",
    "import requests\n",
    "\n",
    "data = {\n",
    "    \"item\": {\"target\": 47, \"nums\": [86, 22, 88, 72]},\n",
    "    \"sample\": {\n",
    "        \"output_json\": {\n",
    "            \"expression\": \"(72 + 22) / (88 - 86)\",\n",
    "            \"result\": 47,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "result = requests.post(\n",
    "    URL,\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-Functions-Key\": os.getenv(\"X_FUNCTIONS_KEY\"),\n",
    "    },\n",
    "    json=data,\n",
    ")\n",
    "\n",
    "print(f\"> Got HTTP {result.status_code}\")\n",
    "if result.status_code == 200:\n",
    "    score = result.json()[\"score\"]\n",
    "    print(f\"> Score is {score}.\")\n",
    "    if score != 5:\n",
    "        raise RuntimeError(\"Uh oh...you might want to check your grader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c692cc",
   "metadata": {},
   "source": [
    "Now we're ready to define our Eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Eval\n",
    "baseline_eval = client.evals.create(\n",
    "    name=f\"endpoint-countdown-eval-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=EVAL_DATA_SOURCE,\n",
    "    # testing_criteria=[ENDPOINT_GRADER],\n",
    "    # testing_criteria=[PYTHON_GRADER],\n",
    "    testing_criteria=[PYTHON_GRADER, ENDPOINT_GRADER],\n",
    ")\n",
    "print(f\"> Created eval {baseline_eval.id}:\")\n",
    "print(baseline_eval.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Runs\n",
    "USER_PROMPT = \"\"\"\n",
    "target: {{item.target}}\n",
    "numbers: {{item.nums}}\n",
    "\"\"\".strip()\n",
    "\n",
    "RUNS = []\n",
    "for model in [\"gpt-4.1\", \"o4-mini\", \"gpt-5\"]:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": {\"type\": \"file_id\", \"id\": eval_file.id},\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"developer\",\n",
    "                    \"content\": {\"type\": \"input_text\", \"text\": DEVELOPER_PROMPT},\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": {\"type\": \"input_text\", \"text\": USER_PROMPT},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": {\n",
    "            \"response_format\": RESPONSE_SCHEMA,\n",
    "        },\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"endpoint-countdown-eval-run-{model}-{UNIQUE_ENOUGH_KEY}\",\n",
    "        eval_id=baseline_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE,\n",
    "    )\n",
    "    print(f\"> Created run {run.id}: {run.name}\")\n",
    "    RUNS.append(run)\n",
    "\n",
    "print(f\"> Created {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de97ef1",
   "metadata": {},
   "source": [
    "# Reinforcement Fine Tuning\n",
    "Now let's train a model to improve upon our out-of-box o4-mini experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611870dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training and validation data.\n",
    "with open(\"./data/countdown_train_100.jsonl\", mode=\"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "training_file = client.files.wait_for_processing(training_file.id)\n",
    "print(f\"> Uploaded training file {training_file.id}\")\n",
    "\n",
    "with open(\"./data/countdown_valid_50.jsonl\", mode=\"rb\") as f:\n",
    "    validation_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "validation_file = client.files.wait_for_processing(validation_file.id)\n",
    "print(f\"> Uploaded validation file {validation_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c17f95",
   "metadata": {},
   "source": [
    "We can drop the `pass_threshold` settings from the individual graders. We can set it on\n",
    "the `multi_grader` we'll use to combine both Python and Endpoint graders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our RFT job\n",
    "import copy\n",
    "\n",
    "# scrub pass thresholds.\n",
    "RFT_ENDPOINT_GRADER = copy.deepcopy(ENDPOINT_GRADER)\n",
    "if \"pass_threshold\" in RFT_ENDPOINT_GRADER:\n",
    "    del RFT_ENDPOINT_GRADER[\"pass_threshold\"]\n",
    "RFT_PYTHON_GRADER = copy.deepcopy(PYTHON_GRADER)\n",
    "if \"pass_threshold\" in RFT_PYTHON_GRADER:\n",
    "    del RFT_PYTHON_GRADER[\"pass_threshold\"]\n",
    "\n",
    "\n",
    "MULTI_GRADER = {\n",
    "    \"type\": \"multi\",\n",
    "    \"name\": \"Combined local and remote grader\",\n",
    "    \"graders\": {\n",
    "        \"python\": PYTHON_GRADER,\n",
    "        \"endpoint\": RFT_ENDPOINT_GRADER,\n",
    "    },\n",
    "    \"calculate_output\": \"(python + endpoint) / 2\",  # average the scores\n",
    "    \"pass_threshold\": 5,\n",
    "}\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    suffix=f\"dv-endpoint-countdown-ft-{UNIQUE_ENOUGH_KEY}\",\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    training_file=training_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    method={\n",
    "        \"type\": \"reinforcement\",\n",
    "        \"reinforcement\": {\n",
    "            # \"grader\": RFT_ENDPOINT_GRADER,\n",
    "            # \"grader\": PYTHON_GRADER,\n",
    "            \"grader\": MULTI_GRADER,\n",
    "            \"response_format\": RESPONSE_SCHEMA,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(f\"> Created RFT job {job.id}\")\n",
    "print(job.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
