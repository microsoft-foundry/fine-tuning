{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ecd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from scripts.eval_utils import AsyncEvalClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup evaluation client\n",
    "client = AsyncEvalClient()\n",
    "print(\"ðŸŽ‰ Azure OpenAI Evaluation Client ready!\")\n",
    "\n",
    "IMAGE_MODEL_DEPLOYMENT_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd49e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.image_utils import load_and_create_image_dataset, display_items\n",
    "\n",
    "# Load dataset from Hugging Face and create the image dataset for evaluation\n",
    "load_and_create_image_dataset(\"google-research-datasets/conceptual_captions\")\n",
    "\n",
    "# Display the created evaluation file\n",
    "display_items(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the evaluation file to Azure OpenAI\n",
    "eval_file_id = await client.upload_file(\n",
    "    file_name=\"image_emotion_evaluation.jsonl\",\n",
    "    file_path=\"./data/image_emotion_evaluation.jsonl\")\n",
    "print(f\"âœ… Eval file ID: {eval_file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = {\n",
    "      \"type\": \"score_model\",\n",
    "      \"name\": \"Image to Text Grader\",\n",
    "      \"model\": IMAGE_MODEL_DEPLOYMENT_NAME,\n",
    "      \"input\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are an expert grader. Judge how well the model response {{sample.output_text}} describes the image as well as matches the caption {{item.caption}}. Output a score of 1 if its an excelent match with both. If it's somewhat compatible, output a score around 0.5. Otherwise, give a score of 0.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \n",
    "                    \"type\": \"input_text\", \n",
    "                    \"text\": \"Caption: {{ item.caption }}\"\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"input_image\", \n",
    "                    \"image_url\": \"{{ item.image_url }}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "      ],\n",
    "      \"range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"pass_threshold\": 0.5\n",
    "    }\n",
    "\n",
    "eval_id = await client.create_eval_sdk(\n",
    "    name=\"Image Caption Evaluation\",\n",
    "    testing_criteria=[score_model],\n",
    "    data_source_config={\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"image_url\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The URL of the image to be evaluated.\"\n",
    "        },\n",
    "        \"caption\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The caption describing the image.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"image_url\",\n",
    "        \"caption\"\n",
    "      ]\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ae59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = {\n",
    "    \"type\": \"completions\",\n",
    "    \"model\": IMAGE_MODEL_DEPLOYMENT_NAME,\n",
    "    \"sampling_params\": {\n",
    "      \"temperature\": 0.8\n",
    "    },\n",
    "    \"source\": {\n",
    "      \"type\": \"file_id\",\n",
    "      \"id\": eval_file_id\n",
    "    },\n",
    "    \"input_messages\": {\n",
    "      \"type\": \"template\",\n",
    "      \"template\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are an assistant that analyzes images and provides captions that accurately describe the content of the image.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"type\": \"message\",\n",
    "          \"content\": {\n",
    "              \"type\": \"input_image\",\n",
    "              \"image_url\": \"{{ item.image_url }}\",\n",
    "              \"detail\": \"auto\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "}\n",
    "\n",
    "run = await client.create_eval_run_sdk(eval_id, \"Image Caption Evaluation\", data_source)\n",
    "run_id = run['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "while True:\n",
    "    run = await client.get_eval_run_sdk(eval_id=eval_id, run_id=run_id)\n",
    "    if run['status'] == \"completed\":\n",
    "        output_items_response = await client.get_eval_run_output_items_sdk(\n",
    "            eval_id=eval_id, run_id=run_id)\n",
    "\n",
    "        # Get the actual list of items from the response object\n",
    "        output_items = output_items_response.data if hasattr(output_items_response, 'data') else output_items_response\n",
    "        \n",
    "        print(\"Sample output item:\")\n",
    "        print(output_items[0])\n",
    "\n",
    "        # Create DataFrame with safe access to nested fields\n",
    "        df_data = {\n",
    "            \"id\": [],\n",
    "            \"grading_results\": [],\n",
    "            \"expected_caption\": [],\n",
    "            \"model_response\": []\n",
    "        }\n",
    "\n",
    "        for item in output_items:\n",
    "            # Convert Pydantic model to dict if needed\n",
    "            item_dict = item.model_dump() if hasattr(item, 'model_dump') else item\n",
    "            \n",
    "            df_data[\"id\"].append(item_dict.get(\"id\", \"N/A\"))\n",
    "            df_data[\"grading_results\"].append(item_dict.get(\"status\", \"N/A\"))\n",
    "            \n",
    "            # Safely get expected caption\n",
    "            datasource_item = item_dict.get('datasource_item', {})\n",
    "            df_data[\"expected_caption\"].append(datasource_item.get(\"caption\", \"N/A\"))\n",
    "            \n",
    "            # Check if audio output exists\n",
    "            sample = item_dict.get(\"sample\", {})\n",
    "            output = sample.get(\"output\", {})\n",
    "            output_transcript = output[0].get(\"content\")\n",
    "            df_data[\"model_response\"].append(output_transcript)\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        display(df)\n",
    "        break\n",
    "    if run['status'] == \"failed\":\n",
    "        print(\"Evaluation run failed:\")\n",
    "        print(run.get('error', 'Unknown error'))\n",
    "        break\n",
    "    print(f\"Status: {run['status']}. Waiting...\")\n",
    "    await asyncio.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
