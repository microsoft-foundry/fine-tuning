# Reinforcement Fine-Tuning with OpenR1-Math-220k Dataset

This cookbook demonstrates how to fine-tune language models using **Reinforcement Fine-Tuning (RFT)** with the OpenR1-Math-220k dataset on Azure AI. This dataset contains 220,000 advanced mathematical reasoning problems with verified step-by-step solutions, making it ideal for teaching models complex mathematical problem-solving.

## Overview

Reinforcement Fine-Tuning (RFT) is a powerful technique for training language models on tasks where:
- Multiple valid solution paths exist
- Correctness can be verified automatically
- Step-by-step reasoning is crucial
- The task requires structured, multi-hop thinking

This cookbook uses the **OpenR1-Math-220k dataset**, which contains advanced mathematics problems from college-level and competition mathematics with 2-4 verified reasoning traces per problem generated by DeepSeek R1. The problems cover diverse mathematical domains and require detailed chain-of-thought reasoning.

## Dataset Information

**Source**: [OpenR1-Math-220k on Kaggle](https://www.kaggle.com/datasets/alejopaullier/openr1-math-220k) | [Hugging Face](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)

**Dataset Statistics**:
- **Full dataset**: 220,000 mathematical reasoning problems (7 parquet files, 1.45 GB total)
- **Source parquet**: train-00000-of-00007.parquet (13,391 problems with 27,614 verified reasoning traces)
- **Training set**: 2,725 examples (45 MB)
- **Validation set**: 575 examples (10 MB)

> **Important**: 
> - ALL data is REAL - extracted directly from parquet files with NO synthetic additions
> - Each problem has 2-4 verified reasoning traces; we use multiple traces for improved RFT learning


**What the Data Contains**:
The OpenR1-Math-220k dataset consists of advanced mathematical problems with verified solutions. Each problem includes:
- **Problem statement**: Complex mathematics question requiring multi-step reasoning
- **Reasoning traces**: 2-4 verified step-by-step solutions showing detailed mathematical thinking
- **Final answer**: Solution clearly marked in `\boxed{}` format
- **Verification status**: 88% verified using Math Verify tool, 12% verified using Llama-3.3-70B-Instruct

**Problem Domains Covered**:
- Algebra and polynomial factorization
- Calculus and optimization
- Probability theory and expected values
- Geometry and trigonometry
- Number theory and combinatorics
- Linear algebra and matrix operations
- Complex numbers and abstract mathematics

**Task Complexity**: 
- Advanced multi-step mathematical reasoning
- Problems require abstract algebraic manipulation
- Multi-hop logical reasoning (typically 5-15 reasoning steps)
- Advanced mathematical concepts beyond basic arithmetic
- Solutions averaging 8k tokens, complex problems up to 16k tokens

**Why RFT is Perfect for This Task**:
- **Multiple valid solution paths**: Many ways to solve each problem
- **Verifiable correctness**: Mathematical answers can be automatically checked
- **Structured reasoning**: Benefits from learning multiple verified approaches
- **Quality over memorization**: Model learns reasoning patterns, not just answers

## What You'll Learn

This cookbook teaches you how to:

1. Download and prepare the OpenR1-Math-220k dataset from Kaggle
2. Convert Parquet files to JSONL format for Microsoft Foundry fine-tuning
3. Set up your Azure AI environment for RFT
4. Create a grading function to evaluate mathematical reasoning quality
5. Configure and launch an RFT fine-tuning job
6. Monitor training progress and model performance
7. Deploy and test your fine-tuned mathematical reasoning model

## Prerequisites

- Azure subscription with Microsoft Foundry project (requires **Azure AI User** role)
- Python 3.9 or higher
- Familiarity with Jupyter notebooks
- Kaggle account to download the OpenR1-Math-220k dataset
- Understanding of basic mathematical concepts

## Supported Models

RFT in Azure AI Foundry supports the following models:

- **o4-mini**
- **gpt-5 (PrPr)**


> **Note**: Model availability may vary by region. Check the [Azure OpenAI model availability](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) page for current regional support.

## Files in This Cookbook

- **README.md**: This file - comprehensive documentation
- **requirements.txt**: Python dependencies required for the cookbook
- **rft_math_reasoning.ipynb**: Step-by-step notebook implementation
- **training.jsonl**: Training dataset
- **validation.jsonl**: Validation dataset

## Quick Start


### 1. Install Dependencies

```powershell
pip install -r requirements.txt
```

### 2. Prepare the Dataset

You can use the training and validation dataset as is from this directory or you can prepare your own dataset also.

### 3. Set Up Environment Variables

Create a `.env` file in the root of this directory with your Azure credentials:

```env
# Required for RFT Fine-Tuning
AZURE_AI_PROJECT_ENDPOINT=<your-endpoint>
AZURE_SUBSCRIPTION_ID=<your-subscription-id>
AZURE_RESOURCE_GROUP=<your-resource-group>
AZURE_AOAI_ACCOUNT=<your-foundry-account-name>
MODEL_NAME=<your-base-model-name>
```

### 4. Run the Notebook

Open `rft_math_reasoning.ipynb` and follow the step-by-step instructions to:
- Set up the grading function for mathematical correctness
- Launch RFT training with optimal hyperparameters
- Monitor training progress
- Deploy and test your fine-tuned model

## Dataset Format

The RFT format for mathematical reasoning follows this structure:

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a mathematical reasoning expert. Solve problems with detailed step-by-step thinking and provide final answers in \\boxed{} format."
    },
    {
      "role": "user",
      "content": "The numbers 2, 3, 5, 7, 11, 13 are arranged in a multiplication table with three along the top and three down the left. What is the largest possible sum of the nine entries?"
    },
    {
      "role": "assistant",
      "content": "[Detailed step-by-step reasoning chain showing mathematical thinking, intermediate calculations, and final answer in \\boxed{} format]"
    }
  ]
}
```

Each training example contains:
- **system message**: Instructions for mathematical problem-solving behavior
- **user message**: The mathematical problem statement
- **assistant message**: One of 2-4 verified reasoning traces with step-by-step solution

## Training Configuration

Recommended hyperparameters for RFT with mathematical reasoning:

- **Model**: o4-mini
- **Epochs**: 2-3 (mathematical reasoning benefits from focused training)
- **Batch Size**: 1-2 (long reasoning chains require careful processing)
- **Learning Rate Multiplier**: 0.5-1.0 (conservative for preserving reasoning ability)
- **Max Tokens**: 16384 (accommodate long reasoning chains)

These can be adjusted based on your computational resources and specific requirements.

## Grading Mathematical Solutions

The grading function for RFT evaluates:

1. **Answer Correctness**: Final answer matches expected result
2. **Reasoning Quality**: Logical progression and mathematical validity
3. **Format Compliance**: Proper use of `\boxed{}` for final answer
4. **Completeness**: All required steps shown with explanations

The grader assigns rewards (0.0 to 1.0) based on these criteria, allowing the model to learn from better reasoning paths.

## Expected Outcomes

After fine-tuning with RFT on OpenR1-Math-220k, your model should:

- Generate detailed step-by-step mathematical reasoning chains
- Solve college-level and competition mathematics problems
- Handle diverse mathematical domains (algebra, calculus, geometry, etc.)
- Produce properly formatted answers with `\boxed{}` notation
- Demonstrate improved logical reasoning and problem decomposition
- Show better performance on multi-hop mathematical reasoning compared to base model
- Generate solutions with clear intermediate steps and explanations


## Additional Resources

- [Azure OpenAI Fine-Tuning Documentation](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning)
- [OpenR1-Math-220k dataset](https://www.kaggle.com/datasets/alejopaullier/openr1-math-220k)

---

**Note**: This is an advanced fine-tuning task requiring significant computational resources. Ensure your Azure subscription has adequate quota for fine-tuning large language models with long context lengths.
