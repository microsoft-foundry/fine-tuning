{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Fine-Tuning (RFT) with OpenR1-Math-220k Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models using **Reinforcement Fine-Tuning (RFT)** with the OpenR1-Math-220k dataset - a collection of 220,000 advanced mathematical reasoning problems with verified step-by-step solutions.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Understand reinforcement fine-tuning (RFT) and how it differs from supervised fine-tuning (SFT)\n",
    "2. Define a grader/reward function for mathematical reasoning\n",
    "3. Prepare and upload mathematical reasoning datasets\n",
    "4. Create and configure an RFT job using the OpenAI method\n",
    "5. Monitor training progress and evaluate model performance\n",
    "6. Deploy and test your RFT fine-tuned model\n",
    "\n",
    "**Note**: Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install all required packages from requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-projects>=2.0.0b1 (from -r requirements.txt (line 2))\n",
      "  Using cached azure_ai_projects-2.0.0b3-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting openai (from -r requirements.txt (line 5))\n",
      "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting azure-identity (from -r requirements.txt (line 8))\n",
      "  Using cached azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 12))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.35.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_storage_blob-12.28.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai->-r requirements.txt (line 5))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\work\\amlrepos\\rftcookbook\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.15.0)\n",
      "Collecting cryptography>=2.5 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting msrest>=0.7.1 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-mgmt-core>=1.6.0 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\work\\amlrepos\\rftcookbook\\fine-tuning\\env\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Using cached azure_ai_projects-2.0.0b3-py3-none-any.whl (240 kB)\n",
      "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
      "Using cached azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
      "Using cached azure_storage_blob-12.28.0-py3-none-any.whl (431 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 30.7/131.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 92.2/131.6 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 131.6/131.6 kB 970.8 kB/s eta 0:00:00\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: urllib3, typing-inspection, tqdm, sniffio, python-dotenv, PyJWT, pydantic-core, pycparser, oauthlib, jiter, isodate, idna, h11, distro, charset_normalizer, certifi, annotated-types, requests, pydantic, httpcore, cffi, anyio, requests-oauthlib, httpx, cryptography, azure-core, openai, msrest, azure-storage-blob, azure-mgmt-core, msal, azure-mgmt-cognitiveservices, msal-extensions, azure-identity, azure-ai-projects\n",
      "Successfully installed PyJWT-2.10.1 annotated-types-0.7.0 anyio-4.12.1 azure-ai-projects-2.0.0b3 azure-core-1.37.0 azure-identity-1.25.1 azure-mgmt-cognitiveservices-14.1.0 azure-mgmt-core-1.6.0 azure-storage-blob-12.28.0 certifi-2026.1.4 cffi-2.0.0 charset_normalizer-3.4.4 cryptography-46.0.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 isodate-0.7.2 jiter-0.12.0 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 oauthlib-3.3.1 openai-2.14.0 pycparser-2.23 pydantic-2.12.5 pydantic-core-2.41.5 python-dotenv-1.2.1 requests-2.32.5 requests-oauthlib-2.0.0 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2 urllib3-2.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Azure Environment\n",
    "\n",
    "Set your Microsoft Foundry Project endpoint, model name and other environment variables. We're using **o4-mini** in this example for RFT.\n",
    "\n",
    "Create a .env file with:\n",
    "\n",
    "```\n",
    "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint>\n",
    "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
    "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
    "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
    "MODEL_NAME=o4-mini\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_name = os.environ.get(\"MODEL_NAME\", \"o4-mini\")\n",
    "\n",
    "# Define RFT dataset file paths\n",
    "training_file_path = \"training_rft.jsonl\"\n",
    "validation_file_path = \"validation_rft.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to Microsoft Foundry Project\n",
    "\n",
    "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows.\n",
    "\n",
    "**Important**: Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Microsoft Foundry Project\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "openai_client = project_client.get_openai_client()\n",
    "\n",
    "print(\"Connected to Microsoft Foundry Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Mathematical Grader for RFT\n",
    "\n",
    "Reinforcement Fine-Tuning (RFT) requires a grader function to evaluate model outputs. Unlike SFT which learns from examples, RFT learns from reward signals.\n",
    "\n",
    "For mathematical reasoning, we use a **Python-based grader** that deterministically verifies the correctness of the final answer by:\n",
    "1. Extracting the answer from the model's response using `\\\\boxed{}` notation\n",
    "2. Comparing it against the reference answer from the training data\n",
    "3. Returning a score of 1.0 for correct answers and 0.0 for incorrect ones\n",
    "\n",
    "This approach is more appropriate for math problems than LLM-based scoring, as mathematical correctness is objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python RFT grader configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Python-based grader for Azure OpenAI RFT\n",
    "# Compares final answers using \\\\boxed{} notation only\n",
    "\n",
    "grading_function = \"\"\"import re\n",
    "\n",
    "def normalize(ans: str):\n",
    "    try:\n",
    "        if not isinstance(ans, str):\n",
    "            return []\n",
    "        parts = re.split(r\"[,\\s]+\", ans.strip())\n",
    "        return sorted(p for p in parts if p)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_model_answer(text: str):\n",
    "    try:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        pattern = r\"\\\\\\\\boxed\\\\{\"\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        if not matches:\n",
    "            return \"\"\n",
    "        \n",
    "        last_match = matches[-1]\n",
    "        start = last_match.end()\n",
    "        \n",
    "        brace_count = 1\n",
    "        i = start\n",
    "        while i < len(text) and brace_count > 0:\n",
    "            if text[i] == '{':\n",
    "                brace_count += 1\n",
    "            elif text[i] == '}':\n",
    "                brace_count -= 1\n",
    "            i += 1\n",
    "        \n",
    "        if brace_count == 0:\n",
    "            return text[start:i-1].strip()\n",
    "        \n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def grade(sample, item):\n",
    "    try:\n",
    "        # Get model output - handle both dict and object access\n",
    "        if isinstance(sample, dict):\n",
    "            output_text = sample.get(\"output_text\", \"\") or sample.get(\"output_json\", \"\")\n",
    "        else:\n",
    "            output_text = getattr(sample, \"output_text\", \"\") or getattr(sample, \"output_json\", \"\")\n",
    "        \n",
    "        # Get reference answer\n",
    "        if isinstance(item, dict):\n",
    "            ref_raw = item.get(\"answer\", \"\")\n",
    "        else:\n",
    "            ref_raw = getattr(item, \"answer\", \"\")\n",
    "        \n",
    "        # Handle None or empty values\n",
    "        if not output_text:\n",
    "            return 0.0\n",
    "        if not ref_raw:\n",
    "            return 0.0\n",
    "            \n",
    "        # Convert output_json to string if it's a dict/object\n",
    "        if isinstance(output_text, dict):\n",
    "            output_text = str(output_text)\n",
    "        \n",
    "        pred_raw = extract_model_answer(str(output_text))\n",
    "        \n",
    "        if not pred_raw:\n",
    "            return 0.0\n",
    "\n",
    "        pred = normalize(pred_raw)\n",
    "        ref = normalize(str(ref_raw))\n",
    "\n",
    "        return 1.0 if pred == ref else 0.0\n",
    "    except Exception:\n",
    "        # Always return 0.0 on any error to prevent job failure\n",
    "        return 0.0\n",
    "\"\"\"\n",
    "\n",
    "grader = {\n",
    "    \"type\": \"python\",\n",
    "    \"source\": grading_function\n",
    "}\n",
    "\n",
    "print(\"Python RFT grader configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Test Grader Function (Optional)\n",
    "\n",
    "Test the grader locally with a sample from the training data to verify it works correctly before submitting the RFT job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS | Simple numeric answer - CORRECT\n",
      "PASS | Simple numeric answer - WRONG\n",
      "PASS | Missing \\boxed{} - should FAIL even if answer is correct\n",
      "PASS | Multiple \\boxed{} - uses LAST one\n",
      "PASS | Comma-separated values - order doesn't matter\n",
      "PASS | Space-separated values - normalized\n",
      "PASS | LaTeX expression\n",
      "PASS | Empty \\boxed{}\n",
      "ALL TESTS PASSED\n"
     ]
    }
   ],
   "source": [
    "exec(grading_function, globals())\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Simple numeric answer - CORRECT\",\n",
    "        \"sample\": {\"output_text\": \"The final answer is \\\\boxed{42}\"},\n",
    "        \"item\": {\"answer\": \"42\"},\n",
    "        \"expected\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Simple numeric answer - WRONG\",\n",
    "        \"sample\": {\"output_text\": \"The final answer is \\\\boxed{99}\"},\n",
    "        \"item\": {\"answer\": \"42\"},\n",
    "        \"expected\": 0.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Missing \\\\boxed{} - should FAIL even if answer is correct\",\n",
    "        \"sample\": {\"output_text\": \"The answer is 42\"},\n",
    "        \"item\": {\"answer\": \"42\"},\n",
    "        \"expected\": 0.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Multiple \\\\boxed{} - uses LAST one\",\n",
    "        \"sample\": {\"output_text\": \"First: \\\\boxed{wrong}, Final: \\\\boxed{42}\"},\n",
    "        \"item\": {\"answer\": \"42\"},\n",
    "        \"expected\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Comma-separated values - order doesn't matter\",\n",
    "        \"sample\": {\"output_text\": \"Answer: \\\\boxed{3, 2, 1}\"},\n",
    "        \"item\": {\"answer\": \"1, 2, 3\"},\n",
    "        \"expected\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Space-separated values - normalized\",\n",
    "        \"sample\": {\"output_text\": \"Answer: \\\\boxed{9 6 4}\"},\n",
    "        \"item\": {\"answer\": \"4, 6, 9\"},\n",
    "        \"expected\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LaTeX expression\",\n",
    "        \"sample\": {\"output_text\": \"Solution: \\\\boxed{\\\\sqrt{2}}\"},\n",
    "        \"item\": {\"answer\": \"\\\\sqrt{2}\"},\n",
    "        \"expected\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Empty \\\\boxed{}\",\n",
    "        \"sample\": {\"output_text\": \"Answer: \\\\boxed{}\"},\n",
    "        \"item\": {\"answer\": \"42\"},\n",
    "        \"expected\": 0.0\n",
    "    }\n",
    "]\n",
    "all_passed = True\n",
    "\n",
    "for test in test_cases:\n",
    "    score = grade(test[\"sample\"], test[\"item\"])\n",
    "    passed = (score == test[\"expected\"])\n",
    "    all_passed = all_passed and passed\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"{status} | {test['name']}\")\n",
    "    if not passed:\n",
    "        print(f\"       Expected: {test['expected']}, Got: {score}\")\n",
    "if all_passed:\n",
    "    print(\"ALL TESTS PASSED\")\n",
    "else:\n",
    "    print(\"SOME TESTS FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Training Files\n",
    "\n",
    "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Uploading validation file...\n",
      "Training file ID: file-ef920ef1469b4750971a7601ec5f9189\n",
      "Validation file ID: file-2d11b1d6f7414ff996c9bc9c25382927\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading training file...\")\n",
    "with open(training_file_path, \"rb\") as f:\n",
    "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(\"Uploading validation file...\")\n",
    "with open(validation_file_path, \"rb\") as f:\n",
    "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "train_file_id = train_file.id\n",
    "val_file_id = validation_file.id\n",
    "\n",
    "print(f\"Training file ID: {train_file_id}\")\n",
    "print(f\"Validation file ID: {val_file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wait for File Processing\n",
    "\n",
    "Microsoft Foundry needs to process the uploaded files before they can be used for fine-tuning. This step ensures the files are validated and ready for the RFT job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for files to be processed...\n",
      "Files ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for files to be processed...\")\n",
    "openai_client.files.wait_for_processing(train_file_id)\n",
    "openai_client.files.wait_for_processing(val_file_id)\n",
    "print(\"Files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Reinforcement Fine-Tuning Job\n",
    "\n",
    "Create a reinforcement fine-tuning job with your uploaded datasets and grader function. Configure hyperparameters to control the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Reinforcement Fine-Tuning job...\n",
      "Fine-tuning job created!\n",
      "Job ID: ftjob-927c273b843b483a9b4512204cd01c39\n",
      "Status: pending\n",
      "Model: o4-mini-2025-04-16\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Reinforcement Fine-Tuning job...\")\n",
    "\n",
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    model=model_name,\n",
    "    method={\n",
    "        \"type\": \"reinforcement\",\n",
    "        \"reinforcement\": {\n",
    "            \"grader\": grader,\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 2,\n",
    "                \"batch_size\": 1,\n",
    "                \"learning_rate_multiplier\": 1.0,\n",
    "                \"eval_interval\": 5,\n",
    "                \"eval_samples\": 2,\n",
    "                \"reasoning_effort\": \"medium\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    extra_body={\n",
    "        \"trainingType\": \"Standard\"\n",
    "    },\n",
    "    suffix=\"math-reasoning-rft\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created!\")\n",
    "print(f\"Job ID: {fine_tuning_job.id}\")\n",
    "print(f\"Status: {fine_tuning_job.status}\")\n",
    "print(f\"Model: {fine_tuning_job.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitor Training Progress\n",
    "\n",
    "Track the status of your fine-tuning job. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "print(f\"Status: {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieve Fine-Tuned Model\n",
    "\n",
    "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "\n",
    "if completed_job.status == \"succeeded\":\n",
    "    fine_tuned_model = completed_job.fine_tuned_model\n",
    "    print(f\"Fine-tuned Model ID: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(f\"Status: {completed_job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Deploy Fine-Tuned Model\n",
    "\n",
    "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
    "\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
    "\n",
    "deployment_name = \"o4-mini-math-reasoning-rft\"\n",
    "\n",
    "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
    "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model, version=\"1\")\n",
    "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
    "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=200)\n",
    "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
    "    \n",
    "    print(f\"Deploying fine-tuned model: {fine_tuned_model}\")\n",
    "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "        resource_group_name=resource_group,\n",
    "        account_name=account_name,\n",
    "        deployment_name=deployment_name,\n",
    "        deployment=deployment_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting for deployment to complete...\")\n",
    "    deployment.result()\n",
    "\n",
    "print(f\"Model deployment completed: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Fine-Tuned Model\n",
    "\n",
    "Test your fine-tuned model by solving a mathematical reasoning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_problem = \"\"\"Solve for x: 3x + 7 = 22\"\"\"\n",
    "\n",
    "print(\"Testing fine-tuned model...\")\n",
    "print(f\"Problem:{test_problem}\")\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model=deployment_name,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a mathematical reasoning assistant. Solve problems step-by-step, showing your work clearly. Provide the final answer in \\\\boxed{} format.\"},\n",
    "        {\"role\": \"user\", \"content\": test_problem}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Model Response:{response.output_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
