{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4098466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements-demo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd5785",
   "metadata": {},
   "source": [
    "# Setup\n",
    "You should have a local `.env` file with at least:\n",
    "* `X_FUNCTIONS_KEY`\n",
    "* `AZURE_FUNCTIONS_ENDPOINT`\n",
    "* `AZURE_OPENAI_API_KEY`\n",
    "* `AZURE_OPENAI_ENDPOINT`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178405ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a1c62",
   "metadata": {},
   "source": [
    "For now, we're using an API key. We configure the non-Azure OpenAI client to talk to the OpenAI/v1 API in Foundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d588d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Created OpenAI client.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "print(\"> Created OpenAI client.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6353d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Using unique enough key: f83f16a5\n"
     ]
    }
   ],
   "source": [
    "# We'll create a \"unique enough\" identifier that lets us run this notebook\n",
    "# multiple times and easily keep track of things each run creates.\n",
    "import uuid\n",
    "\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]\n",
    "print(f\"> Using unique enough key: {UNIQUE_ENOUGH_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d5c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_PROMPT = \"\"\"\n",
    "You are an expert in arithmetic problem solving. Given a target number and a list of\n",
    "numbers, your task is to combine all of the numbers exactly once using addition (+),\n",
    "subtraction (-), multiplication (x), or division (/) to reach the target.\n",
    "\n",
    "- You must use every number exactly once.\n",
    "- Use parentheses as needed to control the order of operations.\n",
    "- Return only a valid JSON object with the following exact syntax:\n",
    "  {\n",
    "    \"expression\": <the expression>,\n",
    "    \"result\": <the result>\n",
    "  }\n",
    "- The expression should be a string representation of the mathematical expression.\n",
    "- The result should be an integer.\n",
    "- If the exact target is not possible, return the closest valid result using all numbers.\n",
    "\n",
    "# Example 1\n",
    "target: 850\n",
    "numbers: [100, 75, 50, 25, 6, 3]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((100 * 6) + (75 + 25) + (3 * 50))\",\n",
    "  \"result\": 850\n",
    "}\n",
    "\n",
    "# Example 2\n",
    "target: 945\n",
    "numbers: [25, 100, 9, 3, 6, 2]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((100 * 9) + (6 * 3) + (25 + 2))\",\n",
    "  \"result\": 945\n",
    "}\n",
    "\n",
    "# Example 3\n",
    "target: 310\n",
    "numbers: [7, 50, 75, 3, 8, 2]\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"expression\": \"((75 * 3) + (50 * 2) - (8 + 7))\",\n",
    "  \"result\": 310\n",
    "}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b731b",
   "metadata": {},
   "source": [
    "# Baseline Evaluation\n",
    "We're first going to assess how some models perform natively at this task given the above prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317250db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Uploaded eval file file-1aRH3LHGE7oHQRsmZxkXMi\n"
     ]
    }
   ],
   "source": [
    "# Upload our datasets for eval\n",
    "\n",
    "with open(\"./data/countdown_eval_25.jsonl\", mode=\"rb\") as f:\n",
    "    eval_file = client.files.create(file=f, purpose=\"evals\")\n",
    "eval_file = client.files.wait_for_processing(eval_file.id)\n",
    "\n",
    "print(f\"> Uploaded eval file {eval_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b273004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to describe the source and schema for our files.\n",
    "EVAL_DATA_SOURCE = {\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"target\": {\"type\": \"integer\"},\n",
    "            \"nums\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n",
    "        },\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "    \"type\": \"custom\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace46dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our expected Response schema from the models. We'll use structured outputs\n",
    "# so the model must output only JSON.\n",
    "RESPONSE_SCHEMA = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_expression\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\"type\": \"string\"},\n",
    "                \"result\": {\"type\": \"integer\"},\n",
    "            },\n",
    "            \"required\": [\"expression\", \"result\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e7cb1",
   "metadata": {},
   "source": [
    "## Endpoint Grader\n",
    "Our endpoint grader points to a remote endpoint that hosts the grader function.\n",
    "\n",
    "It has an API surface that's basically the Python grader, but via POST'ing JSON\n",
    "to an endpoint.\n",
    "\n",
    "We're using an Azure Function with key based auth, so each request will need\n",
    "the `X-Functions-Key` header with our authentication key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a19c0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Defined endpoint grader for https://countdown-endpoint-demo.azurewebsites.net/api/grader\n"
     ]
    }
   ],
   "source": [
    "# First we'll define our Grader.\n",
    "import os\n",
    "\n",
    "URL = os.getenv(\"AZURE_FUNCTIONS_ENDPOINT\")\n",
    "ENDPOINT_GRADER = {\n",
    "    \"type\": \"endpoint\",\n",
    "    \"name\": \"Remote Countdown Grader\",\n",
    "    \"url\": URL,\n",
    "    \"headers\": {\n",
    "        \"X-Functions-Key\": os.getenv(\"X_FUNCTIONS_KEY\"),\n",
    "    },\n",
    "    \"pass_threshold\": 5.0,\n",
    "    # \"rate_limit\": 50,\n",
    "}\n",
    "print(f\"> Defined endpoint grader for {URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc68632",
   "metadata": {},
   "source": [
    "Let's now make sure our Endpoint Grader is reachable. If it's not, there's no sense in\n",
    "submitting an Eval or RFT job. We can POST some sample input and check if we get back\n",
    "a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c3874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Got HTTP 200\n",
      "> Score is 5.\n"
     ]
    }
   ],
   "source": [
    "# Manually test the grader.\n",
    "import requests\n",
    "\n",
    "data = {\n",
    "    \"item\": {\"target\": 47, \"nums\": [86, 22, 88, 72]},\n",
    "    \"sample\": {\n",
    "        \"output_json\": {\n",
    "            \"expression\": \"(72 + 22) / (88 - 86)\",\n",
    "            \"result\": 47,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "result = requests.post(\n",
    "    URL,\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-Functions-Key\": os.getenv(\"X_FUNCTIONS_KEY\"),\n",
    "    },\n",
    "    json=data,\n",
    ")\n",
    "\n",
    "print(f\"> Got HTTP {result.status_code}\")\n",
    "if result.status_code == 200:\n",
    "    score = result.json()[\"score\"]\n",
    "    print(f\"> Score is {score}.\")\n",
    "    if score != 5:\n",
    "        raise RuntimeError(\"Uh oh...you might want to check your grader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f531e2b",
   "metadata": {},
   "source": [
    "For comparison, we'll use the same code but run as a Python grader.\n",
    "\n",
    "In practice, this makes no sense...but it demonstrates how similar the two\n",
    "graders are in shape.\n",
    "\n",
    "The code is in [grader.py](./grader.py), so we can import the module and\n",
    "use Python's native `inspect` module to dump it as source code into a\n",
    "string we pass to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0873b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Defined Python grader\n",
      "{'name': 'Python Countdown Grader',\n",
      " 'pass_threshold': 5.0,\n",
      " 'source': '\"\"\"\\n'\n",
      "           'Python Grader logic for OpenAI Evals\\n'\n",
      "           '\\n'\n",
      "           'Example from '\n",
      "           'https://github.com/azure-ai-foundry/fine-tuning/blob/main/Demos/RFT_Countdown/demo_with_python_grader.ipynb\\n'\n",
      "           '\"\"\"\\n'\n",
      "           'from typing import Any, Dict\\n'\n",
      "           'import ast\\n'\n",
      "           'import json\\n'\n",
      "           'import logging\\n'\n",
      "           'import re\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def _eval(n):\\n'\n",
      "           '    if isinstance(n, ast.Constant):\\n'\n",
      "           '        return n.value\\n'\n",
      "           '\\n'\n",
      "           '    if isinstance(n, ast.BinOp) and type(n.op) in {\\n'\n",
      "           '        ast.Add: lambda a, b: a + b,\\n'\n",
      "           '        ast.Sub: lambda a, b: a - b,\\n'\n",
      "           '        ast.Mult: lambda a, b: a * b,\\n'\n",
      "           '        ast.Div: lambda a, b: a / b,\\n'\n",
      "           '        ast.FloorDiv: lambda a, b: a // b,\\n'\n",
      "           '        ast.Mod: lambda a, b: a % b,\\n'\n",
      "           '        ast.Pow: lambda a, b: a**b,\\n'\n",
      "           '    }:\\n'\n",
      "           '        return {\\n'\n",
      "           '            ast.Add: lambda a, b: a + b,\\n'\n",
      "           '            ast.Sub: lambda a, b: a - b,\\n'\n",
      "           '            ast.Mult: lambda a, b: a * b,\\n'\n",
      "           '            ast.Div: lambda a, b: a / b,\\n'\n",
      "           '            ast.FloorDiv: lambda a, b: a // b,\\n'\n",
      "           '            ast.Mod: lambda a, b: a % b,\\n'\n",
      "           '            ast.Pow: lambda a, b: a**b,\\n'\n",
      "           '        }[type(n.op)](_eval(n.left), _eval(n.right))\\n'\n",
      "           '\\n'\n",
      "           '    if isinstance(n, ast.UnaryOp) and type(n.op) in {\\n'\n",
      "           '        ast.UAdd: lambda a: +a,\\n'\n",
      "           '        ast.USub: lambda a: -a,\\n'\n",
      "           '    }:\\n'\n",
      "           '        return {\\n'\n",
      "           '            ast.UAdd: lambda a: +a,\\n'\n",
      "           '            ast.USub: lambda a: -a,\\n'\n",
      "           '        }[\\n'\n",
      "           '            type(n.op)\\n'\n",
      "           '        ](_eval(n.operand))\\n'\n",
      "           '\\n'\n",
      "           '    raise ValueError(\"bad expr\")\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def _safe_eval(e):\\n'\n",
      "           '    return _eval(ast.parse(e, mode=\"eval\").body)\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def grade(sample: Dict[str, Any], item: Dict[str, Any]) -> float:\\n'\n",
      "           '    \"\"\"\\n'\n",
      "           '    Implements the OpenAI Python Grader API, grading the given '\n",
      "           'sample in the\\n'\n",
      "           '    context of the provided item.\\n'\n",
      "           '\\n'\n",
      "           '    Returns a score in all cases, with 0 returned in case of '\n",
      "           'error.\\n'\n",
      "           '    \"\"\"\\n'\n",
      "           '    # Be flexible about where our output comes from, but it *must* '\n",
      "           'be JSON.\\n'\n",
      "           '    output = {}\\n'\n",
      "           '    if \"output_json\" in sample:\\n'\n",
      "           '        output = sample[\"output_json\"]\\n'\n",
      "           '    else:\\n'\n",
      "           '        # fallback to output_text but it better be JSON!\\n'\n",
      "           '        try:\\n'\n",
      "           '            output = json.loads(sample[\"output_text\"])\\n'\n",
      "           '        except Exception as e:\\n'\n",
      "           '            logging.error(\"failed to find JSON output in '\n",
      "           'output_json and output_text: {e}\")\\n'\n",
      "           '            return 0\\n'\n",
      "           '    if not output:\\n'\n",
      "           '        logging.error(\"failed to find non-null JSON output in '\n",
      "           'sample\")\\n'\n",
      "           '        return 0\\n'\n",
      "           '    \\n'\n",
      "           '    # Extract our Expression and Result\\n'\n",
      "           '    expr = output.get(\"expression\")\\n'\n",
      "           '    if not expr:\\n'\n",
      "           '        logging.warning(\"expression was empty\")\\n'\n",
      "           '        return 0\\n'\n",
      "           '    result = output.get(\"result\")\\n'\n",
      "           '    if not result:\\n'\n",
      "           '        logging.warning(\"result was empty\")\\n'\n",
      "           '        return 0\\n'\n",
      "           '\\n'\n",
      "           '    # Perform the evaluation.\\n'\n",
      "           '    try:\\n'\n",
      "           '        expr_val = _safe_eval(expr)\\n'\n",
      "           '    except ValueError as e:\\n'\n",
      "           '        logging.error(f\"value error evaluating expression: {e}\")\\n'\n",
      "           '        return 0\\n'\n",
      "           '    \\n'\n",
      "           '    # TODO: handle parsing item more cleanly.\\n'\n",
      "           '    try:\\n'\n",
      "           '        # Check if all numbers were used.\\n'\n",
      "           '        used = sorted(map(int, re.findall(r\"-?\\\\d+\", expr)))\\n'\n",
      "           '        expected = sorted(map(int, item[\"nums\"]))\\n'\n",
      "           '        if used != expected:\\n'\n",
      "           '            logging.info(\"all numbers were not used exactly '\n",
      "           'once\")\\n'\n",
      "           '            return 0\\n'\n",
      "           '\\n'\n",
      "           '        sr = int(float(result))\\n'\n",
      "           '        it = int(float(item[\"target\"]))\\n'\n",
      "           '\\n'\n",
      "           '        # Score function\\n'\n",
      "           '        if expr_val != sr:\\n'\n",
      "           '            return 1\\n'\n",
      "           '        if sr == it:\\n'\n",
      "           '            return 5\\n'\n",
      "           '        if abs(sr - it) <= 1:\\n'\n",
      "           '            return 4\\n'\n",
      "           '        if abs(sr - it) <= 5:\\n'\n",
      "           '            return 3\\n'\n",
      "           '        return 2\\n'\n",
      "           '    except Exception as e:\\n'\n",
      "           '        logging.error(f\"exception while grading: {e}\")\\n'\n",
      "           '    return 0\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'if __name__ == \"__main__\":\\n'\n",
      "           '    import json\\n'\n",
      "           '    data = {}\\n'\n",
      "           '    try:\\n'\n",
      "           '        with open(\"./test.json\", mode=\"rb\") as f:\\n'\n",
      "           '            data = json.load(f)\\n'\n",
      "           '        sample, item = data[\"sample\"], data[\"item\"]\\n'\n",
      "           '        \\n'\n",
      "           '        print(f\"grading with:\\\\n\\\\tsample: {sample}\\\\n\\\\titem: '\n",
      "           '{item}\")\\n'\n",
      "           '        score = grade(sample, item)\\n'\n",
      "           '        print(f\"score: {score}\")\\n'\n",
      "           '    except Exception as e:\\n'\n",
      "           '        import sys\\n'\n",
      "           '        print(e, file=sys.stderr)\\n',\n",
      " 'type': 'python'}\n"
     ]
    }
   ],
   "source": [
    "# Let's load an analagous Python Grader using the same code.\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "import grader\n",
    "\n",
    "code = inspect.getsource(grader)\n",
    "\n",
    "PYTHON_GRADER = {\n",
    "    \"type\": \"python\",\n",
    "    \"name\": \"Python Countdown Grader\",\n",
    "    \"source\": code,\n",
    "    \"pass_threshold\": 5.0,\n",
    "}\n",
    "print(\"> Defined Python grader\")\n",
    "pprint(PYTHON_GRADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c692cc",
   "metadata": {},
   "source": [
    "Now we're ready to define our Eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4874d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Created eval eval_69416fde86fc8191ac0d04598efd2413:\n",
      "{\n",
      "  \"id\": \"eval_69416fde86fc8191ac0d04598efd2413\",\n",
      "  \"created_at\": 1765896158,\n",
      "  \"data_source_config\": {\n",
      "    \"schema\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"item\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"target\": {\n",
      "              \"type\": \"integer\"\n",
      "            },\n",
      "            \"nums\": {\n",
      "              \"type\": \"array\",\n",
      "              \"items\": {\n",
      "                \"type\": \"integer\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"sample\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"model\": {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"choices\": {\n",
      "              \"type\": \"array\",\n",
      "              \"items\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                  \"message\": {\n",
      "                    \"type\": \"object\",\n",
      "                    \"properties\": {\n",
      "                      \"role\": {\n",
      "                        \"type\": \"string\",\n",
      "                        \"enum\": [\n",
      "                          \"assistant\"\n",
      "                        ]\n",
      "                      },\n",
      "                      \"content\": {\n",
      "                        \"type\": [\n",
      "                          \"string\",\n",
      "                          \"array\",\n",
      "                          \"null\"\n",
      "                        ]\n",
      "                      },\n",
      "                      \"refusal\": {\n",
      "                        \"type\": [\n",
      "                          \"boolean\",\n",
      "                          \"null\"\n",
      "                        ]\n",
      "                      },\n",
      "                      \"tool_calls\": {\n",
      "                        \"type\": [\n",
      "                          \"array\",\n",
      "                          \"null\"\n",
      "                        ],\n",
      "                        \"items\": {\n",
      "                          \"type\": \"object\",\n",
      "                          \"properties\": {\n",
      "                            \"type\": {\n",
      "                              \"type\": \"string\",\n",
      "                              \"enum\": [\n",
      "                                \"function\"\n",
      "                              ]\n",
      "                            },\n",
      "                            \"function\": {\n",
      "                              \"type\": \"object\",\n",
      "                              \"properties\": {\n",
      "                                \"name\": {\n",
      "                                  \"type\": \"string\"\n",
      "                                },\n",
      "                                \"arguments\": {\n",
      "                                  \"type\": \"string\"\n",
      "                                }\n",
      "                              },\n",
      "                              \"required\": [\n",
      "                                \"name\",\n",
      "                                \"arguments\"\n",
      "                              ]\n",
      "                            },\n",
      "                            \"id\": {\n",
      "                              \"type\": \"string\"\n",
      "                            }\n",
      "                          },\n",
      "                          \"required\": [\n",
      "                            \"type\",\n",
      "                            \"function\",\n",
      "                            \"id\"\n",
      "                          ]\n",
      "                        }\n",
      "                      },\n",
      "                      \"function_call\": {\n",
      "                        \"type\": [\n",
      "                          \"object\",\n",
      "                          \"null\"\n",
      "                        ],\n",
      "                        \"properties\": {\n",
      "                          \"name\": {\n",
      "                            \"type\": \"string\"\n",
      "                          },\n",
      "                          \"arguments\": {\n",
      "                            \"type\": \"string\"\n",
      "                          }\n",
      "                        },\n",
      "                        \"required\": [\n",
      "                          \"name\",\n",
      "                          \"arguments\"\n",
      "                        ]\n",
      "                      }\n",
      "                    },\n",
      "                    \"required\": [\n",
      "                      \"role\"\n",
      "                    ]\n",
      "                  },\n",
      "                  \"finish_reason\": {\n",
      "                    \"type\": \"string\"\n",
      "                  }\n",
      "                },\n",
      "                \"required\": [\n",
      "                  \"index\",\n",
      "                  \"message\",\n",
      "                  \"finish_reason\"\n",
      "                ]\n",
      "              }\n",
      "            },\n",
      "            \"output_text\": {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"output_json\": {\n",
      "              \"type\": \"object\"\n",
      "            },\n",
      "            \"output_tools\": {\n",
      "              \"type\": \"array\",\n",
      "              \"items\": {\n",
      "                \"type\": \"object\"\n",
      "              }\n",
      "            },\n",
      "            \"output_reasoning_summary\": {\n",
      "              \"type\": [\n",
      "                \"string\",\n",
      "                \"null\"\n",
      "              ]\n",
      "            },\n",
      "            \"output_audio\": {\n",
      "              \"type\": [\n",
      "                \"object\",\n",
      "                \"null\"\n",
      "              ]\n",
      "            },\n",
      "            \"input_tools\": {\n",
      "              \"type\": \"array\",\n",
      "              \"items\": {\n",
      "                \"type\": \"object\"\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"model\",\n",
      "            \"choices\"\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"item\",\n",
      "        \"sample\"\n",
      "      ]\n",
      "    },\n",
      "    \"type\": \"custom\",\n",
      "    \"max_items\": null\n",
      "  },\n",
      "  \"metadata\": {},\n",
      "  \"name\": \"endpoint-countdown-eval-f83f16a5\",\n",
      "  \"object\": \"eval\",\n",
      "  \"testing_criteria\": [\n",
      "    {\n",
      "      \"name\": \"Python Countdown Grader\",\n",
      "      \"source\": \"\\\"\\\"\\\"\\nPython Grader logic for OpenAI Evals\\n\\nExample from https://github.com/azure-ai-foundry/fine-tuning/blob/main/Demos/RFT_Countdown/demo_with_python_grader.ipynb\\n\\\"\\\"\\\"\\nfrom typing import Any, Dict\\nimport ast\\nimport json\\nimport logging\\nimport re\\n\\n\\ndef _eval(n):\\n    if isinstance(n, ast.Constant):\\n        return n.value\\n\\n    if isinstance(n, ast.BinOp) and type(n.op) in {\\n        ast.Add: lambda a, b: a + b,\\n        ast.Sub: lambda a, b: a - b,\\n        ast.Mult: lambda a, b: a * b,\\n        ast.Div: lambda a, b: a / b,\\n        ast.FloorDiv: lambda a, b: a // b,\\n        ast.Mod: lambda a, b: a % b,\\n        ast.Pow: lambda a, b: a**b,\\n    }:\\n        return {\\n            ast.Add: lambda a, b: a + b,\\n            ast.Sub: lambda a, b: a - b,\\n            ast.Mult: lambda a, b: a * b,\\n            ast.Div: lambda a, b: a / b,\\n            ast.FloorDiv: lambda a, b: a // b,\\n            ast.Mod: lambda a, b: a % b,\\n            ast.Pow: lambda a, b: a**b,\\n        }[type(n.op)](_eval(n.left), _eval(n.right))\\n\\n    if isinstance(n, ast.UnaryOp) and type(n.op) in {\\n        ast.UAdd: lambda a: +a,\\n        ast.USub: lambda a: -a,\\n    }:\\n        return {\\n            ast.UAdd: lambda a: +a,\\n            ast.USub: lambda a: -a,\\n        }[\\n            type(n.op)\\n        ](_eval(n.operand))\\n\\n    raise ValueError(\\\"bad expr\\\")\\n\\n\\ndef _safe_eval(e):\\n    return _eval(ast.parse(e, mode=\\\"eval\\\").body)\\n\\n\\ndef grade(sample: Dict[str, Any], item: Dict[str, Any]) -> float:\\n    \\\"\\\"\\\"\\n    Implements the OpenAI Python Grader API, grading the given sample in the\\n    context of the provided item.\\n\\n    Returns a score in all cases, with 0 returned in case of error.\\n    \\\"\\\"\\\"\\n    # Be flexible about where our output comes from, but it *must* be JSON.\\n    output = {}\\n    if \\\"output_json\\\" in sample:\\n        output = sample[\\\"output_json\\\"]\\n    else:\\n        # fallback to output_text but it better be JSON!\\n        try:\\n            output = json.loads(sample[\\\"output_text\\\"])\\n        except Exception as e:\\n            logging.error(\\\"failed to find JSON output in output_json and output_text: {e}\\\")\\n            return 0\\n    if not output:\\n        logging.error(\\\"failed to find non-null JSON output in sample\\\")\\n        return 0\\n    \\n    # Extract our Expression and Result\\n    expr = output.get(\\\"expression\\\")\\n    if not expr:\\n        logging.warning(\\\"expression was empty\\\")\\n        return 0\\n    result = output.get(\\\"result\\\")\\n    if not result:\\n        logging.warning(\\\"result was empty\\\")\\n        return 0\\n\\n    # Perform the evaluation.\\n    try:\\n        expr_val = _safe_eval(expr)\\n    except ValueError as e:\\n        logging.error(f\\\"value error evaluating expression: {e}\\\")\\n        return 0\\n    \\n    # TODO: handle parsing item more cleanly.\\n    try:\\n        # Check if all numbers were used.\\n        used = sorted(map(int, re.findall(r\\\"-?\\\\d+\\\", expr)))\\n        expected = sorted(map(int, item[\\\"nums\\\"]))\\n        if used != expected:\\n            logging.info(\\\"all numbers were not used exactly once\\\")\\n            return 0\\n\\n        sr = int(float(result))\\n        it = int(float(item[\\\"target\\\"]))\\n\\n        # Score function\\n        if expr_val != sr:\\n            return 1\\n        if sr == it:\\n            return 5\\n        if abs(sr - it) <= 1:\\n            return 4\\n        if abs(sr - it) <= 5:\\n            return 3\\n        return 2\\n    except Exception as e:\\n        logging.error(f\\\"exception while grading: {e}\\\")\\n    return 0\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    import json\\n    data = {}\\n    try:\\n        with open(\\\"./test.json\\\", mode=\\\"rb\\\") as f:\\n            data = json.load(f)\\n        sample, item = data[\\\"sample\\\"], data[\\\"item\\\"]\\n        \\n        print(f\\\"grading with:\\\\n\\\\tsample: {sample}\\\\n\\\\titem: {item}\\\")\\n        score = grade(sample, item)\\n        print(f\\\"score: {score}\\\")\\n    except Exception as e:\\n        import sys\\n        print(e, file=sys.stderr)\\n\",\n",
      "      \"type\": \"python\",\n",
      "      \"image_tag\": \"2025-05-08\",\n",
      "      \"pass_threshold\": 5.0,\n",
      "      \"id\": \"Python Countdown Grader-6927232f-7d48-443a-b2c6-17dbe839c17d\",\n",
      "      \"grdr_id\": null,\n",
      "      \"inactive_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Remote Countdown Grader\",\n",
      "      \"type\": \"endpoint\",\n",
      "      \"id\": \"Remote Countdown Grader-e54f7be8-bd6d-47f7-8c54-6296828fdf09\",\n",
      "      \"grdr_id\": null,\n",
      "      \"headers\": {\n",
      "        \"X-Functions-Key\": \"**********\"\n",
      "      },\n",
      "      \"inactive_at\": null,\n",
      "      \"max_rps\": null,\n",
      "      \"pass_threshold\": 5.0,\n",
      "      \"url\": \"https://countdown-endpoint-demo.azurewebsites.net/api/grader\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davevoutila\\src\\fine-tuning\\.venv\\Lib\\site-packages\\pydantic\\main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `literal['label_model']` - serialized value may not be as expected [field_name='type', input_value='endpoint', input_type=str])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StringCheckGrader` - serialized value may not be as expected [field_name='testing_criteria', input_value=LabelModelGrader(input=No...ebsites.net/api/grader'), input_type=LabelModelGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `TestingCriterionEvalGraderTextSimilarity` - serialized value may not be as expected [field_name='testing_criteria', input_value=LabelModelGrader(input=No...ebsites.net/api/grader'), input_type=LabelModelGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `TestingCriterionEvalGraderPython` - serialized value may not be as expected [field_name='testing_criteria', input_value=LabelModelGrader(input=No...ebsites.net/api/grader'), input_type=LabelModelGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `TestingCriterionEvalGraderScoreModel` - serialized value may not be as expected [field_name='testing_criteria', input_value=LabelModelGrader(input=No...ebsites.net/api/grader'), input_type=LabelModelGrader])\n",
      "  return self.__pydantic_serializer__.to_json(\n"
     ]
    }
   ],
   "source": [
    "# Create our Eval\n",
    "baseline_eval = client.evals.create(\n",
    "    name=f\"endpoint-countdown-eval-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=EVAL_DATA_SOURCE,\n",
    "    testing_criteria=[PYTHON_GRADER, ENDPOINT_GRADER],\n",
    ")\n",
    "print(f\"> Created eval {baseline_eval.id}:\")\n",
    "print(baseline_eval.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff4fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Created run evalrun_69416fe2e9948191a7f1b14b4a61c507: endpoint-countdown-eval-run-gpt-4.1-f83f16a5\n",
      "> Created run evalrun_69416fe46ecc8191bca0e91f85916012: endpoint-countdown-eval-run-o4-mini-f83f16a5\n",
      "> Created 2 runs.\n"
     ]
    }
   ],
   "source": [
    "# Define our Runs\n",
    "USER_PROMPT = \"\"\"\n",
    "target: {{item.target}}\n",
    "numbers: {{item.nums}}\n",
    "\"\"\".strip()\n",
    "\n",
    "RUNS = []\n",
    "for model in [\"gpt-4.1\", \"o4-mini\"]:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": {\"type\": \"file_id\", \"id\": eval_file.id},\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"developer\",\n",
    "                    \"content\": {\"type\": \"input_text\", \"text\": DEVELOPER_PROMPT},\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": {\"type\": \"input_text\", \"text\": USER_PROMPT},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": {\n",
    "            \"response_format\": RESPONSE_SCHEMA,\n",
    "        },\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"endpoint-countdown-eval-run-{model}-{UNIQUE_ENOUGH_KEY}\",\n",
    "        eval_id=baseline_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE,\n",
    "    )\n",
    "    print(f\"> Created run {run.id}: {run.name}\")\n",
    "    RUNS.append(run)\n",
    "\n",
    "print(f\"> Created {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de97ef1",
   "metadata": {},
   "source": [
    "# Reinforcement Fine Tuning\n",
    "Now let's train a model to improve upon our out-of-box o4-mini experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611870dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Uploaded training file file-HB4b7Mstv3BGb4GrQvCx5t\n",
      "> Uploaded validation file file-Nmy5sFnMJp73MRmgXyQBtE\n"
     ]
    }
   ],
   "source": [
    "# Upload training and validation data.\n",
    "with open(\"./data/countdown_train_25.jsonl\", mode=\"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "training_file = client.files.wait_for_processing(training_file.id)\n",
    "print(f\"> Uploaded training file {training_file.id}\")\n",
    "\n",
    "with open(\"./data/countdown_valid_10.jsonl\", mode=\"rb\") as f:\n",
    "    validation_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "validation_file = client.files.wait_for_processing(validation_file.id)\n",
    "print(f\"> Uploaded validation file {validation_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c17f95",
   "metadata": {},
   "source": [
    "We can drop the `pass_threshold` settings from the individual graders. We can set it on\n",
    "the `multi_grader` we'll use to combine both Python and Endpoint graders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574dda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Created RFT job ftjob-QRLPhli5Hi1Yzoe1ODNRrnhH\n",
      "{\n",
      "  \"id\": \"ftjob-QRLPhli5Hi1Yzoe1ODNRrnhH\",\n",
      "  \"created_at\": 1765896170,\n",
      "  \"error\": {},\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"model\": \"o4-mini-2025-04-16\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": \"org-hTGGKhbVgQIQFEXlrcWgLHk9\",\n",
      "  \"result_files\": [],\n",
      "  \"seed\": 2146903865,\n",
      "  \"status\": \"validating_files\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-HB4b7Mstv3BGb4GrQvCx5t\",\n",
      "  \"validation_file\": \"file-Nmy5sFnMJp73MRmgXyQBtE\",\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"metadata\": null,\n",
      "  \"method\": {\n",
      "    \"type\": \"reinforcement\",\n",
      "    \"reinforcement\": {\n",
      "      \"grader\": {\n",
      "        \"name\": \"Combined local and remote grader\",\n",
      "        \"type\": \"multi\",\n",
      "        \"graders\": {\n",
      "          \"python\": {\n",
      "            \"type\": \"python\",\n",
      "            \"name\": \"Python Countdown Grader\",\n",
      "            \"source\": \"\\\"\\\"\\\"\\nPython Grader logic for OpenAI Evals\\n\\nExample from https://github.com/azure-ai-foundry/fine-tuning/blob/main/Demos/RFT_Countdown/demo_with_python_grader.ipynb\\n\\\"\\\"\\\"\\nfrom typing import Any, Dict\\nimport ast\\nimport json\\nimport logging\\nimport re\\n\\n\\ndef _eval(n):\\n    if isinstance(n, ast.Constant):\\n        return n.value\\n\\n    if isinstance(n, ast.BinOp) and type(n.op) in {\\n        ast.Add: lambda a, b: a + b,\\n        ast.Sub: lambda a, b: a - b,\\n        ast.Mult: lambda a, b: a * b,\\n        ast.Div: lambda a, b: a / b,\\n        ast.FloorDiv: lambda a, b: a // b,\\n        ast.Mod: lambda a, b: a % b,\\n        ast.Pow: lambda a, b: a**b,\\n    }:\\n        return {\\n            ast.Add: lambda a, b: a + b,\\n            ast.Sub: lambda a, b: a - b,\\n            ast.Mult: lambda a, b: a * b,\\n            ast.Div: lambda a, b: a / b,\\n            ast.FloorDiv: lambda a, b: a // b,\\n            ast.Mod: lambda a, b: a % b,\\n            ast.Pow: lambda a, b: a**b,\\n        }[type(n.op)](_eval(n.left), _eval(n.right))\\n\\n    if isinstance(n, ast.UnaryOp) and type(n.op) in {\\n        ast.UAdd: lambda a: +a,\\n        ast.USub: lambda a: -a,\\n    }:\\n        return {\\n            ast.UAdd: lambda a: +a,\\n            ast.USub: lambda a: -a,\\n        }[\\n            type(n.op)\\n        ](_eval(n.operand))\\n\\n    raise ValueError(\\\"bad expr\\\")\\n\\n\\ndef _safe_eval(e):\\n    return _eval(ast.parse(e, mode=\\\"eval\\\").body)\\n\\n\\ndef grade(sample: Dict[str, Any], item: Dict[str, Any]) -> float:\\n    \\\"\\\"\\\"\\n    Implements the OpenAI Python Grader API, grading the given sample in the\\n    context of the provided item.\\n\\n    Returns a score in all cases, with 0 returned in case of error.\\n    \\\"\\\"\\\"\\n    # Be flexible about where our output comes from, but it *must* be JSON.\\n    output = {}\\n    if \\\"output_json\\\" in sample:\\n        output = sample[\\\"output_json\\\"]\\n    else:\\n        # fallback to output_text but it better be JSON!\\n        try:\\n            output = json.loads(sample[\\\"output_text\\\"])\\n        except Exception as e:\\n            logging.error(\\\"failed to find JSON output in output_json and output_text: {e}\\\")\\n            return 0\\n    if not output:\\n        logging.error(\\\"failed to find non-null JSON output in sample\\\")\\n        return 0\\n    \\n    # Extract our Expression and Result\\n    expr = output.get(\\\"expression\\\")\\n    if not expr:\\n        logging.warning(\\\"expression was empty\\\")\\n        return 0\\n    result = output.get(\\\"result\\\")\\n    if not result:\\n        logging.warning(\\\"result was empty\\\")\\n        return 0\\n\\n    # Perform the evaluation.\\n    try:\\n        expr_val = _safe_eval(expr)\\n    except ValueError as e:\\n        logging.error(f\\\"value error evaluating expression: {e}\\\")\\n        return 0\\n    \\n    # TODO: handle parsing item more cleanly.\\n    try:\\n        # Check if all numbers were used.\\n        used = sorted(map(int, re.findall(r\\\"-?\\\\d+\\\", expr)))\\n        expected = sorted(map(int, item[\\\"nums\\\"]))\\n        if used != expected:\\n            logging.info(\\\"all numbers were not used exactly once\\\")\\n            return 0\\n\\n        sr = int(float(result))\\n        it = int(float(item[\\\"target\\\"]))\\n\\n        # Score function\\n        if expr_val != sr:\\n            return 1\\n        if sr == it:\\n            return 5\\n        if abs(sr - it) <= 1:\\n            return 4\\n        if abs(sr - it) <= 5:\\n            return 3\\n        return 2\\n    except Exception as e:\\n        logging.error(f\\\"exception while grading: {e}\\\")\\n    return 0\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    import json\\n    data = {}\\n    try:\\n        with open(\\\"./test.json\\\", mode=\\\"rb\\\") as f:\\n            data = json.load(f)\\n        sample, item = data[\\\"sample\\\"], data[\\\"item\\\"]\\n        \\n        print(f\\\"grading with:\\\\n\\\\tsample: {sample}\\\\n\\\\titem: {item}\\\")\\n        score = grade(sample, item)\\n        print(f\\\"score: {score}\\\")\\n    except Exception as e:\\n        import sys\\n        print(e, file=sys.stderr)\\n\"\n",
      "          },\n",
      "          \"endpoint\": {\n",
      "            \"type\": \"endpoint\",\n",
      "            \"name\": \"Remote Countdown Grader\",\n",
      "            \"url\": \"https://countdown-endpoint-demo.azurewebsites.net/api/grader\",\n",
      "            \"headers\": {\n",
      "              \"X-Functions-Key\": \"**********\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"calculate_output\": \"(python + endpoint) / 2\"\n",
      "      },\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": \"auto\",\n",
      "        \"compute_multiplier\": \"auto\",\n",
      "        \"eval_interval\": \"auto\",\n",
      "        \"eval_samples\": \"auto\",\n",
      "        \"learning_rate_multiplier\": \"auto\",\n",
      "        \"n_epochs\": \"auto\",\n",
      "        \"reasoning_effort\": \"default\"\n",
      "      },\n",
      "      \"response_format\": {\n",
      "        \"type\": \"json_schema\",\n",
      "        \"json_schema\": {\n",
      "          \"name\": \"math_expression\",\n",
      "          \"strict\": true,\n",
      "          \"schema\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "              \"expression\": {\n",
      "                \"type\": \"string\"\n",
      "              },\n",
      "              \"result\": {\n",
      "                \"type\": \"integer\"\n",
      "              }\n",
      "            },\n",
      "            \"required\": [\n",
      "              \"expression\",\n",
      "              \"result\"\n",
      "            ],\n",
      "            \"additionalProperties\": false\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"user_provided_suffix\": \"dv-endpoint-countdown-ft-f83f16a5\",\n",
      "  \"usage_metrics\": null,\n",
      "  \"shared_with_openai\": false,\n",
      "  \"eval_id\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davevoutila\\src\\fine-tuning\\.venv\\Lib\\site-packages\\pydantic\\main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `literal['string_check']` - serialized value may not be as expected [field_name='type', input_value='multi', input_type=str])\n",
      "  PydanticSerializationUnexpectedValue(Expected `TextSimilarityGrader` - serialized value may not be as expected [field_name='grader', input_value=StringCheckGrader(input=N...python + endpoint) / 2'), input_type=StringCheckGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `PythonGrader` - serialized value may not be as expected [field_name='grader', input_value=StringCheckGrader(input=N...python + endpoint) / 2'), input_type=StringCheckGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `ScoreModelGrader` - serialized value may not be as expected [field_name='grader', input_value=StringCheckGrader(input=N...python + endpoint) / 2'), input_type=StringCheckGrader])\n",
      "  PydanticSerializationUnexpectedValue(Expected `MultiGrader` - serialized value may not be as expected [field_name='grader', input_value=StringCheckGrader(input=N...python + endpoint) / 2'), input_type=StringCheckGrader])\n",
      "  return self.__pydantic_serializer__.to_json(\n"
     ]
    }
   ],
   "source": [
    "# create our RFT job\n",
    "import copy\n",
    "\n",
    "# scrub pass thresholds as they currently cannot be used with multi-grader\n",
    "RFT_ENDPOINT_GRADER = copy.deepcopy(ENDPOINT_GRADER)\n",
    "if \"pass_threshold\" in RFT_ENDPOINT_GRADER:\n",
    "    del RFT_ENDPOINT_GRADER[\"pass_threshold\"]\n",
    "RFT_PYTHON_GRADER = copy.deepcopy(PYTHON_GRADER)\n",
    "if \"pass_threshold\" in RFT_PYTHON_GRADER:\n",
    "    del RFT_PYTHON_GRADER[\"pass_threshold\"]\n",
    "\n",
    "\n",
    "RFT_MULTI_GRADER = {\n",
    "    \"type\": \"multi\",\n",
    "    \"name\": \"Combined local and remote grader\",\n",
    "    \"graders\": {\n",
    "        \"python\": RFT_PYTHON_GRADER,\n",
    "        \"endpoint\": RFT_ENDPOINT_GRADER,\n",
    "    },\n",
    "    \"calculate_output\": \"(python + endpoint) / 2\",  # average the scores\n",
    "}\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    suffix=f\"dv-endpoint-countdown-ft-{UNIQUE_ENOUGH_KEY}\",\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    training_file=training_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    method={\n",
    "        \"type\": \"reinforcement\",\n",
    "        \"reinforcement\": {\n",
    "            \"grader\": RFT_MULTI_GRADER,\n",
    "            \"response_format\": RESPONSE_SCHEMA,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(f\"> Created RFT job {job.id}\")\n",
    "print(job.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
