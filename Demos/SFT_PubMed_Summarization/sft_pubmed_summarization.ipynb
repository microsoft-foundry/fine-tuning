{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with PubMed Dataset on Microsoft Foundry\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models using **Supervised Fine-Tuning (SFT)** with the PubMed Medical Research Summarization dataset.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Understand supervised fine-tuning for medical text summarization\n",
    "2. Prepare and format medical research data\n",
    "3. Upload datasets to Microsoft Foundry\n",
    "4. Create and monitor a supervised fine-tuning job\n",
    "5. Deploy and test your fine-tuned model\n",
    "\n",
    "**Note**: Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install all required packages from requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-projects>=2.0.0b1 (from -r requirements.txt (line 2))\n",
      "  Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting openai (from -r requirements.txt (line 5))\n",
      "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting azure-identity (from -r requirements.txt (line 8))\n",
      "  Using cached azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 12))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.35.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\work\\amlrepos\\fine-tuning\\envmed\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (4.15.0)\n",
      "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached azure_storage_blob-12.27.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai->-r requirements.txt (line 5))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai->-r requirements.txt (line 5))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting msrest>=0.7.1 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-mgmt-core>=1.6.0 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\envmed\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl (234 kB)\n",
      "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
      "Using cached azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
      "Using cached azure_storage_blob-12.27.1-py3-none-any.whl (428 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: urllib3, typing-inspection, tqdm, sniffio, python-dotenv, PyJWT, pydantic-core, pycparser, oauthlib, jiter, isodate, idna, h11, distro, charset_normalizer, certifi, annotated-types, requests, pydantic, httpcore, cffi, anyio, requests-oauthlib, httpx, cryptography, azure-core, openai, msrest, azure-storage-blob, azure-mgmt-core, msal, azure-mgmt-cognitiveservices, azure-ai-projects, msal-extensions, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 annotated-types-0.7.0 anyio-4.12.0 azure-ai-projects-2.0.0b2 azure-core-1.37.0 azure-identity-1.25.1 azure-mgmt-cognitiveservices-14.1.0 azure-mgmt-core-1.6.0 azure-storage-blob-12.27.1 certifi-2025.11.12 cffi-2.0.0 charset_normalizer-3.4.4 cryptography-46.0.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 isodate-0.7.2 jiter-0.12.0 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 oauthlib-3.3.1 openai-2.14.0 pycparser-2.23 pydantic-2.12.5 pydantic-core-2.41.5 python-dotenv-1.2.1 requests-2.32.5 requests-oauthlib-2.0.0 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2 urllib3-2.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Azure Environment\n",
    "\n",
    "Set your Microsoft Foundry Project endpoint and model name. We're using **gpt-4.1** in this example, but you can use other supported GPT models. Copy the file `.env.template` (located in this folder), and save it as file named `.env`. Enter appropriate values for the environment variables used for the job you want to run.\n",
    "\n",
    "```\n",
    "MICROSOFT_FOUNDRY_PROJECT_ENDPOINT=<your-endpoint>\n",
    "MODEL_NAME=gpt-4.1\n",
    "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
    "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
    "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ.get(\"MICROSOFT_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_name = os.environ.get(\"MODEL_NAME\")\n",
    "\n",
    "# Define dataset file paths\n",
    "training_file_path = \"training.jsonl\"\n",
    "validation_file_path = \"validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to Microsoft Foundry Project\n",
    "\n",
    "Connect to Microsoft Foundry Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows.\n",
    "\n",
    "**Important**: Ensure you have the **Azure AI User** role assigned to your account for the Microsoft Foundry Project resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Microsoft Foundry Project\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
    "openai_client = project_client.get_openai_client()\n",
    "\n",
    "print(\"Connected to Microsoft Foundry Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Training Files\n",
    "\n",
    "Upload the training and validation JSONL files to Microsoft Foundry. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training file...\n",
      "Training file ID: file-05083606477b49b696a6500f066cd028\n",
      "\n",
      "Uploading validation file...\n",
      "Validation file ID: file-7aa581fd19ec4c959322612351f44638\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading training file...\")\n",
    "with open(training_file_path, \"rb\") as f:\n",
    "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "print(f\"Training file ID: {train_file.id}\")\n",
    "\n",
    "print(\"\\nUploading validation file...\")\n",
    "with open(validation_file_path, \"rb\") as f:\n",
    "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
    "print(f\"Validation file ID: {validation_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wait for File Processing\n",
    "\n",
    "Microsoft Foundry needs to process the uploaded files before they can be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for files to be processed...\n",
      "Files ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for files to be processed...\")\n",
    "openai_client.files.wait_for_processing(train_file.id)\n",
    "openai_client.files.wait_for_processing(validation_file.id)\n",
    "print(\"Files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Supervised Fine-Tuning Job\n",
    "\n",
    "Create a supervised fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
    "\n",
    "**Hyperparameters:**\n",
    "1. **n_epochs (3)**: Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
    "2. **batch_size (1)**: Number of training examples processed together in each iteration. Smaller batches provide more frequent updates. Typical range: 1-8.\n",
    "3. **learning_rate_multiplier (1.0)**: Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
    "\n",
    "**Note**: Adjust these based on your dataset size and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating supervised fine-tuning job...\n",
      "Fine-tuning job created!\n",
      "Job ID: ftjob-ccae5a26756f4dcb833f221c88e633c8\n",
      "Status: pending\n",
      "Model: gpt-4.1-2025-04-14\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating supervised fine-tuning job...\")\n",
    "\n",
    "fine_tune_job = openai_client.fine_tuning.jobs.create(\n",
    "    model=model_name,\n",
    "    training_file=train_file.id,\n",
    "    validation_file=validation_file.id,    \n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\"hyperparameters\": {\"n_epochs\": 3, \"batch_size\": 1, \"learning_rate_multiplier\": 1.0}},\n",
    "    },\n",
    "    extra_body={\"trainingType\": \"GlobalStandard\"},\n",
    "    suffix=\"pubmed-summarization\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job created!\")\n",
    "print(f\"Job ID: {fine_tune_job.id}\")\n",
    "print(f\"Status: {fine_tune_job.status}\")\n",
    "print(f\"Model: {fine_tune_job.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Training Progress\n",
    "\n",
    "Track the status of your fine-tuning job. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
    "print(f\"Status: {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Retrieve Fine-Tuned Model\n",
    "\n",
    "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
    "\n",
    "if completed_job.status == \"succeeded\":\n",
    "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
    "    print(f\"Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
    "else:\n",
    "    print(f\"Status: {completed_job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deploy the Fine-Tuned Model\n",
    "\n",
    "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
    "\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
    "\n",
    "deployment_name = \"gpt-4o-pubmed-finetuned\"\n",
    "\n",
    "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
    "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
    "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
    "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=50)\n",
    "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
    "    \n",
    "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
    "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "        resource_group_name=resource_group,\n",
    "        account_name=account_name,\n",
    "        deployment_name=deployment_name,\n",
    "        deployment=deployment_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting for deployment to complete...\")\n",
    "    deployment.result()\n",
    "\n",
    "print(f\"Model deployment completed: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Fine-Tuned Model\n",
    "\n",
    "Test your fine-tuned model by generating a summary for a sample medical research article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = \"\"\"Background: Alzheimer's disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and memory loss. Recent studies have suggested that neuroinflammation plays a crucial role in disease pathogenesis. Objective: This study investigated the role of microglial activation and inflammatory cytokines in early-stage Alzheimer's disease progression. Methods: We conducted a longitudinal study of 150 patients with mild cognitive impairment (MCI) over 36 months. Cerebrospinal fluid (CSF) samples were analyzed for inflammatory markers including IL-6, TNF-α, and IL-1β. Brain imaging using PET scans assessed microglial activation. Results: Patients who progressed to AD showed significantly elevated levels of IL-6 (p<0.001) and TNF-α (p<0.01) at baseline compared to stable MCI patients. Microglial activation was observed in the hippocampus and entorhinal cortex regions. Multivariate analysis revealed that combined inflammatory markers predicted AD conversion with 78% accuracy. Conclusion: Elevated neuroinflammatory markers and microglial activation in MCI patients are associated with increased risk of progression to Alzheimer's disease, suggesting potential therapeutic targets for early intervention.\"\"\"\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model=deployment_name,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a medical research summarization assistant. Create concise, accurate abstracts of medical research articles that capture the key findings and clinical implications.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize this medical research article:\\n\\n{test_article}\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Congratulations!\n",
    "\n",
    "You've successfully fine-tuned a model for medical research summarization using the PubMed dataset!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Test with more examples**: Try different medical research articles to evaluate performance\n",
    "2. **Adjust hyperparameters**: Experiment with different epoch counts, batch sizes, or learning rates\n",
    "3. **Deploy to production**: Integrate your fine-tuned model into healthcare applications\n",
    "4. **Fine-tune further**: Use your own medical publications or clinical notes for specialized summarization\n",
    "5. **Explore use cases**: Apply the model to systematic reviews, literature analysis, or clinical decision support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envrft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
