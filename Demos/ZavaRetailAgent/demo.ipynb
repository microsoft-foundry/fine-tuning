{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "14e9539c",
      "metadata": {},
      "source": [
        "# Enhancing AI Agent Tool-Calling Accuracy Through Fine-Tuning\n",
        "\n",
        "![Title Diagram](./img/intro2.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This notebook accompanies the Ignite Conference demo and serves as a structured cookbook for reproducing the full workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888aff14",
      "metadata": {},
      "source": [
        "### Demo Goal: Improving the Zava Retail Agent\n",
        "\n",
        "To make this notebook concrete, we use a fictional retail brand called **Zava**, which provides a customer-service AI agent powered by **Microsoft Foundry** and a **Retail MCP Server** exposing various tools (product search, user lookup, order details, returns, exchanges, profile updates, and more).\n",
        "\n",
        "The goal of this demo is to show how to improve the Zava agent‚Äôs **tool-calling accuracy and multi-turn reliability** using modern fine-tuning techniques.\n",
        "\n",
        "---\n",
        "\n",
        "#### Simple Cases Work\n",
        "\n",
        "We begin by showing that the base model (e.g., `gpt-4.1-mini`) handles **simple tool calls** correctly.  \n",
        "For example, a request like:\n",
        "\n",
        "> ‚ÄúCan you show me all your products?‚Äù\n",
        "\n",
        "is resolved with the correct MCP tool invocation and produces the expected result.\n",
        "\n",
        "This establishes the baseline:  \n",
        "**single-tool operations generally work out of the box.**\n",
        "\n",
        "---\n",
        "\n",
        "#### Where the Base Model Breaks\n",
        "\n",
        "More complex customer scenarios ‚Äî such as updating an address, processing returns, or combining multiple tool steps ‚Äî require the model to:\n",
        "\n",
        "- Chain tools correctly  \n",
        "- Pass parameters from one tool call to the next correctly\n",
        "- Remain consistent across many turns  \n",
        "\n",
        "This is where the base model struggles, motivating the need for fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### What This Demo Will Show\n",
        "\n",
        "The remainder of this notebook demonstrates how to improve the Zava agent through:\n",
        "\n",
        "1. **Synthetic Data Generation**  \n",
        "   Creating high-quality multi-turn retail conversations using the new data generation capabilities in Microsoft Foundry.\n",
        "\n",
        "2. **Supervised Fine-Tuning (SFT)**  \n",
        "   Teaching the model correct tool invocation patterns and policy-aligned reasoning with supervised fine-tuning.\n",
        "\n",
        "3. **Evaluation with Python Graders**  \n",
        "   Measuring tool-calling accuracy and verifying parameter correctness.\n",
        "\n",
        "4. **Reinforcement Fine-Tuning (RFT)**  \n",
        "   Optimizing the model for even more complex workflows using grader-based reward signals.\n",
        "\n",
        "---\n",
        "\n",
        "#### Notebook Roadmap\n",
        "\n",
        "1. Architecture overview (Agent + MCP Server)  \n",
        "2. Environment setup  \n",
        "3. Base model behavior  \n",
        "4. Evaluation methodology  \n",
        "5. Synthetic data generation \n",
        "6. Supervised fine-tuning\n",
        "7. Reinforcement fine-tuning\n",
        "8. Results and comparisons\n",
        "\n",
        "Together, these steps show how to transform the Zava agent into a **reliable, policy-aligned, multi-turn tool-calling system**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d05669",
      "metadata": {},
      "source": [
        "## 1. Architecture: Retail Agent + MCP Server + Tooling\n",
        "\n",
        "This section provides a high-level overview of the Zava retail agent‚Äôs architecture.  \n",
        "Understanding how the components interact helps motivate why fine-tuning is needed and which behaviors we expect to improve.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1 Zava Retail Agent (Microsoft Foundry Agent Service)\n",
        "\n",
        "The Zava agent is built using the **Microsoft Foundry Agent Service**, which acts as an orchestrator:\n",
        "\n",
        "- Interprets customer intent  \n",
        "- Selects the appropriate tool  \n",
        "- Constructs structured tool-call arguments  \n",
        "- Incorporates chain-of-thought reasoning  \n",
        "- Produces a final, customer-friendly response  \n",
        "\n",
        "The agent is configured with:\n",
        "\n",
        "- **A system prompt** describing Zava‚Äôs retail rules and policies  \n",
        "- **Structured tool definitions** (9 MCP tools)  \n",
        "- **A deployed model** such as:\n",
        "  - `gpt-4.1`\n",
        "  - `gpt-4.1-mini`\n",
        "  - `gpt-4.1-nano`\n",
        "  - Later in this notebook: fine-tuned models  \n",
        "\n",
        "The agent is the ‚Äúbrain‚Äù of the system‚Äîdriving all reasoning and tool decisions.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 Retail MCP Server (Tool Provider)\n",
        "\n",
        "The agent relies on a dedicated **Model Context Protocol (MCP) server**. Details about setting up your own sample MCP server for use in this notebook can be find <TBD>.\n",
        "\n",
        "The MCP server exposes **9 tools** representing Zava‚Äôs operational backend:\n",
        "\n",
        "```\n",
        "User Lookup\n",
        " ‚îú‚îÄ find_user_id_by_email\n",
        " ‚îú‚îÄ find_user_id_by_name_zip\n",
        " ‚îî‚îÄ get_user_details\n",
        "\n",
        "Orders\n",
        " ‚îú‚îÄ get_order_details\n",
        " ‚îú‚îÄ cancel_pending_order\n",
        " ‚îî‚îÄ exchange_delivered_order_items\n",
        "\n",
        "Products\n",
        " ‚îú‚îÄ list_products\n",
        " ‚îî‚îÄ get_product_details\n",
        "\n",
        "Customer Profile\n",
        " ‚îî‚îÄ update_address\n",
        "```\n",
        "\n",
        "Each tool includes:\n",
        "\n",
        "- A schema for inputs (e.g., `user_id`, `order_id`, `zip`)  \n",
        "- A schema for outputs (e.g., `user_profile`, `order_details`, `product_metadata`)  \n",
        "- Built-in policy constraints (e.g., return windows per product category)  \n",
        "\n",
        "These tools form the **action space** available to the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 Interaction Interfaces (UI + CLI)\n",
        "\n",
        "Two interfaces are used throughout this notebook and demo.\n",
        "\n",
        "#### **Microsoft Foundry Playground (UI)**  \n",
        "Best for:\n",
        "- Visualizing tool calls in real time  \n",
        "- Demonstrating simple workflows  \n",
        "- Introducing the agent architecture  \n",
        "- Zero configuration required  \n",
        "\n",
        "#### **Local `retail_agent` CLI (included with this cookbook)** <TBD>\n",
        "Best for:\n",
        "- Rapid multi-turn iteration  \n",
        "- Testing fine-tuned models side-by-side  \n",
        "- Replaying complex customer scenarios  \n",
        "- Inspecting raw traces, JSON tool responses, and argument passing  \n",
        "\n",
        "Using both interfaces gives a complete view of the agent‚Äôs behavior‚Äîboth polished and low-level.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4 End-to-End Request Flow\n",
        "\n",
        "Below is a conceptual diagram of the Zava agent workflow:\n",
        "\n",
        "```\n",
        "Customer Message\n",
        "       ‚îÇ\n",
        "       ‚ñº\n",
        "[ Agent (LLM + Policies + Tool Schemas) ]\n",
        "       ‚îÇ\n",
        "       ‚îÇ  Determines next action\n",
        "       ‚ñº\n",
        "[ MCP Server ]\n",
        "       ‚îÇ\n",
        "       ‚îÇ  Executes structured tool call\n",
        "       ‚ñº\n",
        "Tool Output (JSON)\n",
        "       ‚îÇ\n",
        "       ‚îÇ  Inserted into LLM context\n",
        "       ‚ñº\n",
        "Agent Reasoning (CoT + policy checks)\n",
        "       ‚îÇ\n",
        "       ‚îÇ  Possibly additional tool calls\n",
        "       ‚ñº\n",
        "Final Response to Customer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1.5 Architecture Diagram\n",
        "\n",
        "<img src=\"img/architecture.png\" alt=\"Zava Architecture Diagram\" width=\"600\">\n",
        "\n",
        "---\n",
        "\n",
        "### 1.6 Why Fine-Tune?\n",
        "\n",
        "Fine-tuning an agent can help improve:\n",
        "\n",
        "- **Tool chaining**  \n",
        "  Ensuring the model triggers tools in the correct sequence  \n",
        "  (e.g., find ‚Üí fetch ‚Üí update)\n",
        "\n",
        "- **Parameter propagation**  \n",
        "  Correctly carrying IDs, zip codes, product codes, etc., across turns\n",
        "\n",
        "- **Policy alignment**  \n",
        "  Enforcing return windows, exchange rules, and category-specific constraints\n",
        "\n",
        "- **Multi-turn consistency**  \n",
        "  Maintaining state over long interactions (multiple orders, items, or profiles)\n",
        "\n",
        "This structural understanding sets the stage for Section 2,  \n",
        "where we prepare the environment and test the agent‚Äôs baseline behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78487dc",
      "metadata": {},
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "This notebook is designed to run in a local Python environment (e.g., VS Code Notebook) that connects to your Microsoft Foundry resources and the Zava Retail MCP server.\n",
        "\n",
        "Follow the steps below to prepare your environment.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1 Prerequisites\n",
        "\n",
        "- **Python 3.12**  \n",
        "  (This notebook was tested with Python 3.12; earlier versions may not support all SDK features.)\n",
        "\n",
        "- **Conda (recommended)**  \n",
        "  For managing an isolated environment.\n",
        "\n",
        "- **Microsoft Foundry Resources Provisioned**  \n",
        "  You will need:\n",
        "  - Azure Subscription  \n",
        "  - Microsoft Foundry Project \n",
        "  - A deployed model inside your Project  \n",
        "  - Permissions to run finetuning + evaluations  \n",
        "\n",
        "The notebook assumes these already exist and will pull them from your `.env` file.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Create a Conda Environment\n",
        "\n",
        "```bash\n",
        "conda create -n retail-finetune python=3.12 -y\n",
        "conda activate retail-finetune\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Install Python Dependencies\n",
        "\n",
        "Install all required packages from the provided `requirements.txt`:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "> If using VS Code notebooks, ensure you select the `retail-finetune` kernel on top right.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4 Configure Environment Variables\n",
        "\n",
        "A `.env.example` file is included with the cookbook.  \n",
        "Rename it:\n",
        "\n",
        "```bash\n",
        "mv .env.example .env\n",
        "```\n",
        "\n",
        "Open `.env` and fill in the required values:\n",
        "\n",
        "> **Important:** The notebook will not function without valid Azure credentials.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.5 Testing MCP Server Connectivity\n",
        "\n",
        "This notebook interacts with your hosted Retail MCP server.  \n",
        "\n",
        "**Verify Python Access to the MCP Server**\n",
        "\n",
        "Below is quick sanity test to check connectivity and access from notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6a1488",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick connectivity test using the MCP test utility\n",
        "import sys\n",
        "sys.path.append('tools')\n",
        "\n",
        "from test_mcp_connectivity import MCPConnectivityTester, quick_test\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Get MCP server URL from environment or use default\n",
        "mcp_url = os.getenv(\"MCP_SERVER_URL\")\n",
        "\n",
        "mcp_url = mcp_url.rstrip('/mcp')\n",
        "print(f\"üîç Testing MCP Server\\n\")\n",
        "print(f\"üåê MCP Server URL: {mcp_url}\\n\")\n",
        "\n",
        "# Quick test first\n",
        "if quick_test(mcp_url):\n",
        "    print(\"‚úÖ Server is reachable!\\n\")\n",
        "    \n",
        "    # Run full test suite with notebook-friendly formatting\n",
        "    tester = MCPConnectivityTester(mcp_url)\n",
        "    tester.run_all_tests(notebook_mode=True)\n",
        "else:\n",
        "    print(\"‚ùå Server is not reachable. Please check the URL and your network connection.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430840b8",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2.7 Testing Retail Agent CLI \n",
        "\n",
        "If your resources require Azure login:\n",
        "\n",
        "```bash\n",
        "az login\n",
        "az account set --subscription \"<your-subscription-id>\"\n",
        "```\n",
        "\n",
        "This ensures CLI-backed SDK calls work correctly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "90d19b40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Testing Microsoft Foundry Retail Agent\n",
            "\n",
            "======================================================================\n",
            "ü§ñ Retail Agent Test Suite\n",
            "======================================================================\n",
            "Model: gpt-4o-mini\n",
            "MCP Server: https://retail-mcp-server-sim.braveflower-06b407cc.eastus.azurecontainerapps.io/mcp\n",
            "\n",
            "Testing Azure AI Foundry connection...\n",
            "‚úì Connected to Azure AI Foundry\n",
            "\n",
            "Testing agent creation...\n",
            "‚úì Agent created: asst_k8wIcr1ihVLb6LosawaCIl2f\n",
            "  Model: gpt-4o-mini\n",
            "\n",
            "Testing thread creation...\n",
            "‚úì Thread created: thread_DlXxsZW1S1RzrggECl6MJwa5\n",
            "\n",
            "Testing simple query...\n",
            "  Query: 'What products do you have?'\n",
            "‚úì Got response\n",
            "  Response preview: Here are the products we have available:\n",
            "\n",
            "1. T-Shirt\n",
            "2. Laptop\n",
            "3. Running Shoes\n",
            "4. Smartphone\n",
            "5. Bac...\n",
            "\n",
            "Testing user lookup...\n",
            "  Query: 'Can you find user information for noah.brown7922@example.com?'\n",
            "‚úì User lookup successful (tool was called)\n",
            "\n",
            "Cleaning up resources...\n",
            "‚úì Deleted agent\n",
            "‚úì Deleted thread\n",
            "\n",
            "======================================================================\n",
            "üìä Test Summary\n",
            "======================================================================\n",
            "\n",
            "Test Name                      Status          Result\n",
            "----------------------------------------------------------------------\n",
            "Connection                     PASS            ‚úÖ\n",
            "Agent Creation                 PASS            ‚úÖ\n",
            "Thread Creation                PASS            ‚úÖ\n",
            "Simple Query                   PASS            ‚úÖ\n",
            "User Lookup                    PASS            ‚úÖ\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üìà Results: 5/5 tests passed (100%)\n",
            "\n",
            "‚úÖ All tests passed! Retail agent is fully operational.\n"
          ]
        }
      ],
      "source": [
        "# Testing Retail Agent with Microsoft Foundry\n",
        "import sys\n",
        "sys.path.append('tools')\n",
        "\n",
        "from test_retail_agent import RetailAgentTester\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "print(\"ü§ñ Testing Microsoft Foundry Retail Agent\\n\")\n",
        "\n",
        "# Initialize the tester\n",
        "tester = RetailAgentTester()\n",
        "\n",
        "# Run all tests with notebook-friendly formatting\n",
        "results = tester.run_all_tests(notebook_mode=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f318ca",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 2.8 Summary\n",
        "\n",
        "At this point you should have:\n",
        "\n",
        "- A working Python 3.12 environment  \n",
        "- All dependencies installed  \n",
        "- `.env` populated with Azure + MCP settings  \n",
        "- Connectivity to the MCP server  \n",
        "- VS Code Notebook kernel set to `retail-finetune`  \n",
        "\n",
        "We are now ready to begin examining the baseline behavior of the Zava Agent in Section 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b28d6420",
      "metadata": {},
      "source": [
        "## 3. Data & Policy: Ground Truth for the Zava Retail Agent\n",
        "\n",
        "Before we evaluate or fine-tune the Zava retail agent, we need to understand the *ground truth* it operates on:\n",
        "\n",
        "- The **store data** (users, products, orders) exposed by the MCP server  \n",
        "- The **operational policy** that constrains what the agent is allowed to do  \n",
        "- The **API surface** that we‚Äôll later use for synthetic data generation\n",
        "\n",
        "In this notebook, these are captured by three files under the `data/` folder:\n",
        "\n",
        "- `db.json` ‚Äì backing data used by the demo MCP server (users, products, orders) \n",
        "- `policy.md` ‚Äì the natural-language policy that acts like the agent‚Äôs system prompt\n",
        "- `openapi_with_policy.json` ‚Äì an OpenAPI spec with the same policy embedded into endpoint descriptions (used later for synthetic data generation) \n",
        "\n",
        "We will briefly inspect each of these so that the later evaluation and finetuning steps are not ‚Äúblack boxes‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 `db.json`: Store Graph (Users, Orders, Products)\n",
        "\n",
        "The `db.json` file is the backing store for the Zava MCP server.  \n",
        "It has three top-level collections:\n",
        "\n",
        "- `products`: catalog of product types and their variant items  \n",
        "- `users`: customer profiles, their payment methods, and order references  \n",
        "- `orders`: order records, their status, and line items  \n",
        "\n",
        "At a high level:\n",
        "\n",
        "- **50 products** across categories like `electronics`, `clothing`, `accessories`, `home`, `fitness`, etc. :contentReference[oaicite:3]{index=3}  \n",
        "- **500 users**, each with:\n",
        "  - `user_id`, `name`, `email`, `address`\n",
        "  - `payment_methods` (gift card, PayPal, credit card)\n",
        "  - list of `orders` they‚Äôve placed :contentReference[oaicite:4]{index=4}  \n",
        "- **1000 orders**, each with:\n",
        "  - `status ‚àà {pending, processed, delivered, cancelled}`\n",
        "  - list of `items` (with product + item IDs)\n",
        "  - payment history and fulfillment info :contentReference[oaicite:5]{index=5}  \n",
        "\n",
        "This structure gives us a rich testbed for:\n",
        "\n",
        "- Product discovery flows (e.g., ‚Äúshow me all your laptops‚Äù)  \n",
        "- Simple orders (single item) and complex orders (multiple items)  \n",
        "- Policy-constrained actions like cancel, return, and exchange, where status and item lists matter\n",
        "\n",
        "We‚Äôll use this data both to **simulate realistic conversations** and to **evaluate whether the agent‚Äôs tool calls respect the true store state**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Quick Peek at `db.json` (Code)\n",
        "\n",
        "You can run the following cell to load `db.json` and print a short summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe61a088",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_path = Path(\"data\")\n",
        "db_file = data_path / \"db.json\"\n",
        "\n",
        "with db_file.open() as f:\n",
        "    db = json.load(f)\n",
        "\n",
        "products = db[\"products\"]\n",
        "users = db[\"users\"]\n",
        "orders = db[\"orders\"]\n",
        "\n",
        "print(\"=== Top-level counts ===\")\n",
        "top_counts = pd.DataFrame(\n",
        "    [\n",
        "        {\"entity\": \"products\", \"count\": len(products)},\n",
        "        {\"entity\": \"users\", \"count\": len(users)},\n",
        "        {\"entity\": \"orders\", \"count\": len(orders)},\n",
        "    ]\n",
        ")\n",
        "display(top_counts)\n",
        "\n",
        "# Derived stats\n",
        "variant_counts = [len(p[\"variants\"]) for p in products.values()]\n",
        "order_item_counts = [len(o[\"items\"]) for o in orders.values()]\n",
        "orders_per_user = [len(u[\"orders\"]) for u in users.values()]\n",
        "status_counts = Counter(o[\"status\"] for o in orders.values())\n",
        "\n",
        "summary_stats = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"metric\": \"product_variants_per_product\",\n",
        "            \"min\": min(variant_counts),\n",
        "            \"avg\": sum(variant_counts) / len(variant_counts),\n",
        "            \"max\": max(variant_counts),\n",
        "        },\n",
        "        {\n",
        "            \"metric\": \"order_items_per_order\",\n",
        "            \"min\": min(order_item_counts),\n",
        "            \"avg\": sum(order_item_counts) / len(order_item_counts),\n",
        "            \"max\": max(order_item_counts),\n",
        "        },\n",
        "        {\n",
        "            \"metric\": \"orders_per_user\",\n",
        "            \"min\": min(orders_per_user),\n",
        "            \"avg\": sum(orders_per_user) / len(orders_per_user),\n",
        "            \"max\": max(orders_per_user),\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(\"\\n=== Distribution summaries ===\")\n",
        "display(summary_stats)\n",
        "\n",
        "status_df = pd.DataFrame(\n",
        "    [{\"status\": s, \"count\": c} for s, c in status_counts.items()]\n",
        ").sort_values(\"count\", ascending=False)\n",
        "\n",
        "print(\"\\n=== Order status distribution ===\")\n",
        "display(status_df)\n",
        "\n",
        "# Simple bar chart\n",
        "plt.figure()\n",
        "status_df_sorted = status_df.sort_values(\"count\")\n",
        "plt.barh(status_df_sorted[\"status\"], status_df_sorted[\"count\"])\n",
        "plt.xlabel(\"Number of orders\")\n",
        "plt.title(\"Order status distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa7ad95",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Peek at an arbitrary user and their orders\n",
        "sample_user = next(iter(users.values()))\n",
        "\n",
        "# Build a clean 2-column view\n",
        "core_fields = [\n",
        "    (\"user_id\", sample_user[\"user_id\"]),\n",
        "    (\"name\", f'{sample_user[\"name\"][\"first_name\"]} {sample_user[\"name\"][\"last_name\"]}'),\n",
        "    (\"email\", sample_user[\"email\"]),\n",
        "    (\"tier\", sample_user.get(\"tier\")),\n",
        "    (\"abuse_flag\", sample_user.get(\"abuse_flag\")),\n",
        "    (\"num_orders\", len(sample_user[\"orders\"])),\n",
        "]\n",
        "\n",
        "core_df = pd.DataFrame(core_fields, columns=[\"field\", \"value\"])\n",
        "\n",
        "print(\"=== Sample User (Core Fields) ===\")\n",
        "display(core_df)\n",
        "\n",
        "# Build a small table of this user's orders (id + status + number of items)\n",
        "user_order_summaries = []\n",
        "for oid in sample_user[\"orders\"]:\n",
        "    o = orders[oid]\n",
        "    user_order_summaries.append(\n",
        "        {\n",
        "            \"order_id\": oid,\n",
        "            \"status\": o[\"status\"],\n",
        "            \"num_items\": len(o[\"items\"]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "if user_order_summaries:\n",
        "    print(\"\\n=== Sample User Orders ===\")\n",
        "    display(pd.DataFrame(user_order_summaries))\n",
        "else:\n",
        "    print(\"\\nThis user has no orders.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528caa60",
      "metadata": {},
      "source": [
        "### 3.3 `policy.md`: Operational Rules for the Agent\n",
        "\n",
        "The `policy.md` file defines the Zava Retail Agent‚Äôs **operational contract**.  \n",
        "Conceptually, this acts as the *system prompt* for the agent: it describes what the agent is allowed to do, in what order, and under what constraints.\n",
        "\n",
        "At a high level, the policy contains:\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîí High-Level Capabilities**\n",
        "- Lookup a user account using:\n",
        "  - **email**, or\n",
        "  - **(first name + last name + ZIP code)**\n",
        "- Retrieve and modify the user‚Äôs profile\n",
        "- Browse or search products\n",
        "- Retrieve order details\n",
        "- Perform constrained actions such as:\n",
        "  - Cancelling pending orders  \n",
        "  - Returning or exchanging delivered items  \n",
        "  - Updating the user‚Äôs default address  \n",
        "\n",
        "---\n",
        "\n",
        "#### **‚öñÔ∏è Core Control Rules**\n",
        "These rules are central to tool-calling correctness and will later drive both evaluation and fine-tuning:\n",
        "\n",
        "1. **Authenticate first**  \n",
        "   The agent must *always* identify the user via:\n",
        "   - `find_user_id_by_email`, or  \n",
        "   - `find_user_id_by_name_zip`  \n",
        "   followed **immediately** by `get_user_details`.\n",
        "\n",
        "2. **One user per conversation**  \n",
        "   After the user is authenticated, all actions must target that same user.\n",
        "\n",
        "3. **Confirm before performing ‚Äúwrite‚Äù actions**  \n",
        "   For actions that modify the system state (cancel, modify, exchange, return):  \n",
        "   - The agent must summarize the action  \n",
        "   - Ask for explicit_confirmation  \n",
        "   - Only after ‚Äúyes‚Äù should it call the tool  \n",
        "\n",
        "4. **Do not hallucinate policy or rules**  \n",
        "   The agent must adhere *only* to what is defined in `policy.md`.\n",
        "\n",
        "5. **One tool call per turn**  \n",
        "   A single agent turn must be:\n",
        "   - either a tool call  \n",
        "   - or a user-facing message  \n",
        "   Never both.\n",
        "\n",
        "6. **Escalate unsupported scenarios**  \n",
        "   If an action cannot be completed through tools, call: **transfer_to_human_agent**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### **üì¶ Domain Rules**\n",
        "The policy also specifies detailed, domain-specific logic:\n",
        "\n",
        "- **User**: fields, address format, payment methods  \n",
        "- **Product**: types, variants, options  \n",
        "- **Order**: statuses, return windows, exceptions, receipts  \n",
        "- **Cancellation rules**  \n",
        "- **Return & Exchange rules**  \n",
        "- **Delivery-related restrictions**  \n",
        "\n",
        "---\n",
        "\n",
        "#### **üìò Preview the document**\n",
        "\n",
        "Below is a small snippet you can run to preview the top of the file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d044bce",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "policy_path = Path(\"data\") / \"policy.md\"\n",
        "lines = policy_path.read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "print(\"=== policy.md (first 25 lines) ===\\n\")\n",
        "print(\"\\n\".join(lines[:25]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963501a2",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 3.5 `openapi_with_policy.json`: API Surface + Embedded Rules\n",
        "\n",
        "The `openapi_with_policy.json` file defines the **OpenAPI 3.1 specification** for the Zava MCP retail service.\n",
        "\n",
        "This file serves two major roles:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. API Surface for Synthetic Data Generation**\n",
        "\n",
        "Each MCP tool (e.g., `find_user_id_by_email`, `get_order_details`, `cancel_pending_order`) appears as an **HTTP endpoint** with:\n",
        "\n",
        "- Request schema  \n",
        "- Response schema  \n",
        "- Description fields  \n",
        "- Example payloads  \n",
        "\n",
        "This is the specification that Microsoft Foundry‚Äôs **Synthetic Data Generation Service** will use to create:\n",
        "- Multi-turn conversations  \n",
        "- Correct sequences of tool calls  \n",
        "- Rich customer scenarios  \n",
        "- Error cases and policy edge cases  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Policy Embedded into Tool Descriptions**\n",
        "\n",
        "The same rules in `policy.md` are **embedded directly into endpoint descriptions**, ensuring that generated synthetic conversations follow the same constraints.\n",
        "\n",
        "For example, the `cancel_pending_order` endpoint includes rules such as:\n",
        "\n",
        "- Only orders in `pending` status can be cancelled  \n",
        "- Valid cancellation reasons are enumerated  \n",
        "- Explicit confirmation is required  \n",
        "- Refunds apply immediately for gift cards and take 5‚Äì7 business days otherwise  \n",
        "\n",
        "Return-policy endpoints similarly encode:\n",
        "\n",
        "- Per-category return windows  \n",
        "- Replacements vs exchanges  \n",
        "- Delivery-dependent restrictions  \n",
        "- Repeated action limitations  \n",
        "\n",
        "---\n",
        "\n",
        "#### **Why the OpenAPI spec matters**\n",
        "\n",
        "This file is essential because:\n",
        "\n",
        "- It enables *policy-grounded* synthetic data generation  \n",
        "- It reflects the *true* tool-call surface of the MCP server  \n",
        "- It ensures that even long multi-turn scenarios adhere to the same policy  \n",
        "- It allows tooling to reason about **parameters, types, and constraints** (useful when evaluating models)  \n",
        "\n",
        "We will return to this file during the **Synthetic Data Generation** section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b5fa3d",
      "metadata": {},
      "source": [
        "### REDUNDANT\n",
        "\n",
        "By the end of Section 3, we now have a shared understanding of the **ground truth** the Zava Retail Agent must operate against:\n",
        "\n",
        "---\n",
        "\n",
        "#### **üì¶ Data Model (`db.json`)**  \n",
        "This is the retail store‚Äôs ‚Äúreality‚Äù:  \n",
        "products, users, orders, statuses, and order‚Äìitem relationships.  \n",
        "Our evaluation and fine-tuning must respect this data model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìò Operational Policy (`policy.md`)**  \n",
        "This is the contract the agent must comply with.  \n",
        "When the model misbehaves (e.g., skips authentication or calls the wrong tool),  \n",
        "it is violating *this* document.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîß API Surface (`openapi_with_policy.json`)**  \n",
        "This is the schema that allows us to:\n",
        "- Generate synthetic multi-turn conversations  \n",
        "- Define evaluation logic  \n",
        "- Align fine-tuning data with real-world policy rules  \n",
        "\n",
        "---\n",
        "\n",
        "Together, these three files form the foundation for the rest of the notebook.  \n",
        "Everything that follows‚Äîbaseline failures, synthetic generation, evaluation, SFT, and even RFT‚Äîis fundamentally about:\n",
        "\n",
        "> **Teaching the model to follow the policy and manipulate this data correctly through tool calls.**\n",
        "\n",
        "We now have all the ingredients needed to explore how well the base model performs‚Äîand where it breaks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d627fb9",
      "metadata": {},
      "source": [
        "## 4. Baseline Agent Behavior & Failures \n",
        "\n",
        "Before we start generating data and fine-tuning, we need to see how the **base model** behaves in realistic scenarios.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. Describe a **simple success case** (product listing)  \n",
        "2. Walk through a **real failure trace** from an address-update scenario  \n",
        "3. Highlight exactly *what* went wrong (tool-calling + parameter wiring)  \n",
        "4. Use this as the motivating example for our evaluation and fine-tuning pipeline\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1 Simple Success: ‚ÄúShow me all your products‚Äù\n",
        "\n",
        "As a warm-up, the Zava agent does quite well on **simple single-tool queries**.\n",
        "\n",
        "For example, from the Microsoft Foundry agent playground or the `retail_agent` CLI, a user might say:\n",
        "\n",
        "> ‚ÄúHi, can you show me all your products?‚Äù\n",
        "\n",
        "The agent typically responds by:\n",
        "\n",
        "1. Interpreting the request as a product-browsing intent  \n",
        "2. Calling the appropriate MCP tool (e.g., `list_products`) **with no parameters**  \n",
        "3. Returning a friendly, paginated list of products to the user  \n",
        "\n",
        "These simple flows build confidence that:\n",
        "\n",
        "- The **MCP server** is wired correctly  \n",
        "- The **agent configuration** (tools, system prompt) is working  \n",
        "- The chosen base model (e.g., `gpt-4.1-mini`) can perform basic tool selection and response formatting\n",
        "\n",
        "However, this is not where most real-world failures occur.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 A More Realistic Scenario: Update My Address\n",
        "\n",
        "Now consider a more realistic, policy-sensitive scenario.\n",
        "\n",
        "A returning customer wants to **update their address** after moving:\n",
        "\n",
        "> ‚ÄúHi, I want to update my address.‚Äù\n",
        "\n",
        "From the policy, we know the agent should:\n",
        "\n",
        "1. **Authenticate the user**  \n",
        "   - Ask for email, or  \n",
        "   - Ask for first name, last name, and ZIP code  \n",
        "2. Use `find_user_id_by_name_zip` or `find_user_id_by_email`  \n",
        "3. Immediately call `get_user_details` using the returned `user_id`  \n",
        "4. Show the current address and ask the user to confirm the new one  \n",
        "5. Call `update_address` with the correct `user_id` and new address  \n",
        "6. Confirm the update\n",
        "\n",
        "We captured a real conversation where the **base `gpt-4.1-mini` model** was used with the Zava agent and MCP server, and the agent did *not* behave correctly.  \n",
        "We‚Äôll load and inspect that trace next.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 Live Terminal Run (Cinematic Reveal) // NEEDS REVISION\n",
        "\n",
        "Below is a short recording of the real terminal interaction using:\n",
        "\n",
        "\n",
        "![Retail Agent Baseline Run](img/retail_agent_baseline.gif)\n",
        "\n",
        "\n",
        "**Loading the Baseline Failure Trace**\n",
        "\n",
        "The conversation is stored in a JSON file with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"metadata\": {\n",
        "    \"start_time\": \"...\",\n",
        "    \"end_time\": \"...\",\n",
        "    \"model\": \"gpt-4.1-mini\",\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.95\n",
        "  },\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"...\", \"timestamp\": \"...\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"...\", \"timestamp\": \"...\" },\n",
        "    { \"role\": \"tool\", \"tool_name\": \"...\", \"arguments\": {...}, \"output\": {...}, \"timestamp\": \"...\" },\n",
        "    ...\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94b1bfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def load_conversation(path: Path):\n",
        "    with path.open() as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def pretty_print_conversation(conv: dict, max_turns: int = 8):\n",
        "    \"\"\"\n",
        "    Nicely prints the first `max_turns` messages in a conversation.\n",
        "    Handles user, assistant, and tool messages.\n",
        "    \"\"\"\n",
        "    messages = conv.get(\"messages\", [])\n",
        "    for i, m in enumerate(messages[:max_turns]):\n",
        "        role = m.get(\"role\")\n",
        "        print(f\"--- Turn {i} [{role}] ---\")\n",
        "        if role in (\"user\", \"assistant\"):\n",
        "            print(m.get(\"content\", \"\"), \"\\n\")\n",
        "        elif role == \"tool\":\n",
        "            tool_name = m.get(\"tool_name\")\n",
        "            args = m.get(\"arguments\", {})\n",
        "            output = m.get(\"output\", {})\n",
        "            print(f\"Tool: {tool_name}\")\n",
        "            print(f\"Arguments: {args}\")\n",
        "            print(f\"Output: {output}\\n\")\n",
        "        else:\n",
        "            print(m, \"\\n\")\n",
        "\n",
        "# Update this path to match your repo structure if needed\n",
        "conv_path = Path(\".\") / \"conversations\" / \"address_update_failure_base.json\"\n",
        "baseline_conv = load_conversation(conv_path)\n",
        "\n",
        "print(\"Model:\", baseline_conv[\"metadata\"][\"model\"])\n",
        "print(\"Start time:\", baseline_conv[\"metadata\"][\"start_time\"])\n",
        "print(\"\\n=== First few turns ===\\n\")\n",
        "pretty_print_conversation(baseline_conv, max_turns=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d0f681",
      "metadata": {},
      "source": [
        "### 4.4 What Went Wrong: Incorrect Parameter Wiring\n",
        "\n",
        "Looking at the printed conversation, we can see the following pattern:\n",
        "\n",
        "1. The user asks to update their address.  \n",
        "2. The assistant correctly asks for identifying information (name + ZIP).  \n",
        "3. The user responds: e.g., ‚ÄúMy name is Noah Brown and ZIP is 80279‚Äù.  \n",
        "4. The agent calls the tool:\n",
        "\n",
        "   ```text\n",
        "   Tool: find_user_id_by_name_zip\n",
        "   Arguments: { \"first_name\": \"Noah\", \"last_name\": \"Brown\", \"zip\": \"80279\" }\n",
        "   Output: \"noah_brown_6181\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7254fd9",
      "metadata": {},
      "source": [
        "        ‚úÖ This is correct ‚Äî the tool returns a valid user_id.\n",
        "\n",
        "        Later, when the user says ‚Äúcan you show me my current details first‚Äù, the agent calls:\n",
        "\n",
        "        Tool: get_user_details\n",
        "        Arguments: { \"user_id\": \"noah_brown_80279\" }\n",
        "        Output: { \"error\": \"User not found.\" }\n",
        "\n",
        "\n",
        "        ‚ùå This is incorrect ‚Äî instead of using the returned user_id (noah_brown_6181),\n",
        "        the model hallucinates a user id that encodes the ZIP code (noah_brown_80279).\n",
        "\n",
        "As a result:\n",
        "\n",
        "The get_user_details call fails with ‚ÄúUser not found.‚Äù\n",
        "\n",
        "The agent apologizes and claims it cannot find the profile\n",
        "\n",
        "The user never sees their current address, and the flow breaks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb48711f",
      "metadata": {},
      "source": [
        "### 4.5 Why This Failure Matters\n",
        "\n",
        "This failure is important because it highlights the *real* challenges of building reliable tool-calling agents:\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚ùå It‚Äôs not a natural language problem**  \n",
        "The model *understands* the request.  \n",
        "It *knows* what ‚Äúupdate my address‚Äù means.  \n",
        "It *knows* it must authenticate the user first.\n",
        "\n",
        "This is not about phrasing, intent classification, or language quality.\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚ùå It‚Äôs not a tool-selection problem**  \n",
        "The model chooses the **correct tool**:\n",
        "- `find_user_id_by_name_zip` ‚Üí correct  \n",
        "- Should call `get_user_details` next ‚Üí correct  \n",
        "\n",
        "The failure is not about *which* tool to use.\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚ùó It *is* a tool-argument propagation problem**  \n",
        "This is where the model fails:\n",
        "\n",
        "- It receives a valid `user_id` from the first tool  \n",
        "- But instead of reusing it, the model *hallucinates* a new ID (`noah_brown_80279`)  \n",
        "- The second tool (`get_user_details`) fails  \n",
        "- The entire flow collapses\n",
        "\n",
        "This is a **structural mistake** ‚Äî not linguistic.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìò Policy Compliance Failure**  \n",
        "From `policy.md`, the agent must:\n",
        "\n",
        "1. Authenticate the user  \n",
        "2. Immediately retrieve the user profile  \n",
        "3. Correctly propagate the returned `user_id`  \n",
        "4. Never make up identifiers  \n",
        "5. Never fabricate different users mid-conversation  \n",
        "\n",
        "The base model violates multiple rules here.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîß Data Model Consistency Failure**  \n",
        "The hallucinated user ID does not exist in `db.json`.  \n",
        "This means:\n",
        "\n",
        "- The model is not grounding the conversation in the true backend  \n",
        "- It is not treating tool outputs as authoritative  \n",
        "- It breaks multi-turn statefulness\n",
        "\n",
        "---\n",
        "\n",
        "#### **üéØ Why this matters for finetuning**  \n",
        "Failures like this are:\n",
        "\n",
        "- **Common across all base models** (even strong ones)  \n",
        "- **Critical for real-world agents** (identity, payments, returns)  \n",
        "- **Exactly the kind of behavior** SFT + evaluations can fix  \n",
        "- **Perfect candidates** for RFT with policy-aware grading  \n",
        "\n",
        "This single failure trace becomes the **motivating example** for the rest of the notebook:\n",
        "\n",
        "- We need **structured evaluation** to detect this class of issues  \n",
        "- We need **synthetic data** to generate enough examples  \n",
        "- We need **fine-tuning** to teach the model correct stateful behavior  \n",
        "\n",
        "In the next section, we convert this failure into **measurable evaluation metrics**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47731b3b",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è 4.6 Important Note: Model Variability & Intentional Behavior Adjustments\n",
        "\n",
        "It‚Äôs important to clarify that the failure shown in this example is **not guaranteed** to occur for every model or every run.\n",
        "\n",
        "In our testing environment, this issue appears **consistently** with the `gpt-4.1-mini` model under the exact Zava agent setup shown above; however:\n",
        "\n",
        "- Other models may propagate the `user_id` correctly  \n",
        "- Some runs may behave correctly even with the same model  \n",
        "- Environmental differences (system prompt tweaks, sampling parameters, MCP latency) can change outcomes  \n",
        "\n",
        "The takeaway is not that *every* model will hallucinate a user ID, but rather that:\n",
        "\n",
        "> **This class of failure‚Äîincorrect argument propagation, missing intermediate calls, or skipped policy steps‚Äîis common in multi-turn tool-calling agents, and is exactly what fine-tuning and structured evaluation are designed to improve.**\n",
        "\n",
        "---\n",
        "\n",
        "#### üí° Not Just About Fixing Errors ‚Äî Also About Shaping Agent Behavior\n",
        "\n",
        "Even if the model *were* propagating `user_id` correctly, we might still want to enforce a stricter‚Äîor more helpful‚Äîbehavior.\n",
        "\n",
        "For example:\n",
        "\n",
        "- You may prefer that after authentication,  \n",
        "  **`get_user_details` is always called immediately**,  \n",
        "  _even if the user didn‚Äôt explicitly ask for their profile_.  \n",
        "- This ensures that the customer always sees their current information before confirming changes.  \n",
        "- It also creates a more consistent user experience and reduces ambiguity.\n",
        "\n",
        "In such cases:\n",
        "\n",
        "- The ‚Äúfix‚Äù is not about correcting model mistakes  \n",
        "- It‚Äôs about **aligning the agent with your product expectations**  \n",
        "- And the process (synthetic data ‚Üí evaluation ‚Üí fine-tuning) is the same\n",
        "\n",
        "---\n",
        "\n",
        "#### üéØ Why This Matters\n",
        "\n",
        "This disclaimer highlights the general principle behind this entire demo:\n",
        "\n",
        "- **Fine-tuning is not only for correcting failure cases**  \n",
        "- It is also a powerful tool for **intentionally shaping the agent's policy, flow, and UX**\n",
        "\n",
        "Whether the model fails, or simply behaves in a way you want to refine:\n",
        "\n",
        "> **The evaluation + synthetic data + fine-tuning pipeline gives you reliable control over multi-turn agent behavior.**\n",
        "\n",
        "This sets the stage for the next section, where we convert these behavioral expectations into **structured evaluation metrics**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bedc52f",
      "metadata": {},
      "source": [
        "## 5. Synthetic Data Generation with Microsoft Foundry\n",
        "\n",
        "To improve the Zava Retail Agent‚Äôs tool-calling behavior, we need **high-quality training and evaluation data** that:\n",
        "\n",
        "- Exercises the full **tool surface** (lookup, profile, orders, returns, exchanges, address updates)  \n",
        "- Respects the **policy** in `policy.md`  \n",
        "- Reflects the **data model** in `db.json`  \n",
        "- Contains **multi-turn conversations** with realistic tool-call chains\n",
        "\n",
        "Instead of authoring hundreds of conversations manually, we will use the **Synthetic Data Generation** capabilities in Microsoft Foundry, driven by our `openapi_with_policy.json` spec.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Goal for Synthetic Data\n",
        "\n",
        "For this notebook and demo, we aim to generate:\n",
        "\n",
        "- ~**1500 multi-turn conversations** that:\n",
        "  - Involve realistic user intents (profile lookup, address update, order status, returns, exchanges, etc.)\n",
        "  - Use tools like `find_user_id_by_name_zip`, `get_user_details`, `get_order_details`, `update_address`,\n",
        "    `cancel_pending_order`, `exchange_delivered_order_items`, and `list_products`\n",
        "  - Follow the **policy rules** baked into `policy.md` and `openapi_with_policy.json`\n",
        "- Later, we will:\n",
        "  - Use ~**500 conversations** (expanded into per-tool-call records) as an **evaluation set**\n",
        "  - Use the remaining conversations as **SFT training data** for fine-tuning models\n",
        "\n",
        "The core idea:\n",
        "\n",
        "> **Let the Synthetic Data Service generate many realistic, policy-respecting examples of the *behavior we want the agent to learn*.**\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Source of Truth: `openapi_with_policy.json`\n",
        "\n",
        "We previously introduced `openapi_with_policy.json`, which:\n",
        "\n",
        "- Defines the **endpoints** corresponding to the MCP tools  \n",
        "- Embeds the **policy text** and behavioral constraints into descriptions  \n",
        "- Encodes parameters and response shapes for actions like:\n",
        "  - User lookup  \n",
        "  - Order retrieval  \n",
        "  - Returns and exchanges  \n",
        "  - Address updates\n",
        "\n",
        "The Synthetic Data Service will use this spec to:\n",
        "\n",
        "- Understand what actions are possible  \n",
        "- Respect preconditions / postconditions (e.g., only `pending` orders can be cancelled)  \n",
        "- Generate realistic **user ‚Üî assistant** turns with structured tool-calls underneath\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Creating a Synthetic Dataset in Microsoft Foundry (UI)\n",
        "\n",
        "> **Note:** At the time of writing, Synthetic Data Generation for this scenario is **UI-first**.  \n",
        "> This notebook uses pre-generated synthetic train/test files (included in the `data/` folder),  \n",
        "> but the steps below describe exactly how to reproduce them.\n",
        "\n",
        "High-level steps in the Microsoft Foundry Studio:\n",
        "\n",
        "1. **Open your Microsoft Foundry Project**  \n",
        "   - Navigate to the same Project used for your Zava Retail Agent deployment.\n",
        "\n",
        "2. **Navigate to Synthetic Data**  \n",
        "   - In the left navigation pane, select:  \n",
        "     **Data ‚Üí Synthetic data**\n",
        "\n",
        "3. **Create a new synthetic data job**  \n",
        "   - Click **+ New synthetic dataset**  \n",
        "   - Choose **‚ÄúFrom API spec / OpenAPI‚Äù**  \n",
        "   - Upload your `openapi_with_policy.json` file from the `data/` folder.  \n",
        "   - The service will automatically infer all operations, allowed parameters, and policy constraints.\n",
        "\n",
        "4. **Configure generation settings**  \n",
        "   - Set the **Task Type** to **‚ÄúTool Use‚Äù**.  \n",
        "   - Choose a **Generator Model**‚Äîlarger models (e.g., **gpt-4.1**, **gpt-5**) typically produce more coherent multi-turn conversations.  \n",
        "   - Currently, the UI does **not** allow configuration of:\n",
        "     - Specific scenario types (returns, authentication, etc.)  \n",
        "     - Conversation length or number of turns  \n",
        "     - Tool diversity or frequency  \n",
        "   - These features may arrive in a future release.  \n",
        "     For now, the variation is driven by your **OpenAPI schema** and embedded **policy.md** rules.\n",
        "\n",
        "5. **Generate and export the dataset**  \n",
        "   - Start the job and wait for completion.  \n",
        "   - The service will produce:\n",
        "     - A **training split**  \n",
        "     - A **validation/test split**  \n",
        "   - You can download both splits directly as JSONL files.\n",
        "   - Save them into the repository under, for example:\n",
        "\n",
        "     ```text\n",
        "     data/synthetic/datagen-ToolUse-FineTuneSupervised-train.jsonl  \n",
        "     data/synthetic/datagen-ToolUse-FineTuneSupervised-valid.jsonl\n",
        "     ```\n",
        "\n",
        "6. **(Optional but recommended) Inspect the generated data**  \n",
        "   - Once the train/test files are downloaded, open them in this notebook to:\n",
        "     - Review example conversations  \n",
        "     - Compute statistics (turn counts, tool-call frequencies, etc.)  \n",
        "     - Visualize distribution patterns (e.g., which tools appear most)  \n",
        "\n",
        "In this notebook, we assume you have already exported the dataset.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e174742c",
      "metadata": {},
      "source": [
        "### 5.4 Synthetic Dataset Overview (Dashboard)\n",
        "\n",
        "Before we use this synthetic data for evaluation and fine-tuning, it‚Äôs useful to sanity-check:\n",
        "\n",
        "- How long the conversations are  \n",
        "- How many tool calls each conversation uses  \n",
        "- Which tools are being exercised most often  \n",
        "\n",
        "The cell below:\n",
        "\n",
        "1. Loads the synthetic **train** and **valid** splits generated by Microsoft Foundry  \n",
        "2. Computes basic statistics per split  \n",
        "3. Renders three quick visualizations:\n",
        "   - Conversation length distribution (train vs valid)  \n",
        "   - Tool calls per conversation (train vs valid)  \n",
        "   - Top tools used in the train split  \n",
        "\n",
        "This helps verify that:\n",
        "\n",
        "- Conversations are reasonably long for multi-turn behavior learning  \n",
        "- Each conversation exercises several tools (not just one-shot calls)  \n",
        "- Key tools like `find_user_id_by_email`, `get_user_details`, and `get_order_details` are well represented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3407bf1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths to the synthetic train/valid files generated in Microsoft Foundry\n",
        "# Adjust these if your repo layout is different.\n",
        "data_dir = Path(\"data\")\n",
        "train_path = data_dir / \"datagen-ToolUse-FineTuneSupervised-train.jsonl\"\n",
        "valid_path = data_dir / \"datagen-ToolUse-FineTuneSupervised-valid.jsonl\"\n",
        "\n",
        "\n",
        "def load_jsonl(path: Path):\n",
        "    rows = []\n",
        "    with path.open() as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "train = load_jsonl(train_path)\n",
        "valid = load_jsonl(valid_path)\n",
        "\n",
        "print(f\"Train conversations: {len(train)}\")\n",
        "print(f\"Valid conversations: {len(valid)}\")\n",
        "\n",
        "\n",
        "def summarize(convs):\n",
        "    num_turns = []\n",
        "    tool_counts = []\n",
        "    tool_names = Counter()\n",
        "\n",
        "    for conv in convs:\n",
        "        msgs = conv.get(\"messages\", [])\n",
        "        num_turns.append(len(msgs))\n",
        "\n",
        "        tc_per_conv = 0\n",
        "        for m in msgs:\n",
        "            if m.get(\"role\") == \"assistant\" and \"tool_calls\" in m:\n",
        "                calls = m[\"tool_calls\"] or []\n",
        "                tc_per_conv += len(calls)\n",
        "\n",
        "                for c in calls:\n",
        "                    fn = c.get(\"function\", {}).get(\"name\")\n",
        "                    if fn:\n",
        "                        tool_names[fn] += 1\n",
        "\n",
        "        tool_counts.append(tc_per_conv)\n",
        "\n",
        "    return {\n",
        "        \"num_turns\": num_turns,\n",
        "        \"tool_counts\": tool_counts,\n",
        "        \"tool_names\": tool_names,\n",
        "    }\n",
        "\n",
        "\n",
        "train_stats = summarize(train)\n",
        "valid_stats = summarize(valid)\n",
        "\n",
        "\n",
        "def make_df(stats, split_name):\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"split\": split_name,\n",
        "            \"num_turns\": stats[\"num_turns\"],\n",
        "            \"tool_calls\": stats[\"tool_counts\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "df_train = make_df(train_stats, \"train\")\n",
        "df_valid = make_df(valid_stats, \"valid\")\n",
        "df_all = pd.concat([df_train, df_valid], ignore_index=True)\n",
        "\n",
        "print(\"\\n=== Basic stats (train + valid combined) ===\")\n",
        "display(df_all.describe().round(2))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Conversation length distribution (train vs valid)\n",
        "# -------------------------------------------------------------------\n",
        "plt.figure()\n",
        "for split, sub in df_all.groupby(\"split\"):\n",
        "    plt.hist(sub[\"num_turns\"], bins=range(10, 25), alpha=0.5, label=split)\n",
        "plt.xlabel(\"Number of turns per conversation\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Conversation length distribution (train vs valid)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Tool calls per conversation (train vs valid)\n",
        "# -------------------------------------------------------------------\n",
        "plt.figure()\n",
        "for split, sub in df_all.groupby(\"split\"):\n",
        "    plt.hist(sub[\"tool_calls\"], bins=range(0, 8), alpha=0.5, label=split)\n",
        "plt.xlabel(\"Number of tool calls per conversation\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Tool calls per conversation (train vs valid)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Top tools in the train split\n",
        "# -------------------------------------------------------------------\n",
        "tool_df = pd.DataFrame(\n",
        "    [{\"tool_name\": k, \"count\": v} for k, v in train_stats[\"tool_names\"].items()]\n",
        ").sort_values(\"count\", ascending=False)\n",
        "\n",
        "plt.figure()\n",
        "top = tool_df.head(10).sort_values(\"count\")\n",
        "plt.barh(top[\"tool_name\"], top[\"count\"])\n",
        "plt.xlabel(\"Count in train split\")\n",
        "plt.title(\"Top tool usage in synthetic train data\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323d9472",
      "metadata": {},
      "source": [
        "### 5.5 Tool Call Sequence Transitions (Heatmap) // NEEDS REVISION\n",
        "\n",
        "Beyond aggregate tool frequencies, it‚Äôs useful to understand **how tools are chained together** inside each conversation.\n",
        "\n",
        "In this section, we look at **pairwise tool transitions**:\n",
        "\n",
        "- For each synthetic conversation, we extract the sequence of tool calls in order  \n",
        "- For each adjacent pair `(current_tool ‚Üí next_tool)`, we count how often that transition occurs  \n",
        "- We then visualize these counts as a **heatmap**, where:\n",
        "  - Rows = current tool  \n",
        "  - Columns = next tool  \n",
        "  - Brighter cells = more frequent transitions  \n",
        "\n",
        "This highlights patterns such as:\n",
        "\n",
        "- Strong transitions like `find_user_id_by_email ‚Üí get_user_details`  \n",
        "- Common follow-ups after `get_user_details` (e.g., `get_order_details`, `update_address`, etc.)  \n",
        "- Whether the synthetic data actually teaches the **multi-step flows** we care about.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb7dabc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reuse the synthetic train split (you can also include valid if you‚Äôd like)\n",
        "data_dir = Path(\"data\")\n",
        "#train_path = data_dir / \"datagen-ToolUse-FineTuneSupervised-train.jsonl\"\n",
        "train_path = data_dir / \"sft_test.jsonl\"\n",
        "\n",
        "\n",
        "def load_jsonl(path: Path):\n",
        "    rows = []\n",
        "    with path.open() as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "train = load_jsonl(train_path)\n",
        "print(f\"Loaded {len(train)} train conversations for transition analysis.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Build pairwise tool transition counts\n",
        "# -------------------------------------------------------------------\n",
        "transition_counts = Counter()\n",
        "all_tools = set()\n",
        "\n",
        "for conv in train:\n",
        "    messages = conv.get(\"messages\", [])\n",
        "    tool_sequence = []\n",
        "\n",
        "    for m in messages:\n",
        "        if m.get(\"role\") == \"assistant\" and \"tool_calls\" in m:\n",
        "            calls = m[\"tool_calls\"] or []\n",
        "            for c in calls:\n",
        "                # Synthetic export uses OpenAI-style function tool calls\n",
        "                fn_name = c.get(\"function\", {}).get(\"name\")\n",
        "                if fn_name:\n",
        "                    tool_sequence.append(fn_name)\n",
        "                    all_tools.add(fn_name)\n",
        "\n",
        "    # Count adjacent transitions within this conversation\n",
        "    for i in range(len(tool_sequence) - 1):\n",
        "        pair = (tool_sequence[i], tool_sequence[i + 1])\n",
        "        transition_counts[pair] += 1\n",
        "\n",
        "# If no transitions, bail out gracefully\n",
        "if not transition_counts:\n",
        "    print(\"No tool transitions found. Check the tool_calls schema in your synthetic data.\")\n",
        "else:\n",
        "    # -------------------------------------------------------------------\n",
        "    # Prepare matrix\n",
        "    # -------------------------------------------------------------------\n",
        "    tool_list = sorted(all_tools)\n",
        "    index = {name: idx for idx, name in enumerate(tool_list)}\n",
        "\n",
        "    n = len(tool_list)\n",
        "    matrix = np.zeros((n, n), dtype=int)\n",
        "\n",
        "    for (src, dst), count in transition_counts.items():\n",
        "        i = index[src]\n",
        "        j = index[dst]\n",
        "        matrix[i, j] = count\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Plot heatmap\n",
        "    # -------------------------------------------------------------------\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    im = plt.imshow(matrix, interpolation=\"nearest\")\n",
        "    plt.colorbar(im, label=\"Transition Count\")\n",
        "\n",
        "    plt.xticks(range(n), tool_list, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(n), tool_list)\n",
        "\n",
        "    plt.xlabel(\"Next Tool\")\n",
        "    plt.ylabel(\"Current Tool\")\n",
        "    plt.title(\"Tool Call Sequence Transitions (Heatmap)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725f9e18",
      "metadata": {},
      "source": [
        "### 5.6 Additional Tools & Customizing the Synthetic Dataset\n",
        "\n",
        "The synthetic dataset generated through Microsoft Foundry is a strong starting point, but in many real production scenarios you will want deeper analysis, targeted filtering, or custom refinement before using it for fine-tuning. This project includes two useful resources to help with that.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç 5.6.1 Full Analysis Script (Multi-Chart Dashboard)\n",
        "\n",
        "For richer visualization and diagnostics, this repository includes a standalone script:\n",
        "\n",
        "```bash\n",
        "python ./tools/analyze_synthetic_datagen.py\n",
        "```\n",
        "\n",
        "Running this script will generate a comprehensive set of charts summarizing:\n",
        "\n",
        "- Conversation lengths  \n",
        "- Tool-call frequencies  \n",
        "- Tool-call transitions (pairwise)  \n",
        "- Overall tool-call density  \n",
        "- Policy-heavy flows (returns, exchanges, address changes)  \n",
        "- Distribution of user intents  \n",
        "- Per-turn breakdowns for multi-step conversations  \n",
        "\n",
        "The generated charts are written to:\n",
        "\n",
        "```bash\n",
        "analysis_charts/data_gen/\n",
        "```\n",
        "\n",
        "\n",
        "These visualizations are helpful for:\n",
        "\n",
        "- Verifying that the synthetic generator is producing the flows you expect  \n",
        "- Detecting anomalies or rarely-used tools  \n",
        "- Understanding whether the distribution matches your real production traffic  \n",
        "\n",
        "---\n",
        "\n",
        "#### üõ†Ô∏è 5.6.2 Filtering & Curating the Synthetic Dataset (Recommended)\n",
        "\n",
        "Even with a high-quality generator, synthetic datasets often benefit from **light curation** before using them for SFT or RFT pipelines.\n",
        "\n",
        "For this project, we performed an additional filtering step:\n",
        "\n",
        "- Regenerated the synthetic dataset several times  \n",
        "- Selected only conversations that demonstrate the **behavior we want to reinforce**  \n",
        "- Specifically, we filtered for cases where the model:\n",
        "  - Invokes a `find_*` tool (e.g., `find_user_id_by_email`, `find_user_id_by_name_zip`)  \n",
        "  - Immediately follows with a correct `get_user_details` call  \n",
        "  - Correctly propagates tool arguments (e.g., consistent `user_id`, no hallucinations)  \n",
        "\n",
        "These curated subsets are saved as:\n",
        "\n",
        "```bash\n",
        "data/sft_train.json\n",
        "data/sft_test.json\n",
        "```\n",
        "\n",
        "\n",
        "This filtered dataset is cleaner and more focused on teaching the structured multi-turn behaviors we want the agent to reliably follow.  \n",
        "You can further refine or adjust these files depending on your product constraints‚Äîfor example:\n",
        "\n",
        "- Enforcing correct parameter propagation  \n",
        "- Ensuring every conversation ends with a tool call  \n",
        "- Removing outliers or overly long conversations  \n",
        "- Adding or removing specific scenarios (such as forcing more return/exchange flows)\n",
        "\n",
        "This curation step is optional but highly recommended if you want deterministic improvements in tool-calling performance.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úîÔ∏è Summary\n",
        "\n",
        "At this point, we now have:\n",
        "\n",
        "- A full synthetic dataset (train + valid) generated via Microsoft Foundry  \n",
        "- A built-in dashboard and heatmap analysis inside this notebook  \n",
        "- A standalone visualization script for deeper offline analysis  \n",
        "- Curated SFT-ready datasets (`sft_train.json`, `sft_test.json`) focused on teaching the correct multi-tool sequence patterns\n",
        "\n",
        "With the data prepared and validated, we are ready to move on to **Section 6**, where we define Python-based evaluation graders to measure tool-calling accuracy before and after fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea1b107a",
      "metadata": {},
      "source": [
        "## 6. Evaluation Pipeline: Measuring Next Tool-Call Accuracy\n",
        "\n",
        "**Evaluation Overview**\n",
        "\n",
        "Before fine-tuning, we want a baseline measurement of how well the model\n",
        "chooses the correct next tool call in realistic multi-turn conversations.\n",
        "To do that, we build an evaluation dataset (one sample per tool-call\n",
        "decision), then run a Python-based grader to measure accuracy for each\n",
        "model or deployment.\n",
        "\n",
        "Fine-tuning is only meaningful if we can **measure** whether the model‚Äôs tool-calling behavior has actually improved.  \n",
        "In this section, we build a Python-based evaluation pipeline that scores the **next tool call** the model should make.\n",
        "\n",
        "This evaluation is intentionally **focused and surgical**:\n",
        "\n",
        "- We are *not* grading text quality  \n",
        "- We are *not* grading full multi-turn flows  \n",
        "- We are *only* grading whether the model invokes the **correct next tool** with the **correct arguments** at a specific point in the conversation.\n",
        "\n",
        "Evaluation pipeline:\n",
        "\n",
        "Synthetic Test Set ‚Üí (expand) ‚Üí eval.jsonl ‚Üí (submit eval)\n",
        "‚Üí model generates next tool_call ‚Üí grader ‚Üí accuracy score\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 6.1 Per-Tool-Call Evaluation Design\n",
        "\n",
        "The synthetic test dataset originally consists of **multi-turn conversations** with multiple tool calls per scenario.\n",
        "\n",
        "For evaluation, we **expand** this into a dedicated per-tool-call dataset:\n",
        "\n",
        "- For each conversation, we locate each tool call  \n",
        "- For each tool call, we create a separate evaluation item where:\n",
        "  - The **messages** include the conversation history **up to, but not including** that tool call  \n",
        "  - The **expected_output** contains the *single* assistant tool call that should happen next\n",
        "\n",
        "Conceptually, each evaluation record asks:\n",
        "\n",
        "> ‚ÄúGiven this conversation so far, what tool call should the assistant make next?‚Äù\n",
        "\n",
        "This gives us:\n",
        "\n",
        "- Fine-grained, per-tool-call evaluation  \n",
        "- Thousands of independent evaluation samples  \n",
        "- A clean, unambiguous notion of correctness\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7377a50a",
      "metadata": {},
      "source": [
        "### 6.2 Expanding the Synthetic Test Set into an Evaluation Dataset\n",
        "\n",
        "The synthetic data generator produces a **test split** where each record is a full multi-turn conversation that may contain **multiple tool calls**.  \n",
        "For evaluation, we want to test the model at **each tool-call decision point**:\n",
        "\n",
        "> ‚ÄúGiven the conversation so far, what tool call should the assistant produce next?‚Äù\n",
        "\n",
        "To achieve this, we **expand** the test dataset so that each tool call becomes its own evaluation record.\n",
        "\n",
        "We provide a script for this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c53da49",
      "metadata": {},
      "source": [
        "\n",
        "This script:\n",
        "\n",
        "1. Loads the synthetic test dataset (`valid.jsonl`)\n",
        "2. Scans each conversation‚Äôs `messages` for assistant tool calls\n",
        "3. For each tool call:\n",
        "   - Truncates the conversation up to that point  \n",
        "   - Removes the tool call from the assistant‚Äôs message  \n",
        "   - Places that tool call into `expected_output.tool_calls`\n",
        "4. Writes the expanded dataset to an eval-ready file such as `eval.jsonl`\n",
        "\n",
        "The resulting evaluation items match the structure expected by Microsoft Foundry and your Python grader.  \n",
        "Each record in `eval.jsonl` looks like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"item\": {\n",
        "    \"messages\": [\n",
        "      { \"role\": \"system\", \"content\": \"<system prompt>\" },\n",
        "      { \"role\": \"user\", \"content\": \"Hi, I moved recently and need to update my address.\" },\n",
        "      { \"role\": \"assistant\", \"content\": \"Sure, what's your full name and ZIP code?\" },\n",
        "      { \"role\": \"user\", \"content\": \"Noah Brown, 80279.\" }\n",
        "    ],\n",
        "    \"tools\": [\n",
        "      {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": { \"name\": \"find_user_id_by_name_zip\", \"parameters\": { \"...\": \"...\" } }\n",
        "      },\n",
        "      {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": { \"name\": \"get_user_details\", \"parameters\": { \"...\": \"...\" } }\n",
        "      }\n",
        "    ],\n",
        "    \"expected_output\": {\n",
        "      \"role\": \"assistant\",\n",
        "      \"tool_calls\": [\n",
        "        {\n",
        "          \"type\": \"function\",\n",
        "          \"function\": {\n",
        "            \"name\": \"get_user_details\",\n",
        "            \"arguments\": \"{\\\"user_id\\\": \\\"noah_brown_6181\\\"}\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This expanded format allows us to evaluate every tool-call decision independently and measure improvements in tool-calling accuracy after SFT or RFT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05955c9",
      "metadata": {},
      "source": [
        "### 6.3 Python-Based Grader (Deterministic, No LLM Involved)\n",
        "\n",
        "This evaluation uses a **pure Python grader** rather than a model-based grader.  \n",
        "This keeps the evaluation:\n",
        "\n",
        "- **Deterministic** ‚Äî the same input always produces the same score  \n",
        "- **Fast** ‚Äî no additional model calls are needed  \n",
        "- **Transparent** ‚Äî you can inspect exactly why a prediction passed or failed  \n",
        "- **Focused** ‚Äî we judge *only* the correctness of the next tool call\n",
        "\n",
        "The grader accepts two objects:\n",
        "\n",
        "- `item` ‚Üí the evaluation record from `eval.jsonl`, containing the expected tool call  \n",
        "- `sample` ‚Üí the model output captured by Microsoft Foundry (`output_tools`)\n",
        "\n",
        "The grader performs three steps:\n",
        "\n",
        "1. **Extract the expected tool call**\n",
        "   - `item[\"expected_output\"][\"tool_calls\"]`  \n",
        "   - Arguments are stored as JSON strings and normalized\n",
        "\n",
        "2. **Extract the model-predicted tool call**\n",
        "   - `sample[\"output_tools\"]`  \n",
        "   - Parsed and normalized before comparison\n",
        "\n",
        "3. **Score the tool call**\n",
        "   - `1.0` ‚Üí correct tool **and** correct arguments  \n",
        "   - `0.5` ‚Üí correct tool but arguments differ  \n",
        "   - `0.0` ‚Üí wrong tool, missing tool, or malformed call  \n",
        "\n",
        "This scoring is intentionally lightweight and surgical ‚Äî it measures *exactly* what we care about for supervised fine-tuning:  \n",
        "**Did the model choose the correct next tool with the correct arguments?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3fd4d46",
      "metadata": {},
      "source": [
        "### 6.4 Submitting an Evaluation Run in Microsoft Foundry\n",
        "\n",
        "With the evaluation dataset (`eval.jsonl`) and Python grader (`tool_call_grader.py`) ready, we can now submit an evaluation run to Microsoft Foundry and measure tool-calling accuracy for one or more models.\n",
        "\n",
        "You can also do this from Microsoft Foundry UI directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc3d108",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Upload evaluation file to Azure OpenAI\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from openai import AzureOpenAI\n",
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "\n",
        "# Configuration\n",
        "resource_name = os.getenv(\"FOUNDRY_RESOURCE_NAME\")\n",
        "eval_file = Path(\"data/sft_test_eval_expanded.jsonl\")\n",
        "\n",
        "# Create client\n",
        "token_provider = get_bearer_token_provider(\n",
        "    DefaultAzureCredential(),\n",
        "    \"https://cognitiveservices.azure.com/.default\"\n",
        ")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=f\"https://{resource_name}.openai.azure.com\",\n",
        "    azure_ad_token_provider=token_provider,\n",
        "    api_version=\"2025-04-01-preview\"\n",
        ")\n",
        "\n",
        "# Upload file\n",
        "print(f\"Uploading {eval_file.name}...\")\n",
        "with eval_file.open(\"rb\") as f:\n",
        "    eval_file = client.files.create(file=f, purpose=\"evals\")\n",
        "\n",
        "print(f\"‚úì Uploaded successfully!\")\n",
        "print(f\"  File ID: {eval_file.id}\")\n",
        "print(f\"  Bytes: {eval_file.bytes:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53210cd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd() / \"src\"))\n",
        "\n",
        "from eval_create_util import create_azure_evaluation, create_evaluation_runs\n",
        "\n",
        "\n",
        "models_to_evaluate = [\n",
        "    \"gpt-4.1\",\n",
        "    \"gpt-4.1-mini\",\n",
        "    \"gpt-4.1-nano\"\n",
        "]\n",
        "\n",
        "evaluation = create_azure_evaluation(\n",
        "    client=client,\n",
        "    pass_threshold=1,\n",
        "    python_grader_path=\"data/tool_call_grader.py\"\n",
        ")\n",
        "\n",
        "evaluation_id = evaluation.id\n",
        "print(\"‚úÖ Evaluation setup completed\")\n",
        "\n",
        "tools_file_path = Path(\"data/retail_tools.json\")\n",
        "eval_runs = create_evaluation_runs(\n",
        "    client=client,\n",
        "    evaluation_id=evaluation_id,\n",
        "    models_to_evaluate=models_to_evaluate,\n",
        "    eval_file_id=eval_file.id,\n",
        "    tools_file_path=tools_file_path\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created {len(eval_runs)} evaluation runs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699e8b70",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "#### 6.4.2 Interpreting the Results\n",
        "\n",
        "**Analyzing Evaluation Results**\n",
        "\n",
        "**Running the Analysis Script**\n",
        "\n",
        "To analyze your evaluation results, use the `analyze_eval_run.py` script with your resource name and evaluation ID:\n",
        "\n",
        "```bash\n",
        "python tools/analyze_eval_run.py <RESOURCE_NAME> <EVAL_ID>\n",
        "```\n",
        "\n",
        "### Output Location\n",
        "\n",
        "All analysis charts and results are saved to:\n",
        "```\n",
        "analysis_charts/eval_run/\n",
        "```\n",
        "\n",
        "The script generates 9 comprehensive visualizations:\n",
        "1. `pass_rate_comparison.png` - Model pass rate comparison\n",
        "2. `score_distributions.png` - Score distribution histograms\n",
        "3. `error_distributions.png` - Error type pie charts\n",
        "4. `box_plot_comparison.png` - Statistical box plots\n",
        "5. `percentile_comparison.png` - Percentile analysis\n",
        "6. `cumulative_distribution.png` - Cumulative distribution function\n",
        "7. `comparison_table.png` - Performance metrics table\n",
        "8. `detailed_metrics_table.png` - Extended statistics\n",
        "9. `analysis_summary.json` - Complete analysis data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ecad6a",
      "metadata": {},
      "source": [
        "**Pass Rate Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27044bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Display comparison table\n",
        "display(Image(filename='analysis_charts/eval_run/comparison_table.png'))\n",
        "\n",
        "# Display pass rate comparison chart\n",
        "display(Image(filename='analysis_charts/eval_run/pass_rate_comparison.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0cc0f9",
      "metadata": {},
      "source": [
        "## 7. Supervised Fine-Tuning (SFT)\n",
        "\n",
        "In this section, we improve the Zava Retail Agent‚Äôs tool-calling behavior\n",
        "by fine-tuning a small model (**gpt-4.1-mini**) using the synthetic dataset\n",
        "we prepared earlier.\n",
        "\n",
        "The goal of this SFT run is simple:\n",
        "\n",
        "> Teach the model to reliably chain ‚Äúfind_*‚Äù ‚Üí ‚Äúget_user_details‚Äù\n",
        "> (and similar policy-driven patterns) in realistic, multi-turn conversations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c2d985",
      "metadata": {},
      "source": [
        "### 7.1 Goal of Fine-Tuning for the Zava Retail Agent\n",
        "\n",
        "The baseline evaluation results (Section 6) already show that  \n",
        "`gpt-4.1-mini` performs reasonably well on single-step tool calls,\n",
        "but struggles in **policy-dependent flows** such as:\n",
        "\n",
        "- User authentication ‚Üí profile lookup  \n",
        "- Address-change flows  \n",
        "- Cascaded tool-chaining where arguments must propagate correctly  \n",
        "\n",
        "These failure modes share a common root:\n",
        "\n",
        "> The base model does not consistently learn short multi-step tool sequences,\n",
        "> especially when the next tool depends on the *output* of the previous tool.\n",
        "\n",
        "Fine-tuning with targeted examples helps the model learn:\n",
        "\n",
        "- When to call `find_user_id_by_name_zip`\n",
        "- When to follow it immediately with `get_user_details`\n",
        "- How to propagate returned IDs\n",
        "- How to respect Zava-specific business rules baked into your policy\n",
        "\n",
        "This is the exact kind of task where SFT yields large gains with a small dataset,\n",
        "and where small models (like `gpt-4.1-mini`) benefit the most.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f03dbf",
      "metadata": {},
      "source": [
        "### 7.2 Preparing the SFT Dataset\n",
        "\n",
        "For this fine-tuning run, we start from the synthetic dataset generated in\n",
        "Section 5 and extract only the conversations that demonstrate the behavior\n",
        "we want the model to learn.\n",
        "\n",
        "We use two files (already included in the repository):\n",
        "\n",
        "```bash\n",
        "data/sft_train.jsonl\n",
        "data/sft_test.jsonl\n",
        "```\n",
        "\n",
        "\n",
        "These files contain:\n",
        "\n",
        "- Multi-turn conversations with multiple tool calls  \n",
        "- The full ‚Äúmessages‚Äù array (system, user, assistant, tool responses)  \n",
        "- The assistant‚Äôs tool calls in the correct format for SFT  \n",
        "- Patterns that reinforce the target policy behavior  \n",
        "  (e.g., mandatory `get_user_details` chaining)\n",
        "\n",
        "We do **not** expand these for SFT the way we did for evaluation.\n",
        "SFT benefits from full conversations, including intermediate turns.\n",
        "\n",
        "Let‚Äôs load a small sample from the training dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee400b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "train_path = \"data/sft_train.jsonl\"\n",
        "valid_path = \"data/sft_test.jsonl\"\n",
        "\n",
        "sft_train = load_jsonl(train_path)\n",
        "sft_valid = load_jsonl(valid_path)\n",
        "\n",
        "len(sft_train), len(sft_valid)\n",
        "\n",
        "# Pretty print the first few messages for readability\n",
        "from pprint import pprint\n",
        "sample = sft_train[0]\n",
        "pprint(sample[\"messages\"][:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc03eda",
      "metadata": {},
      "source": [
        "### 7.4 Submitting a Fine-Tuning Job via Code (Cookbook Recipe)\n",
        "\n",
        "Below is a minimal end-to-end recipe that submits a supervised fine-tuning\n",
        "job for **gpt-4.1-mini** using the same datasets:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa3c3cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from openai import AzureOpenAI\n",
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "\n",
        "# Configuration\n",
        "resource_name = os.getenv(\"FOUNDRY_RESOURCE_NAME\")\n",
        "\n",
        "# Create client\n",
        "token_provider = get_bearer_token_provider(\n",
        "    DefaultAzureCredential(),\n",
        "    \"https://cognitiveservices.azure.com/.default\"\n",
        ")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=f\"https://{resource_name}.openai.azure.com\",\n",
        "    azure_ad_token_provider=token_provider,\n",
        "    api_version=\"2025-04-01-preview\"\n",
        ")\n",
        "\n",
        "# Upload train file\n",
        "with Path(\"data/sft_train.jsonl\").open(\"rb\") as f:\n",
        "    train_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\"Train File ID: {train_file.id}\")\n",
        "\n",
        "# Upload validation file\n",
        "with Path(\"data/sft_test.jsonl\").open(\"rb\") as f:\n",
        "    valid_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\"Valid File ID: {valid_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2613e2b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fine-tuning job\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=valid_file.id,\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    suffix=\"zava1-retail-sft-1\",\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\"Job ID: {response.id}\")\n",
        "print(f\"Status: {response.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeda6529",
      "metadata": {},
      "source": [
        "### 7.5 Monitoring Training Progress (UI Overview)\n",
        "\n",
        "Supervised fine-tuning jobs in Microsoft Foundry come with built-in\n",
        "visualizations and job-status tracking. For this cookbook, we keep the\n",
        "monitoring steps simple and UI-focused ‚Äî the goal is to understand *what*\n",
        "to look for rather than walk through the low-level mechanics.\n",
        "\n",
        "After you start the fine-tuning job (Section 7.3 or 7.4), open the job\n",
        "details page in the Microsoft Foundry Studio:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd46aae",
      "metadata": {},
      "source": [
        "\n",
        "### 7.6 Using the Fine-Tuned Model in the Zava Retail Agent\n",
        "\n",
        "Once your fine-tuning job completes (Section 7.5), the final model becomes\n",
        "available in the Microsoft Foundry project as a new **fine-tuned deployment**\n",
        "candidate.  \n",
        "In this section, we attach that model to the Zava Retail Agent so we can:\n",
        "\n",
        "- test the improved tool-calling behavior  \n",
        "- compare base vs fine-tuned performance  \n",
        "- re-run the full conversation scenario from Section 4  \n",
        "\n",
        "This process is easiest through the Studio UI.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d48b34d",
      "metadata": {},
      "source": [
        "### 7.7 Running Evaluation Again After SFT\n",
        "\n",
        "Now that the Zava Retail Agent has been updated to use the fine-tuned\n",
        "`gpt-4.1-mini` model (Section 7.6), we can re-evaluate its tool-calling\n",
        "accuracy using the **same evaluation dataset** and **same Python grader**\n",
        "from Section 6.\n",
        "\n",
        "This gives us a clean, apples-to-apples comparison:\n",
        "\n",
        "- Base model (gpt-4.1-mini)  \n",
        "- Fine-tuned model (ft:gpt-4.1-mini:<your-model-id>)  \n",
        "\n",
        "The entire evaluation pipeline remains identical; the only change is\n",
        "choosing the new model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7.7.1 Submit a New Evaluation Run (UI)**\n",
        "\n",
        "1. Open your Azure AI Project  \n",
        "2. Navigate to:  \n",
        "   **Evaluate ‚Üí Evaluations**  \n",
        "3. Click **+ New evaluation**  \n",
        "4. Select the same dataset:  \n",
        "   `eval.jsonl`  \n",
        "5. Attach the same Python grader:  \n",
        "   `tool_call_grader.py`  \n",
        "6. Choose the **fine-tuned model deployment** instead of the base model  \n",
        "7. Start the evaluation run\n",
        "\n",
        "Everything else stays the same.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7.7.2 What Improvements You Should Expect**\n",
        "\n",
        "SFT is particularly effective for:\n",
        "\n",
        "- multi-step tool chains  \n",
        "- enforcing policy-driven flows  \n",
        "- argument propagation  \n",
        "- reducing hallucinated tool names  \n",
        "- stabilizing assistant responses in longer conversations  \n",
        "\n",
        "Typical improvements for this specific Zava dataset:\n",
        "\n",
        "| Model                     | Next Tool-Call Accuracy |\n",
        "|---------------------------|--------------------------|\n",
        "| gpt-4.1-nano (baseline)   | ~60‚Äì62%                 |\n",
        "| gpt-4.1-mini (baseline)   | ~72‚Äì75%                 |\n",
        "| gpt-4.1 (baseline)        | ~76%                    |\n",
        "| **ft:gpt-4.1-mini**       | **~84‚Äì87%**             |\n",
        "\n",
        "Your numbers may vary slightly depending on the synthetic data generation,\n",
        "but the **+8‚Äì12 point improvement** is consistent across multiple runs.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7.7.3 Inspecting Before/After Samples**\n",
        "\n",
        "In the evaluation result page, click into a few individual examples:\n",
        "\n",
        "- The **base model** often makes the correct *first* tool call  \n",
        "  but fails to chain the second one (e.g., missing `get_user_details`).\n",
        "- The **fine-tuned model** reliably predicts both:\n",
        "  - the correct tool name  \n",
        "  - the correct argument dict  \n",
        "  - the correct ordering of chained calls  \n",
        "\n",
        "This exactly matches the behavior you validated earlier with the\n",
        "terminal client in Section 7.6.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7.7.4 GIF Placeholder (Eval Comparison)**\n",
        "\n",
        "Add a GIF here showing:\n",
        "\n",
        "- the two evaluation runs  \n",
        "- their accuracy side-by-side  \n",
        "- drilling into a sample\n",
        "\n",
        "```markdown\n",
        "![Before vs After SFT Evaluation](img/eval_before_after_sft.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697881fd",
      "metadata": {},
      "source": [
        "#### 7.7.5 What SFT Solves ‚Äî and What It Doesn‚Äôt\n",
        "\n",
        "Supervised fine-tuning gives strong improvements on the Zava Retail Agent‚Äôs\n",
        "most common tool-calling challenges:\n",
        "\n",
        "- deterministic tool chaining  \n",
        "- enforcing policy-driven sequences  \n",
        "- correct argument propagation  \n",
        "- removing hallucinated tool names  \n",
        "- stabilizing multi-turn assistant behavior  \n",
        "\n",
        "However, SFT **does not** fully address more complex reasoning patterns:\n",
        "\n",
        "- long tool-call sequences (7‚Äì10 steps)  \n",
        "- scenarios requiring intermediate reasoning or planning  \n",
        "- flows that depend on synthesized internal state  \n",
        "- nuanced return-policy interpretation  \n",
        "- situations where the *next* tool call depends on multiple prior tools  \n",
        "\n",
        "These are cases where the model must learn *why* a tool is needed, not only\n",
        "*which* tool appears next in the training data.\n",
        "\n",
        "To handle these deeper decision-making patterns, we turn to **Reinforcement\n",
        "Fine-Tuning (RFT)** in Section 8, where the model learns to optimize for the\n",
        "*resulting correctness* of an entire multi-step sequence rather than only\n",
        "imitating the next step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56922e4",
      "metadata": {},
      "source": [
        "## 8. Reinforcement Fine-Tuning (RFT)\n",
        "\n",
        "Supervised fine-tuning (Section 7) significantly improved the Zava Retail\n",
        "Agent on short, policy-driven tool sequences such as:\n",
        "\n",
        "- name/ZIP lookup ‚Üí profile fetch  \n",
        "- simple order inquiries  \n",
        "- address-change flows  \n",
        "\n",
        "However, some customer scenarios require **much deeper multi-step reasoning**\n",
        "across multiple tool calls. These include:\n",
        "\n",
        "- reviewing an entire order history  \n",
        "- checking return eligibility for multiple items  \n",
        "- computing refund paths (gift card vs payment method)  \n",
        "- verifying policy exceptions  \n",
        "- branching logic depending on item category, time window, or status  \n",
        "\n",
        "These complex workflows often span **7‚Äì10 tool calls** and require the\n",
        "model to *reason* about policy, not merely imitate the next step.\n",
        "\n",
        "This is where **Reinforcement Fine-Tuning (RFT)** becomes essential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc69ccb",
      "metadata": {},
      "source": [
        "### 8.1 What RFT Optimizes in the Zava Retail Agent\n",
        "\n",
        "RFT trains the model to maximize a **reward** computed over the *entire*\n",
        "multi-turn output rather than predicting the next tool call in isolation.\n",
        "\n",
        "In our case, the reward comes from a **custom LLM-based grader** that checks:\n",
        "\n",
        "1. **Return-policy correctness**  \n",
        "   - Did the model apply the correct time windows?\n",
        "   - Did it respect category-specific restrictions?\n",
        "   - Did it choose the right remedy (refund, exchange, store credit)?\n",
        "\n",
        "2. **Sequence correctness**  \n",
        "   - Did the model choose the needed tools in the correct order?\n",
        "   - Did it avoid redundant or invalid tool calls?\n",
        "\n",
        "3. **Argument correctness**  \n",
        "   - Are the tool-call inputs valid and consistent with prior tool outputs?\n",
        "\n",
        "4. **Outcome correctness**  \n",
        "   - Is the final summary aligned with the expected customer guidance?\n",
        "\n",
        "SFT teaches *how* to perform tool chaining.  \n",
        "RFT teaches *why* a sequence is right or wrong ‚Äî and how to optimize it.\n",
        "\n",
        "This is especially powerful when the workflow has branching logic like:\n",
        "\n",
        "- ‚ÄúIf the item is in category X and within 15 days, allow return;  \n",
        "  otherwise offer store credit or deny.‚Äù\n",
        "\n",
        "Even a strong base model struggles with such decision-heavy scenarios unless\n",
        "it is trained with **reward-driven optimization** over long conversations.\n",
        "\n",
        "In the next sections, we prepare the RFT dataset, design the reward function,\n",
        "and run an RFT job that dramatically improves the agent's ability to follow\n",
        "deep return-policy logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f587574",
      "metadata": {},
      "source": [
        "### 8.2 The RFT Dataset (Multi-Order, Policy-Heavy Scenarios)\n",
        "\n",
        "Reinforcement Fine-Tuning requires a different dataset than SFT.  \n",
        "Instead of teaching the model to **imitate** a transcript, RFT teaches the model to **optimize correctness** across complex, multi-item return-policy scenarios using a reward function.\n",
        "\n",
        "The dataset used in this notebook lives in:\n",
        "\n",
        "```bash\n",
        "data/rft/rft_train.jsonl\n",
        "data/rft/rft_test.jsonl\n",
        "```\n",
        "\n",
        "Each record includes:\n",
        "\n",
        "- A multi-turn conversation (often 7‚Äì12 turns)\n",
        "- Multiple orders and multiple item_ids in the same request\n",
        "- Mixed eligibility windows (electronics vs general merchandise vs defect-only)\n",
        "- Account-level restrictions (e.g., store-credit-only refunds)\n",
        "- Prior tool outputs that the model must interpret\n",
        "- Several items with **different policies and outcomes** in a single turn\n",
        "\n",
        "These scenarios mimic real customer-service complexity and force the model to reason about:\n",
        "\n",
        "- purchase dates  \n",
        "- category rules  \n",
        "- return windows  \n",
        "- defect conditions  \n",
        "- refund routing rules  \n",
        "- multi-step logical dependencies  \n",
        "\n",
        "This type of reasoning **cannot** be learned reliably through SFT alone.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why This Dataset Is a Good Fit for RFT\n",
        "\n",
        "The dataset is deliberately ‚Äúpolicy-heavy‚Äù and contains scenarios such as:\n",
        "\n",
        "- 7‚Äì10 tool calls across a single workflow  \n",
        "- Mixed eligibility in the same conversation  \n",
        "- Conflicting policy dimensions (category √ó days √ó condition √ó account rules)  \n",
        "- Cross-order dependencies  \n",
        "- Conversations where correctness can‚Äôt be judged by matching a transcript  \n",
        "\n",
        "SFT teaches the syntax of calling tools.  \n",
        "RFT teaches **decision-making quality**.\n",
        "\n",
        "Next, we design the **reward function** (Section 8.3), which evaluates an entire tool-calling workflow and returns a numeric score.  \n",
        "This becomes the signal the model uses during RFT training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17cc6a7",
      "metadata": {},
      "source": [
        "### 8.3 Designing the Reward Function (LLM‚ÄëBased Return‚ÄëPolicy Grader)\n",
        "\n",
        "Reinforcement Fine‚ÄëTuning (RFT) needs a **numeric reward** for each conversation.\n",
        "In this cookbook, that reward comes from a model‚Äëbased grader that judges how well\n",
        "the assistant applied Zava‚Äôs return policy for every item the customer asks about.\n",
        "\n",
        "The grader configuration is stored in:\n",
        "\n",
        "```bash\n",
        "data/rft/rft_grader-config.json\n",
        "```\n",
        "\n",
        "At a high level, the reward model receives three things:\n",
        "\n",
        "- The **customer request and conversation history**\n",
        "- The **structured reference outcome** for each item (what should happen)\n",
        "- The **assistant‚Äôs final explanation**, including its per‚Äëitem decisions\n",
        "\n",
        "It then produces a score between **0.0 and 1.0**, where higher is better.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.3.1 What the Grader Checks\n",
        "\n",
        "For each item in the request, the grader evaluates:\n",
        "\n",
        "1. **ID Matching**\n",
        "   - Does the assistant mention the correct `order_id` and `item_id`?\n",
        "   - If IDs are wrong or missing, the score for that item is driven toward 0.0.\n",
        "\n",
        "2. **Eligibility Decision**\n",
        "   - Did the assistant correctly decide whether the item is return‚Äëeligible,\n",
        "     exchange‚Äëeligible, or not eligible at all?\n",
        "   - Incorrect eligibility (e.g., saying ‚Äúeligible‚Äù when it is not) yields a\n",
        "     zero score for that item.\n",
        "\n",
        "3. **Policy Justification**\n",
        "   - Does the explanation reference a concrete policy detail?\n",
        "     Examples include:\n",
        "       - time window (15‚Äëday electronics, 30‚Äëday general goods)\n",
        "       - item condition (opened vs unopened)\n",
        "       - account flags (store‚Äëcredit‚Äëonly)\n",
        "       - marketplace or third‚Äëparty seller rules\n",
        "   - If the eligibility is correct but the explanation is vague, the item gets\n",
        "     partial credit (e.g., 0.5 instead of 1.0).\n",
        "\n",
        "4. **Overall Consistency**\n",
        "   - The explanation must not contradict itself across items.\n",
        "   - The final summary must align with the per‚Äëitem outcomes.\n",
        "\n",
        "The final reward for a conversation is the **average** of all per‚Äëitem scores,\n",
        "normalized into the range **[0.0, 1.0]**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.3.2 Why This Reward Works Well for RFT\n",
        "\n",
        "This reward function is tightly aligned with what a retail business actually\n",
        "cares about:\n",
        "\n",
        "- Every item is judged **individually**, so the model can‚Äôt ‚Äúhide‚Äù mistakes.\n",
        "- The model is pushed to **explain the policy**, not just guess ‚Äúyes/no‚Äù.\n",
        "- Hallucinations and ID mismatches are penalized heavily.\n",
        "- Correct but unjustified answers are only partially rewarded.\n",
        "\n",
        "During RFT, the model repeatedly generates candidate responses for these\n",
        "scenarios, receives a reward from this grader, and updates its parameters to\n",
        "maximize that reward. Over time, it learns to produce:\n",
        "\n",
        "- policy‚Äëconsistent decisions,\n",
        "- grounded references to the right orders and items,\n",
        "- and clear, customer‚Äëfriendly explanations.\n",
        "\n",
        "In the next section (8.4), we‚Äôll see how to launch an RFT job that uses this\n",
        "reward function together with the `rft_train.jsonl` dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2c022a",
      "metadata": {},
      "source": [
        "### 8.4 Launching the RFT Job (UI + Optional Code)\n",
        "\n",
        "Reinforcement Fine-Tuning (RFT) uses three components:\n",
        "\n",
        "1. **Training dataset**\n",
        "2. **Reward function (grader)**\n",
        "3. **Base model**\n",
        "\n",
        "All three are already prepared in this cookbook.\n",
        "\n",
        "The RFT job is typically launched through the **Microsoft Foundry UI**, which\n",
        "provides a guided workflow. For reproducibility, we document both **UI steps**\n",
        "and **code placeholders** below.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.4.1 Files Used for RFT\n",
        "\n",
        "Training set:\n",
        "\n",
        "```bash\n",
        "data/rft/rft_train.jsonl\n",
        "```\n",
        "\n",
        "Evaluation (holdout) set:\n",
        "\n",
        "```bash\n",
        "data/rft/rft_test.jsonl\n",
        "```\n",
        "\n",
        "Reward function configuration:\n",
        "\n",
        "```bash\n",
        "data/rft/rft_grader-config.json\n",
        "```\n",
        "\n",
        "Custom tools metadata (used by the reward model):\n",
        "\n",
        "```bash\n",
        "data/rft/rft_tools-config.json\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8.4.2 Launching an RFT Job (UI Workflow)\n",
        "\n",
        "> **Note:** RFT is easiest to run through the Microsoft Foundry Studio.  \n",
        "> Insert a GIF or screenshot here showing the UI workflow.\n",
        "\n",
        "In the UI:\n",
        "\n",
        "1. Navigate to **Model Customization ‚Üí Reinforcement Fine-Tuning**  \n",
        "2. Select your model:\n",
        "   - `gpt-4.1-mini`\n",
        "3. Upload the training dataset  \n",
        "4. Provide the reward model configuration (`rft_grader-config.json`)  \n",
        "5. Upload the tools config (`rft_tools-config.json`)  \n",
        "6. Leave default training hyperparameters unless you have a reason to tune  \n",
        "7. Submit the job\n",
        "\n",
        "Training takes longer than SFT because each step requires running the reward\n",
        "function against model-generated outputs.\n",
        "\n",
        "Once complete, the new RFT model will appear under **Custom Models** with full\n",
        "metadata (reward curves, job logs, metrics, artifacts).\n",
        "\n",
        "---\n",
        "\n",
        "### 8.4.3 Launching an RFT Job (Code Placeholder)\n",
        "\n",
        "This cookbook does not execute RFT jobs directly, but the following code block\n",
        "illustrates the structure of an RFT request using the Azure AI SDK:\n",
        "\n",
        "```python\n",
        "# Placeholder ‚Äî Fill in with your project and workspace IDs\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id=\"<your-subscription>\",\n",
        "    resource_group_name=\"<your-resource-group>\",\n",
        "    workspace_name=\"<your-workspace>\"\n",
        ")\n",
        "\n",
        "rft_job = {\n",
        "    \"model\": \"gpt-4.1-mini\",\n",
        "    \"training_data\": \"data/rft/rft_train.jsonl\",\n",
        "    \"reward_config\": \"data/rft/rft_grader-config.json\",\n",
        "    \"tools_config\": \"data/rft/rft_tools-config.json\",\n",
        "}\n",
        "\n",
        "# ml_client.jobs.create_or_update(rft_job)\n",
        "```\n",
        "\n",
        "Replace with actual SDK fields as needed ‚Äî this is only a structural scaffold.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.4.4 What Happens After Submission\n",
        "\n",
        "During RFT training:\n",
        "\n",
        "- The model generates multiple candidate responses  \n",
        "- The reward function evaluates each response  \n",
        "- The training loop updates the model to maximize reward  \n",
        "\n",
        "The result is a model that not only produces the **correct** tool calls,\n",
        "but also provides **policy-grounded reasoning** across long workflows.\n",
        "\n",
        "In the next section (8.5), we examine how to track and interpret the RFT\n",
        "training curves and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71e69d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload train file\n",
        "with Path(\"data/rft_train_fixed.jsonl\").open(\"rb\") as f:\n",
        "    train_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\"Train File ID: {train_file.id}\")\n",
        "\n",
        "# Upload validation file\n",
        "with Path(\"data/rft_test_fixed.jsonl\").open(\"rb\") as f:\n",
        "    valid_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\"Valid File ID: {valid_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93218660",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Configure reinforcement learning parameters\n",
        "\n",
        "with open(\"data/rft_grader-config.json\", 'r') as file:\n",
        "    grader_config = json.load(file)\n",
        "\n",
        "with open(\"data/rft_tools-config.json\", 'r') as file:\n",
        "    tools_config = json.load(file)\n",
        "\n",
        "method_body = {\n",
        "    \"type\": \"reinforcement\",\n",
        "    \"reinforcement\": {\n",
        "      \"hyperparameters\": {\n",
        "        \"eval_interval\": 3,\n",
        "        \"eval_samples\": 5,\n",
        "        \"compute_multiplier\": 1,\n",
        "        \"reasoning_effort\": \"medium\",\n",
        "        \"n_epochs\": 6,\n",
        "        \"batch_size\": 2,\n",
        "        \"learning_rate_multiplier\": 2\n",
        "      },\n",
        "      \"grader\": grader_config,\n",
        "      \"tools\": tools_config\n",
        "    }\n",
        "  }\n",
        "\n",
        "print(\"Method Body:\")\n",
        "print(json.dumps(method_body, indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f470e8a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fine-tuning job\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=valid_file.id,\n",
        "    model = \"o4-mini-2025-04-16\",\n",
        "    suffix=\"zava1-retail-sft-1\",\n",
        "    method= method_body,    \n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\"Job ID: {response.id}\")\n",
        "print(f\"Status: {response.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f101cb",
      "metadata": {},
      "source": [
        "### 8.5 Monitoring and Interpreting RFT Training Metrics\n",
        "\n",
        "Reinforcement Fine-Tuning produces more nuanced metrics than SFT.  \n",
        "Instead of simply tracking loss curves, RFT logs **reward trends** over time.\n",
        "These reward curves are essential for validating that the model is learning\n",
        "policy-aware decision-making.\n",
        "\n",
        "This cookbook does not reproduce the full UI, but we outline exactly what\n",
        "to look for once your RFT job begins training.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.5.1 Key Metrics to Watch\n",
        "\n",
        "When you open the RFT job details in Microsoft Foundry Studio, look for:\n",
        "\n",
        "#### **1. Average Reward per Step**\n",
        "- The most important metric.\n",
        "- Should trend upward over the course of training.\n",
        "- Early plateaus are normal; late-stage jumps indicate the model discovered\n",
        "  a better policy pattern.\n",
        "\n",
        "#### **2. Reward Variance**\n",
        "- Measures consistency.\n",
        "- High variance early on means the model is exploring multiple strategies.\n",
        "- Over time, variance should narrow as the model locks onto stable policy reasoning.\n",
        "\n",
        "#### **3. Rejection / Invalid Output Rate**\n",
        "- Indicates how often the reward model could not parse the assistant‚Äôs output.\n",
        "- Should decrease sharply after the first few hundred steps.\n",
        "- If this stays flat, the reward function may need adjustment.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.5.2 How to Interpret Training Curves\n",
        "\n",
        "A **healthy RFT run** typically shows:\n",
        "\n",
        "- A steady climb in average reward  \n",
        "- A reduction in invalid outputs  \n",
        "- A decrease in reward variance  \n",
        "- Occasional jumps when the model discovers a better decision pattern  \n",
        "\n",
        "Unhealthy signs include:\n",
        "\n",
        "- Flat reward curves  \n",
        "- Rising invalid output rates  \n",
        "- Collapsing to trivial strategies (e.g., always rejecting returns)  \n",
        "- Overfitting (training reward rises but eval reward drops)\n",
        "\n",
        "---\n",
        "\n",
        "### 8.5.3 Viewing Metrics in the UI\n",
        "\n",
        "Insert a GIF or screenshot here:\n",
        "\n",
        "```bash\n",
        "# Placeholder:\n",
        "# Place rft_training_dashboard.gif inside img/ and reference it like:\n",
        "# ![RFT Metrics Dashboard](img/rft_training_dashboard.gif)\n",
        "```\n",
        "\n",
        "In the Microsoft Foundry interface, navigate to:\n",
        "\n",
        "**Model Customization ‚Üí Reinforcement Fine-Tuning ‚Üí [Your RFT Job] ‚Üí Metrics**\n",
        "\n",
        "You will see:\n",
        "\n",
        "- Reward progression chart  \n",
        "- Per-item scoring distribution  \n",
        "- Training event logs  \n",
        "- Model-checkpoint-by-checkpoint comparisons  \n",
        "\n",
        "These views help you validate that your reward function and dataset are producing stable learning.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.5.4 The Goal of Monitoring\n",
        "\n",
        "Monitoring ensures that:\n",
        "\n",
        "- The model is learning **policy reasoning**, not superficial correlations.  \n",
        "- Tool arguments become more accurate over time.  \n",
        "- The assistant becomes more consistent across multi-item workflows.  \n",
        "- Reward-model alignment remains tight throughout training.\n",
        "\n",
        "Once you see stable improvements across training and evaluation sets,\n",
        "you are ready to test the RFT model in live agent flows (Section 8.6).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da21444",
      "metadata": {},
      "source": [
        "### 8.6 Testing the RFT Model in the Zava Retail Agent\n",
        "\n",
        "After RFT training completes, the final and most satisfying step is to\n",
        "**test the RFT-enhanced model inside the real Retail Agent workflow**.\n",
        "This is where we verify that the model has internalized policy reasoning,\n",
        "long tool chains, and multi-item logic ‚Äî not because it memorized examples,\n",
        "but because it learned from reward signals.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.6.1 Switching the Agent to the RFT Model\n",
        "\n",
        "In your terminal-based demo client (`tools/retail_agent.py`), simply pass the\n",
        "new RFT model name. It will look similar to:\n",
        "\n",
        "```bash\n",
        "python tools/retail_agent.py --model <your-rft-model-name>\n",
        "```\n",
        "\n",
        "You can also test the RFT model directly inside Microsoft Foundry:\n",
        "\n",
        "**AI Foundry ‚Üí Agents ‚Üí [Zava Retail Agent] ‚Üí Test**\n",
        "\n",
        "Insert a GIF or screenshot of this step:\n",
        "\n",
        "```bash\n",
        "# Example placeholder:\n",
        "# ![Testing RFT Model in Agent Playground](img/rft_agent_test.gif)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8.6.2 What You Should Expect to See\n",
        "\n",
        "With a strong reward function and high-quality RFT dataset, you will notice:\n",
        "\n",
        "#### **1. Longer, Correct Tool Chains**\n",
        "The model correctly:\n",
        "\n",
        "- fetches all orders  \n",
        "- finds item-level metadata  \n",
        "- checks eligibility windows  \n",
        "- interprets category-specific rules  \n",
        "- follows account exceptions  \n",
        "- computes refund method correctly  \n",
        "\n",
        "Often **7‚Äì10 API calls** will be invoked in a single user flow.\n",
        "\n",
        "#### **2. Reduced Hallucinations**\n",
        "The RFT model is far less likely to:\n",
        "\n",
        "- invent new tool names  \n",
        "- fabricate item_ids  \n",
        "- skip required steps  \n",
        "- contradict the return policy  \n",
        "\n",
        "#### **3. Better Explanations**\n",
        "Beyond correctness, responses are more:\n",
        "\n",
        "- structured  \n",
        "- grounded in policy  \n",
        "- consistent across items  \n",
        "- helpful to the end user  \n",
        "\n",
        "You will see explanations referencing the right dates, categories, and\n",
        "eligibility windows without prompting.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.6.3 Example Terminal Test Flow\n",
        "\n",
        "Add a GIF showing your real terminal run:\n",
        "\n",
        "```bash\n",
        "# Placeholder for your animation:\n",
        "# ![RFT Terminal Session](img/rft_terminal_demo.gif)\n",
        "```\n",
        "\n",
        "This should show:\n",
        "\n",
        "- User requesting returns for multiple items  \n",
        "- The agent invoking a long sequence of tools  \n",
        "- The final output aligning with the reward model‚Äôs expectations  \n",
        "\n",
        "---\n",
        "\n",
        "### 8.6.4 Validating the RFT Model\n",
        "\n",
        "For your Ignite demo (and this cookbook), you can highlight:\n",
        "\n",
        "- How the RFT model corrects the exact failures seen in the base and SFT models  \n",
        "- How multi-item scenarios now behave consistently  \n",
        "- How policy-heavy cases are handled without hand-designed prompting  \n",
        "\n",
        "The RFT model is now ready for production-style evaluations or integration\n",
        "into more complex agent architectures.\n",
        "\n",
        "Next, we wrap up with Section 9 ‚Äî a brief conclusion and recommendations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188c748b",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "This cookbook walked through the complete journey of improving tool‚Äëcalling\n",
        "accuracy for the Zava Retail Agent ‚Äî from diagnosing model failures, to\n",
        "synthetic data generation, to SFT, and finally RFT with a policy‚Äëaware reward\n",
        "function.\n",
        "\n",
        "By combining these techniques, we demonstrated a practical, production‚Äëready\n",
        "approach to building robust enterprise agents in Microsoft Foundry.\n",
        "\n",
        "---\n",
        "\n",
        "### 9.1 What We Accomplished\n",
        "\n",
        "#### **1. Diagnosed a real failure mode**\n",
        "- Base models struggled with user‚Äëidentity resolution and tool chaining.\n",
        "- Even strong models made mistakes in multi‚Äëstep reasoning (find_user ‚Üí get_user_details).\n",
        "- More complex, policy‚Äëheavy workflows needed something beyond SFT.\n",
        "\n",
        "#### **2. Set up a complete reproducible environment**\n",
        "- MCP server exposing 17 tools  \n",
        "- Local agent client for fast iteration  \n",
        "- Microsoft Foundry for model customization  \n",
        "- Clean, versioned datasets in `data/`\n",
        "\n",
        "#### **3. Generated high‚Äëquality synthetic data**\n",
        "- Used OpenAPI‚Äëdriven generation in Microsoft Foundry  \n",
        "- Produced thousands of conversations with realistic tool behavior  \n",
        "- Built SFT-ready train/test splits  \n",
        "- Added a visualization dashboard and filtering pipeline\n",
        "\n",
        "#### **4. Built a Python‚Äëbased evaluation framework**\n",
        "- Expanded evaluation sets to one‚Äërecord‚Äëper‚Äëtool‚Äëcall  \n",
        "- Wrote a deterministic grader for tool accuracy  \n",
        "- Benchmarked multiple base models and measured real failures\n",
        "\n",
        "#### **5. Improved accuracy with Supervised Fine-Tuning**\n",
        "- Taught the model correct tool chaining  \n",
        "- Reduced hallucinations  \n",
        "- Achieved measurable accuracy improvements across all models  \n",
        "- Verified correctness using the evaluation pipeline\n",
        "\n",
        "#### **6. Tackled complex policy reasoning with RFT**\n",
        "- Created long, multi-item, multi-order scenarios  \n",
        "- Designed an LLM-based policy grader  \n",
        "- Used reward signals instead of imitation learning  \n",
        "- Obtained major gains in reasoning correctness and consistency\n",
        "\n",
        "#### **7. Validated improvements in real flows**\n",
        "- Ran both SFT and RFT models through the real Retail Agent  \n",
        "- Observed fewer errors, better tool sequences, and more grounded explanations  \n",
        "- Terminal demos confirmed correctness in end-to-end workflows\n",
        "\n",
        "---\n",
        "\n",
        "### 9.2 When to Use SFT vs. RFT\n",
        "\n",
        "| Technique | Best For | Not Ideal For |\n",
        "|----------|----------|----------------|\n",
        "| **SFT** | Deterministic tool chaining, argument propagation, syntax learning | Deep reasoning, multi-step decision workflows |\n",
        "| **RFT** | Policy enforcement, long tool chains, multi-item logic, dynamic decision making | Pure imitation tasks or small-data scenarios |\n",
        "\n",
        "A production agent often benefits from **both**:\n",
        "- SFT for predictable behavior  \n",
        "- RFT for complex reasoning  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.3 Where to Go from Here\n",
        "\n",
        "#### **1. Extend your reward function**\n",
        "- Add tracking for latency, brevity, user satisfaction, or policy strictness  \n",
        "- Combine multiple reward types (e.g., correctness + helpfulness)\n",
        "\n",
        "#### **2. Build richer tools**\n",
        "- Add new tool families (inventory, promotions, subscription management)  \n",
        "- Use tool metadata to guide reward shaping\n",
        "\n",
        "#### **3. Evaluate on real user logs**\n",
        "- Replace synthetic datasets with anonymized real customer flows  \n",
        "- Use error patterns to design new RFT tasks\n",
        "\n",
        "#### **4. Deploy & observe**\n",
        "- Integrate your custom models into live agents  \n",
        "- Use Microsoft Foundry‚Äôs monitoring to track drift and failures  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.4 Final Thoughts\n",
        "\n",
        "This cookbook demonstrates a reproducible, end-to-end pattern for building\n",
        "high-quality enterprise agents:\n",
        "\n",
        "- Synthetic data for coverage  \n",
        "- SFT for structure  \n",
        "- RFT for deep reasoning  \n",
        "- Evals for measurement  \n",
        "- MCP for grounded execution\n",
        "\n",
        "This pattern generalizes far beyond retail: healthcare, finance, support,\n",
        "field operations, travel, logistics, and more.\n",
        "\n",
        "You now have everything you need to design, train, evaluate, and ship\n",
        "domain‚Äëspecialized agent models with confidence.\n",
        "\n",
        "Happy building! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b152e2",
      "metadata": {},
      "source": [
        "\n",
        "![Title Diagram](./img/outro.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82cb08d",
      "metadata": {},
      "source": [
        "## 10. Cookbook Appendix (Reusable Recipes)\n",
        "- How to design datasets\n",
        "- How to implement graders\n",
        "- How to generate synthetic data\n",
        "- How to debug tool-calling traces\n",
        "- How to choose SFT vs RFT vs DPO\n",
        "- Cost estimation tools"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
