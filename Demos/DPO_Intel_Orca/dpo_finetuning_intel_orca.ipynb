{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Azure AI\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Azure AI\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-ai-projects>=2.0.0b1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (2.0.0b2)\n",
            "Requirement already satisfied: openai in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: azure-identity in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 8)) (1.25.1)\n",
            "Requirement already satisfied: azure-mgmt-cognitiveservices in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 9)) (14.1.0)\n",
            "Requirement already satisfied: python-dotenv in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: isodate>=0.6.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.35.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (1.36.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (12.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (2.12.5)\n",
            "Requirement already satisfied: sniffio in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from openai->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: cryptography>=2.5 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (46.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (1.34.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: msrest>=0.7.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: azure-mgmt-core>=1.6.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: requests>=2.21.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: certifi in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 8)) (2.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: pycparser in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8)) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9)) (3.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Azure Environment\n",
        "Set your Azure AI Project endpoint and model name. We're using **gpt-4.1-mini** in this example, but you can use other supported GPT models. Create a `.env` file with: \n",
        "\n",
        "```\n",
        "AZURE_AI_PROJECT_ENDPOINT=<your-endpoint> \n",
        "MODEL_NAME=gpt-4.1-mini\n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Endpoint: https://foundrysdk-eastus2-foundry-resou.services.ai.azure.com/api/projects/foundrysdk-eastus2-project\n",
            " Model: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\" Endpoint: {endpoint}\")\n",
        "print(f\" Model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Azure AI Project\n",
        "\n",
        "Connect to Azure AI Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Azure AI Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Connected to Azure AI Project\n"
          ]
        }
      ],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\" Connected to Azure AI Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Azure AI. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading training file...\n",
            " Training file ID: file-00a35c0996c44461981127363f36718a\n",
            "\n",
            "Uploading validation file...\n",
            " Validation file ID: file-5ae91d585b2a4681bda6b24016be25bf\n"
          ]
        }
      ],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for files to be processed...\n",
            " Files ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Job ID: ftjob-0b4940c745b9414787b8758ef0e7b36e\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 3,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"Standard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job enqueued. Waiting for jobs ahead to complete.\n"
          ]
        }
      ],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 TPM capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "import time\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4.1-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=50)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    print(\"Waiting for deployment to complete...\")\n",
        "    deployment.result()\n",
        "\n",
        "print(f\" Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
