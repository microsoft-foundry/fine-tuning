{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Azure AI\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Azure AI\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model\n",
        "\n",
        "Note: Execute each cell in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-ai-projects>=2.0.0b1 (from -r requirements.txt (line 2))\n",
            "  Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting openai (from -r requirements.txt (line 5))\n",
            "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting azure-identity (from -r requirements.txt (line 8))\n",
            "  Using cached azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
            "Collecting azure-mgmt-cognitiveservices (from -r requirements.txt (line 9))\n",
            "  Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting azure-ai-evaluation>=1.13.0 (from -r requirements.txt (line 12))\n",
            "  Using cached azure_ai_evaluation-1.13.7-py3-none-any.whl.metadata (49 kB)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 15))\n",
            "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core>=1.35.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (4.15.0)\n",
            "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached azure_storage_blob-12.27.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Collecting sniffio (from openai->-r requirements.txt (line 5))\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai->-r requirements.txt (line 5))\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting cryptography>=2.5 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
            "Collecting msal>=1.30.0 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting msrest>=0.7.1 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting azure-mgmt-core>=1.6.0 (from azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached azure_mgmt_core-1.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyjwt>=2.8.0 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting nltk>=3.9.1 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas<3.0.0,>=2.1.2 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
            "Collecting ruamel.yaml<1.0.0,>=0.17.10 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached ruamel_yaml-0.19.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting Jinja2>=3.1.6 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting aiohttp>=3.0 (from azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.0->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting requests>=2.21.0 (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting cffi>=2.0.0 (from cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.6->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
            "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting click (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.9.1->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "Collecting numpy>=1.23.2 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 5))\n",
            "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ruamel.yaml.clibz>=0.3.3 (from ruamel.yaml<1.0.0,>=0.17.10->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12))\n",
            "  Using cached ruamel_yaml_clibz-0.3.5-cp311-cp311-win_amd64.whl\n",
            "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 5)) (0.4.6)\n",
            "Collecting pycparser (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 8))\n",
            "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Requirement already satisfied: six>=1.5 in c:\\work\\amlrepos\\fine-tuning\\envdpo\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.2->azure-ai-evaluation>=1.13.0->-r requirements.txt (line 12)) (1.17.0)\n",
            "Collecting charset_normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2))\n",
            "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices->-r requirements.txt (line 9))\n",
            "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Using cached azure_ai_projects-2.0.0b2-py3-none-any.whl (234 kB)\n",
            "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "Using cached azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
            "Using cached azure_mgmt_cognitiveservices-14.1.0-py3-none-any.whl (290 kB)\n",
            "Using cached azure_ai_evaluation-1.13.7-py3-none-any.whl (1.1 MB)\n",
            "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
            "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "Using cached azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
            "Using cached azure_mgmt_core-1.6.0-py3-none-any.whl (29 kB)\n",
            "Using cached azure_storage_blob-12.27.1-py3-none-any.whl (428 kB)\n",
            "Using cached cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
            "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
            "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
            "Using cached ruamel_yaml-0.19.0-py3-none-any.whl (117 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
            "Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
            "Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
            "Using cached numpy-2.4.0-cp311-cp311-win_amd64.whl (12.6 MB)\n",
            "Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
            "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Installing collected packages: pytz, urllib3, tzdata, typing-inspection, tqdm, sniffio, ruamel.yaml.clibz, regex, python-dotenv, pyjwt, pydantic-core, pycparser, propcache, oauthlib, numpy, multidict, MarkupSafe, joblib, jiter, isodate, idna, h11, frozenlist, distro, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, ruamel.yaml, requests, pydantic, pandas, nltk, Jinja2, httpcore, cffi, anyio, aiosignal, requests-oauthlib, httpx, cryptography, azure-core, aiohttp, openai, msrest, azure-storage-blob, azure-mgmt-core, msal, azure-mgmt-cognitiveservices, azure-ai-projects, msal-extensions, azure-identity, azure-ai-evaluation\n",
            "Successfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.12.0 attrs-25.4.0 azure-ai-evaluation-1.13.7 azure-ai-projects-2.0.0b2 azure-core-1.37.0 azure-identity-1.25.1 azure-mgmt-cognitiveservices-14.1.0 azure-mgmt-core-1.6.0 azure-storage-blob-12.27.1 certifi-2025.11.12 cffi-2.0.0 charset_normalizer-3.4.4 click-8.3.1 cryptography-46.0.3 distro-1.9.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 isodate-0.7.2 jiter-0.12.0 joblib-1.5.3 msal-1.34.0 msal-extensions-1.3.1 msrest-0.7.1 multidict-6.7.0 nltk-3.9.2 numpy-2.4.0 oauthlib-3.3.1 openai-2.14.0 pandas-2.3.3 propcache-0.4.1 pycparser-2.23 pydantic-2.12.5 pydantic-core-2.41.5 pyjwt-2.10.1 python-dotenv-1.2.1 pytz-2025.2 regex-2025.11.3 requests-2.32.5 requests-oauthlib-2.0.0 ruamel.yaml-0.19.0 ruamel.yaml.clibz-0.3.5 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2 tzdata-2025.3 urllib3-2.6.2 yarl-1.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Evaluation Function\n",
        "\n",
        "Function to evaluate model performance using Azure AI Evaluation SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(deployment_name, num_samples=10):\n",
        "    \"\"\"\n",
        "    Evaluate a model deployment using Azure AI Evaluation SDK.\n",
        "    \n",
        "    Args:\n",
        "        deployment_name: Name of the deployed model to evaluate\n",
        "        num_samples: Number of samples to evaluate (default: 10)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from azure.ai.evaluation import evaluate, CoherenceEvaluator, FluencyEvaluator, RelevanceEvaluator, SimilarityEvaluator, GroundednessEvaluator\n",
        "    from openai import AzureOpenAI\n",
        "    \n",
        "    print(f\"Evaluating deployment: {deployment_name}\")\n",
        "    print(f\"Using {num_samples} samples from training.jsonl\")\n",
        "    \n",
        "    azure_openai_client = AzureOpenAI(\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        api_version=\"2024-08-01-preview\"\n",
        "    )\n",
        "    \n",
        "    print(\"Generating model responses...\")\n",
        "    eval_data = []\n",
        "    with open(\"training.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            sample = json.loads(line)\n",
        "            \n",
        "            messages = sample[\"input\"][\"messages\"]\n",
        "            query = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "            \n",
        "            response = azure_openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            model_response = response.choices[0].message.content\n",
        "            \n",
        "            ground_truth = next((msg[\"content\"] for msg in sample[\"preferred_output\"] if msg[\"role\"] == \"assistant\"), \"\")\n",
        "            \n",
        "            eval_data.append({\n",
        "                \"query\": query,\n",
        "                \"response\": model_response,\n",
        "                \"ground_truth\": ground_truth\n",
        "            })\n",
        "            # Print progress every 10 samples or on the last sample\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == num_samples:\n",
        "                print(f\"  Processed {i+1}/{num_samples}\")\n",
        "    \n",
        "    eval_file = f\"evaluation_data_{deployment_name.replace('-', '_')}.jsonl\"\n",
        "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "        for item in eval_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    model_config = {\n",
        "        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "        \"azure_deployment\": deployment_name,\n",
        "        \"api_version\": \"2024-08-01-preview\",\n",
        "    }\n",
        "    \n",
        "    print(\"Running evaluation with 5 metrics...\")\n",
        "    results = evaluate(\n",
        "        data=eval_file,\n",
        "        evaluators={\n",
        "            \"coherence\": CoherenceEvaluator(model_config=model_config),\n",
        "            \"fluency\": FluencyEvaluator(model_config=model_config),\n",
        "            \"relevance\": RelevanceEvaluator(model_config=model_config),\n",
        "            \"groundedness\": GroundednessEvaluator(model_config=model_config),\n",
        "            \"similarity\": SimilarityEvaluator(model_config=model_config)\n",
        "        },\n",
        "        evaluator_config={\n",
        "            \"default\": {\n",
        "                \"column_mapping\": {\n",
        "                    \"query\": \"${data.query}\",\n",
        "                    \"response\": \"${data.response}\",\n",
        "                    \"ground_truth\": \"${data.ground_truth}\"\n",
        "                }\n",
        "            },\n",
        "            \"groundedness\": {\n",
        "                \"column_mapping\": {\n",
        "                    \"query\": \"${data.query}\",\n",
        "                    \"response\": \"${data.response}\",\n",
        "                    \"context\": \"${data.ground_truth}\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        output_path=f\"./evaluation_results_{deployment_name.replace('-', '_')}\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nEVALUATION RESULTS: {deployment_name}\\n\")\n",
        "    \n",
        "    if \"metrics\" in results:\n",
        "        metrics = results[\"metrics\"]\n",
        "        \n",
        "        coherence = metrics.get('coherence.coherence', metrics.get('coherence'))\n",
        "        fluency = metrics.get('fluency.fluency', metrics.get('fluency'))\n",
        "        relevance = metrics.get('relevance.relevance', metrics.get('relevance'))\n",
        "        groundedness = metrics.get('groundedness.groundedness', metrics.get('groundedness'))\n",
        "        similarity = metrics.get('similarity.similarity', metrics.get('similarity'))\n",
        "        \n",
        "        if coherence is not None:\n",
        "            print(f\"Coherence:      {coherence:.4f} (1-5 scale)\")\n",
        "        if fluency is not None:\n",
        "            print(f\"Fluency:        {fluency:.4f} (1-5 scale)\")\n",
        "        if relevance is not None:\n",
        "            print(f\"Relevance:      {relevance:.4f} (1-5 scale)\")\n",
        "        if groundedness is not None:\n",
        "            print(f\"Groundedness:   {groundedness:.4f} (1-5 scale)\")\n",
        "        if similarity is not None:\n",
        "            print(f\"Similarity:     {similarity:.4f} (1-5 scale)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"Detailed results saved to: ./evaluation_results_{deployment_name.replace('-', '_')}\")\n",
        "    print(f\"Detailed results saved to: ./evaluation_results_{deployment_name.replace('-', '_')}\")\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure Azure Environment\n",
        "Set your Azure AI Project endpoint and model name. We're using **gpt-4.1-mini** in this example, but you can use other supported GPT models. Create a `.env` file with: \n",
        "\n",
        "```\n",
        "# Required for DPO Fine-Tuning\n",
        "AZURE_AI_PROJECT_ENDPOINT=<your-endpoint> \n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "MODEL_NAME=<your-base-model-name>\n",
        "\n",
        "# Required for Model Evaluation\n",
        "AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>\n",
        "AZURE_OPENAI_KEY=<your-azure-openai-api-key>\n",
        "DEPLOYMENT_NAME=<your-deployment-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\"Base model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Azure AI Project\n",
        "\n",
        "Connect to Azure AI Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Azure AI Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Azure AI Project\n"
          ]
        }
      ],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\"Connected to Azure AI Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Azure AI. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading training file...\n",
            " Training file ID: file-4121fac45b5144bab840f2a8bea3eb9c\n",
            "\n",
            "Uploading validation file...\n",
            " Validation file ID: file-b3b53f48582b4bf89741bfbd1f6fd7a1\n"
          ]
        }
      ],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for files to be processed...\n",
            " Files ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Base Model\n",
        "\n",
        "Establish baseline performance metrics by evaluating the base model before DPO fine-tuning. This provides a comparison point to measure improvements after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating base model: gpt-4.1-mini\n",
            "\n",
            "Evaluating deployment: gpt-4.1-mini\n",
            "Using 30 samples from training.jsonl\n",
            "Generating model responses...\n",
            "  Processed 10/30\n",
            "  Processed 20/30\n",
            "  Processed 30/30\n",
            "Running evaluation with 5 metrics...\n",
            "2026-01-02 17:08:05 +0530   26432 execution.bulk     INFO     Finished 1 / 30 lines.\n",
            "2026-01-02 17:08:05 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 8.81 seconds. Estimated time for incomplete lines: 255.49 seconds.\n",
            "2026-01-02 17:08:05 +0530   23304 execution.bulk     INFO     Finished 1 / 30 lines.\n",
            "2026-01-02 17:08:05 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 8.99 seconds. Estimated time for incomplete lines: 260.71 seconds.\n",
            "2026-01-02 17:08:05 +0530   12916 execution.bulk     INFO     Finished 1 / 30 lines.\n",
            "2026-01-02 17:08:05 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 9.04 seconds. Estimated time for incomplete lines: 262.16 seconds.\n",
            "2026-01-02 17:08:06 +0530   17440 execution.bulk     INFO     Finished 1 / 30 lines.\n",
            "2026-01-02 17:08:06 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 9.2 seconds. Estimated time for incomplete lines: 266.8 seconds.\n",
            "2026-01-02 17:08:06 +0530   19684 execution.bulk     INFO     Finished 1 / 30 lines.\n",
            "2026-01-02 17:08:06 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 10.07 seconds. Estimated time for incomplete lines: 292.03 seconds.\n",
            "2026-01-02 17:08:09 +0530   17440 execution.bulk     INFO     Finished 6 / 30 lines.\n",
            "2026-01-02 17:08:09 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 2.04 seconds. Estimated time for incomplete lines: 48.96 seconds.\n",
            "2026-01-02 17:08:10 +0530   12916 execution.bulk     INFO     Finished 8 / 30 lines.\n",
            "2026-01-02 17:08:10 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.67 seconds. Estimated time for incomplete lines: 36.74 seconds.\n",
            "2026-01-02 17:08:11 +0530   12916 execution.bulk     INFO     Finished 10 / 30 lines.\n",
            "2026-01-02 17:08:11 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.45 seconds. Estimated time for incomplete lines: 29.0 seconds.\n",
            "2026-01-02 17:08:11 +0530   17440 execution.bulk     INFO     Finished 10 / 30 lines.\n",
            "2026-01-02 17:08:11 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.46 seconds. Estimated time for incomplete lines: 29.2 seconds.\n",
            "2026-01-02 17:08:12 +0530   23304 execution.bulk     INFO     Finished 10 / 30 lines.\n",
            "2026-01-02 17:08:12 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.52 seconds. Estimated time for incomplete lines: 30.4 seconds.\n",
            "2026-01-02 17:08:12 +0530   26432 execution.bulk     INFO     Finished 10 / 30 lines.\n",
            "2026-01-02 17:08:12 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 30.6 seconds.\n",
            "2026-01-02 17:08:12 +0530   19684 execution.bulk     INFO     Finished 9 / 30 lines.\n",
            "2026-01-02 17:08:12 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 1.73 seconds. Estimated time for incomplete lines: 36.33 seconds.\n",
            "2026-01-02 17:08:13 +0530   19684 execution.bulk     INFO     Finished 10 / 30 lines.\n",
            "2026-01-02 17:08:13 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 1.61 seconds. Estimated time for incomplete lines: 32.2 seconds.\n",
            "2026-01-02 17:08:13 +0530   26432 execution.bulk     INFO     Finished 11 / 30 lines.\n",
            "2026-01-02 17:08:13 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 29.07 seconds.\n",
            "2026-01-02 17:08:14 +0530   23304 execution.bulk     INFO     Finished 11 / 30 lines.\n",
            "2026-01-02 17:08:14 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.57 seconds. Estimated time for incomplete lines: 29.83 seconds.\n",
            "2026-01-02 17:08:14 +0530   12916 execution.bulk     INFO     Finished 11 / 30 lines.\n",
            "2026-01-02 17:08:14 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.6 seconds. Estimated time for incomplete lines: 30.4 seconds.\n",
            "2026-01-02 17:08:14 +0530   17440 execution.bulk     INFO     Finished 11 / 30 lines.\n",
            "2026-01-02 17:08:14 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.6 seconds. Estimated time for incomplete lines: 30.4 seconds.\n",
            "2026-01-02 17:08:15 +0530   12916 execution.bulk     INFO     Finished 13 / 30 lines.\n",
            "2026-01-02 17:08:15 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.44 seconds. Estimated time for incomplete lines: 24.48 seconds.\n",
            "2026-01-02 17:08:16 +0530   19684 execution.bulk     INFO     Finished 11 / 30 lines.\n",
            "2026-01-02 17:08:16 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 1.74 seconds. Estimated time for incomplete lines: 33.06 seconds.\n",
            "2026-01-02 17:08:19 +0530   12916 execution.bulk     INFO     Finished 19 / 30 lines.\n",
            "2026-01-02 17:08:19 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 12.98 seconds.\n",
            "2026-01-02 17:08:19 +0530   17440 execution.bulk     INFO     Finished 19 / 30 lines.\n",
            "2026-01-02 17:08:19 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.19 seconds. Estimated time for incomplete lines: 13.09 seconds.\n",
            "2026-01-02 17:08:19 +0530   12916 execution.bulk     INFO     Finished 20 / 30 lines.\n",
            "2026-01-02 17:08:19 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.15 seconds. Estimated time for incomplete lines: 11.5 seconds.\n",
            "2026-01-02 17:08:20 +0530   17440 execution.bulk     INFO     Finished 20 / 30 lines.\n",
            "2026-01-02 17:08:20 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.16 seconds. Estimated time for incomplete lines: 11.6 seconds.\n",
            "2026-01-02 17:08:20 +0530   23304 execution.bulk     INFO     Finished 20 / 30 lines.\n",
            "2026-01-02 17:08:20 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 11.7 seconds.\n",
            "2026-01-02 17:08:20 +0530   26432 execution.bulk     INFO     Finished 20 / 30 lines.\n",
            "2026-01-02 17:08:20 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 11.7 seconds.\n",
            "2026-01-02 17:08:20 +0530   19684 execution.bulk     INFO     Finished 18 / 30 lines.\n",
            "2026-01-02 17:08:20 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 1.33 seconds. Estimated time for incomplete lines: 15.96 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 21 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.17 seconds. Estimated time for incomplete lines: 10.53 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 22 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.11 seconds. Estimated time for incomplete lines: 8.88 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 23 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 7.49 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 25 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 4.9 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 26 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 3.76 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 27 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 2.73 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 28 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 1.76 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 29 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 0.85 seconds.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Finished 30 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   26432 execution.bulk     INFO     Average execution time for completed lines: 0.82 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 21 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 10.62 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 22 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.13 seconds. Estimated time for incomplete lines: 9.04 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 23 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.08 seconds. Estimated time for incomplete lines: 7.56 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 24 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.04 seconds. Estimated time for incomplete lines: 6.24 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 25 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 5.0 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 26 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 3.84 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 27 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 2.76 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 28 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 1.78 seconds.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Finished 29 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 0.86 seconds.\n",
            "2026-01-02 17:08:21 +0530   19684 execution.bulk     INFO     Finished 20 / 30 lines.\n",
            "2026-01-02 17:08:21 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 1.25 seconds. Estimated time for incomplete lines: 12.5 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 21 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.2 seconds. Estimated time for incomplete lines: 10.8 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 21 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.21 seconds. Estimated time for incomplete lines: 10.89 seconds.\n",
            "2026-01-02 17:08:22 +0530   23304 execution.bulk     INFO     Finished 30 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   23304 execution.bulk     INFO     Average execution time for completed lines: 0.85 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 22 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.16 seconds. Estimated time for incomplete lines: 9.28 seconds.\n",
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"relevance_20260102_113756_925698\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-02 11:37:56.925698+00:00\"\n",
            "Duration: \"0:00:25.459820\"\n",
            "\n",
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"similarity_20260102_113756_929699\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-02 11:37:56.929699+00:00\"\n",
            "Duration: \"0:00:25.477456\"\n",
            "\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 23 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.11 seconds. Estimated time for incomplete lines: 7.77 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 24 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.06 seconds. Estimated time for incomplete lines: 6.36 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 22 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.16 seconds. Estimated time for incomplete lines: 9.28 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 25 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 1.02 seconds. Estimated time for incomplete lines: 5.1 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 26 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 3.96 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 23 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.11 seconds. Estimated time for incomplete lines: 7.77 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 24 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.07 seconds. Estimated time for incomplete lines: 6.42 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 27 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 0.95 seconds. Estimated time for incomplete lines: 2.85 seconds.\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   19684 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 42.000000 seconds.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 383, in _send_with_retries\n",
            "    response = await client.chat.completions.create(**params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
            "    result: _WithUsage = await method(*args, **kwargs)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\work\\AMLRepos\\fine-tuning\\envdpo\\Lib\\site-packages\\openai\\_base_client.py\", line 1597, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4.1-mini for gpt-4.1-mini in East US 2 have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2024-08-01-preview. Please retry after 42 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 28 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 0.92 seconds. Estimated time for incomplete lines: 1.84 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 25 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.04 seconds. Estimated time for incomplete lines: 5.2 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 29 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 0.9 seconds. Estimated time for incomplete lines: 0.9 seconds.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Finished 26 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 4.0 seconds.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Finished 30 / 30 lines.\n",
            "2026-01-02 17:08:22 +0530   17440 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Finished 27 / 30 lines.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 0.97 seconds. Estimated time for incomplete lines: 2.91 seconds.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Finished 28 / 30 lines.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 1.88 seconds.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Finished 29 / 30 lines.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 0.91 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"coherence_20260102_113756_916688\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-02 11:37:56.916688+00:00\"\n",
            "Duration: \"0:00:26.550113\"\n",
            "\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Finished 30 / 30 lines.\n",
            "2026-01-02 17:08:23 +0530   12916 execution.bulk     INFO     Average execution time for completed lines: 0.89 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"groundedness_20260102_113756_932699\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-02 11:37:56.932699+00:00\"\n",
            "Duration: \"0:00:27.429327\"\n",
            "\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 21 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 3.34 seconds. Estimated time for incomplete lines: 30.06 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 22 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 3.2 seconds. Estimated time for incomplete lines: 25.6 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 23 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 3.06 seconds. Estimated time for incomplete lines: 21.42 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 24 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.94 seconds. Estimated time for incomplete lines: 17.64 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 25 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.82 seconds. Estimated time for incomplete lines: 14.1 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 26 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.71 seconds. Estimated time for incomplete lines: 10.84 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 27 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.61 seconds. Estimated time for incomplete lines: 7.83 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 28 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.52 seconds. Estimated time for incomplete lines: 5.04 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 29 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.44 seconds. Estimated time for incomplete lines: 2.44 seconds.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Finished 30 / 30 lines.\n",
            "2026-01-02 17:09:07 +0530   19684 execution.bulk     INFO     Average execution time for completed lines: 2.36 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
            "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= Run Summary =======\n",
            "\n",
            "Run name: \"fluency_20260102_113756_922698\"\n",
            "Run status: \"Completed\"\n",
            "Start time: \"2026-01-02 11:37:56.922698+00:00\"\n",
            "Duration: \"0:01:11.299292\"\n",
            "\n",
            "======= Combined Run Summary (Per Evaluator) =======\n",
            "\n",
            "{\n",
            "    \"coherence\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:26.550113\",\n",
            "        \"completed_lines\": 30,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"fluency\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:01:11.299292\",\n",
            "        \"completed_lines\": 30,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"relevance\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:25.459820\",\n",
            "        \"completed_lines\": 30,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"groundedness\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:27.429327\",\n",
            "        \"completed_lines\": 30,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    },\n",
            "    \"similarity\": {\n",
            "        \"status\": \"Completed\",\n",
            "        \"duration\": \"0:00:25.477456\",\n",
            "        \"completed_lines\": 30,\n",
            "        \"failed_lines\": 0,\n",
            "        \"log_path\": null,\n",
            "        \"error_message\": null,\n",
            "        \"error_code\": null\n",
            "    }\n",
            "}\n",
            "\n",
            "====================================================\n",
            "\n",
            "Evaluation results saved to \"C:\\work\\AMLRepos\\fine-tuning\\Demos\\DPO_Intel_Orca\\evaluation_results_gpt_4.1_mini\".\n",
            "\n",
            "\n",
            "EVALUATION RESULTS: gpt-4.1-mini\n",
            "\n",
            "Coherence:      4.2667 (1-5 scale)\n",
            "Fluency:        3.6333 (1-5 scale)\n",
            "Relevance:      4.9667 (1-5 scale)\n",
            "Groundedness:   4.1000 (1-5 scale)\n",
            "Similarity:     4.6000 (1-5 scale)\n",
            "============================================================\n",
            "Detailed results saved to: ./evaluation_results_gpt_4.1_mini\n",
            "Detailed results saved to: ./evaluation_results_gpt_4.1_mini\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Job ID: ftjob-eb2842107abe43f2a0e0dd3d271146ad\n",
            "Status: pending\n",
            " Job ID: ftjob-aaded7f5c0c44f8da1b577efd79899ee\n",
            "Status: pending\n",
            "Status: running\n",
            "Preprocessing completed for file validation file.\n",
            "Preprocessing completed for file training file.\n",
            "Job enqueued. Waiting for jobs ahead to complete.\n"
          ]
        }
      ],
      "source": [
        "base_deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "print(f\"Evaluating base model: {base_deployment}\\n\")\n",
        "\n",
        "base_results = evaluate_model(base_deployment, num_samples=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Job ID: ftjob-4cad7de198a34baeb4f0c95ff01ac844\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 3,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"GlobalStandard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job enqueued. Waiting for jobs ahead to complete.\n"
          ]
        }
      ],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "import time\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4.1-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=50)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    print(\"Waiting for deployment to complete...\")\n",
        "    deployment.result()\n",
        "\n",
        "print(f\" Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate Fine-Tuned Model\n",
        "\n",
        "Evaluate your model using Azure AI Evaluation SDK to measure quality improvements from DPO fine-tuning.\n",
        "\n",
        "We'll assess 5 key metrics:\n",
        "- **Coherence**: Logical flow and structure\n",
        "- **Fluency**: Grammatical correctness and naturalness\n",
        "- **Relevance**: How well responses address the query\n",
        "- **Groundedness**: Factual accuracy against context\n",
        "- **Similarity**: Alignment with preferred outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Evaluating fine-tuned model: {deployment_name}\\n\")\n",
        "\n",
        "finetuned_results = evaluate_model(deployment_name, num_samples=50)\n",
        "\n",
        "print(\"\\nCompare base model vs fine-tuned model metrics to see DPO improvements!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "envdpo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
